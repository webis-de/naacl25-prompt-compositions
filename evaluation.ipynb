{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from os import listdir, path\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from src.bias_detection.config.settings import RANDOM_SEED\n",
    "from src.bias_detection.data_handler import DataHandler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense QA corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"mistral-7b-instruct-v2\",\n",
    "    \"command-r-v01\",\n",
    "    \"llama3-70b-instruct\",\n",
    "]\n",
    "\n",
    "TRAINING_DATASETS = [\"commonsense_qa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(datasets_to_load=[\"common_qa\"])\n",
    "commonsense_qa_train_df = data_handler.common_qa[\"train\"].rename(\n",
    "    columns={\"label\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "commonsense_qa_val_df = data_handler.common_qa[\"dev\"].rename(\n",
    "    columns={\"label\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "commonsense_qa_test_df = data_handler.common_qa[\"test\"].rename(\n",
    "    columns={\"label\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa_composition_predictions_val_per_model = {}\n",
    "commonsense_qa_composition_predictions_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    for training_dataset in TRAINING_DATASETS:\n",
    "        # Validation set\n",
    "        composition_predictions_val = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"commonsense_qa_val_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_val.columns:\n",
    "                composition_predictions_val[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_val[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            composition_predictions_val = pd.merge(\n",
    "                composition_predictions_val,\n",
    "                df.loc[\n",
    "                    :,\n",
    "                    [\n",
    "                        i\n",
    "                        for i in df.columns\n",
    "                        if i not in [\"input\", \"index\", \"pred_probabilities\"]\n",
    "                    ],\n",
    "                ],\n",
    "                on=\"post_id\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        commonsense_qa_composition_predictions_val_per_model[\n",
    "            f\"{model}__{training_dataset}\"\n",
    "        ] = composition_predictions_val\n",
    "\n",
    "        # Test set\n",
    "        composition_predictions_test = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"commonsense_qa_test_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_test.columns:\n",
    "                composition_predictions_test[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_test[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            # Since we have duplicates, we need to sort them first, then merge them (cannot use\n",
    "            # df.merge properly)\n",
    "            df_sorted = df.sort_values(by=\"post_id\")\n",
    "            composition_predictions_test = composition_predictions_test.sort_values(\n",
    "                by=\"post_id\"\n",
    "            )\n",
    "\n",
    "            composition_predictions_test[f\"pred_best_composition_seed{seed}\"] = (\n",
    "                df_sorted[f\"pred_best_composition_seed{seed}\"]\n",
    "            )\n",
    "\n",
    "        commonsense_qa_composition_predictions_test_per_model[\n",
    "            f\"{model}__{training_dataset}\"\n",
    "        ] = composition_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load composition-specific prediction files\n",
    "\n",
    "\n",
    "def reformat_input(text: str) -> str:\n",
    "    question_answer_split = text.split(\"\\n\")\n",
    "    answer_start = question_answer_split[1].find(\"'\")\n",
    "    answer_end = question_answer_split[1].find(\"'\", answer_start + 1)\n",
    "    answer = question_answer_split[1][answer_start + 1 : answer_end]\n",
    "\n",
    "    return f\"[Q] {question_answer_split[0]} [A] {answer}\"\n",
    "\n",
    "\n",
    "commonsense_qa_output_dir = \"outputs/prompt-predictions/commonsense_qa\"\n",
    "commonsense_qa_predictions_per_composition_val_per_model = {}\n",
    "commonsense_qa_predictions_per_composition_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation set\n",
    "    composition_files_val = [\n",
    "        f\n",
    "        for f in sorted(listdir(commonsense_qa_output_dir))\n",
    "        if \"dev\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_val = {}\n",
    "\n",
    "    for f in composition_files_val:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"commonsense_qa-cot-greedy-dev_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(\n",
    "                f\"commonsense_qa-greedy-dev_{model}_\", \"\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "\n",
    "        df = pd.read_parquet(path.join(commonsense_qa_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(\n",
    "            lambda x: hashlib.md5(reformat_input(x).encode()).hexdigest()\n",
    "        )\n",
    "        predictions_per_composition_val[composition_name] = df\n",
    "\n",
    "    commonsense_qa_predictions_per_composition_val_per_model[model] = (\n",
    "        predictions_per_composition_val\n",
    "    )\n",
    "\n",
    "    # Test set\n",
    "    composition_files_test = [\n",
    "        f\n",
    "        for f in sorted(listdir(commonsense_qa_output_dir))\n",
    "        if \"test\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_test = {}\n",
    "\n",
    "    for f in composition_files_test:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"commonsense_qa-cot-greedy-test_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(\n",
    "                f\"commonsense_qa-greedy-test_{model}_\", \"\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "\n",
    "        df = pd.read_parquet(path.join(commonsense_qa_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(\n",
    "            lambda x: hashlib.md5(reformat_input(x).encode()).hexdigest()\n",
    "        )\n",
    "        predictions_per_composition_test[composition_name] = df\n",
    "\n",
    "    commonsense_qa_predictions_per_composition_test_per_model[model] = (\n",
    "        predictions_per_composition_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Training split\")\n",
    "positive_instances_train = len(\n",
    "    commonsense_qa_train_df[commonsense_qa_train_df.true_label == 1]\n",
    ")\n",
    "negative_instances_train = len(\n",
    "    commonsense_qa_train_df[commonsense_qa_train_df.true_label == 0]\n",
    ")\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_train} ({np.round(positive_instances_train / len(commonsense_qa_train_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_train} ({np.round(negative_instances_train / len(commonsense_qa_train_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Validation split\")\n",
    "positive_instances_val = len(\n",
    "    commonsense_qa_val_df[commonsense_qa_val_df.true_label == 1]\n",
    ")\n",
    "negative_instances_val = len(\n",
    "    commonsense_qa_val_df[commonsense_qa_val_df.true_label == 0]\n",
    ")\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_val} ({np.round(positive_instances_val / len(commonsense_qa_val_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_val} ({np.round(negative_instances_val / len(commonsense_qa_val_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Test split\")\n",
    "positive_instances_test = len(\n",
    "    commonsense_qa_test_df[commonsense_qa_test_df.true_label == 1]\n",
    ")\n",
    "negative_instances_test = len(\n",
    "    commonsense_qa_test_df[commonsense_qa_test_df.true_label == 0]\n",
    ")\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_test} ({np.round(positive_instances_test / len(commonsense_qa_test_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_test} ({np.round(negative_instances_test / len(commonsense_qa_test_df), decimals=3)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive prompting evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa_composition_prediction_scores_per_model = {}\n",
    "\n",
    "for model in commonsense_qa_composition_predictions_val_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "    # Validation split\n",
    "    all_seed_scores_val = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in commonsense_qa_composition_predictions_val_per_model[\n",
    "            model\n",
    "        ].iterrows():\n",
    "            preds = commonsense_qa_predictions_per_composition_val_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "\n",
    "            # TODO: REMOVE; ONLY TEMPORARY FIX FOR BROKEN DATA\n",
    "            # (doesn't have any impact on compelete data, though)\n",
    "            try:\n",
    "                if f\"output_{seed}\" in preds.columns:\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                    )\n",
    "                else:\n",
    "                    # If we don't have predictions for other seeds, use the primary seed\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                    )\n",
    "                y_true_seed.append(\n",
    "                    preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "                )\n",
    "            except IndexError:\n",
    "                # print(f\"No post found for id {row.post_id} in predictions. Skipping for now.\")\n",
    "                pass\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_val.append(scores)\n",
    "\n",
    "    # Test split\n",
    "    all_seed_scores_test = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in commonsense_qa_composition_predictions_test_per_model[\n",
    "            model\n",
    "        ].iterrows():\n",
    "            preds = commonsense_qa_predictions_per_composition_test_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "\n",
    "            # TODO: REMOVE; ONLY TEMPORARY FIX FOR BROKEN DATA\n",
    "            # (doesn't have any impact on compelete data, though)\n",
    "            try:\n",
    "                if f\"output_{seed}\" in preds.columns:\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                    )\n",
    "                else:\n",
    "                    # If we don't have predictions for other seeds, use the primary seed\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                    )\n",
    "                y_true_seed.append(\n",
    "                    preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "                )\n",
    "            except IndexError:\n",
    "                # print(f\"No post found for id {row.post_id} in predictions. Skipping for now.\")\n",
    "                pass\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_test.append(scores)\n",
    "\n",
    "    commonsense_qa_composition_prediction_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        commonsense_qa_composition_prediction_scores_per_model[model][\n",
    "            \"test_macro_precision\"\n",
    "        ],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        commonsense_qa_composition_prediction_scores_per_model[model][\n",
    "            \"test_macro_recall\"\n",
    "        ],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        commonsense_qa_composition_prediction_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa_all_scores_val_per_model = {}\n",
    "commonsense_qa_all_scores_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    all_scores_val = {}\n",
    "    for name, predictions in commonsense_qa_predictions_per_composition_val_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_val[name] = scores\n",
    "\n",
    "    commonsense_qa_all_scores_val_per_model[model] = all_scores_val\n",
    "\n",
    "    # Test split\n",
    "    all_scores_test = {}\n",
    "    for name, predictions in commonsense_qa_predictions_per_composition_test_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_test[name] = scores\n",
    "\n",
    "    commonsense_qa_all_scores_test_per_model[model] = all_scores_test\n",
    "\n",
    "commonsense_qa_all_f1_scores_test_per_model = {}\n",
    "commonsense_qa_all_f1_scores_test_per_model_seed_scores = {}\n",
    "for model in MODELS:\n",
    "    commonsense_qa_all_f1_scores_test_per_model[model] = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in commonsense_qa_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "\n",
    "    commonsense_qa_all_f1_scores_test_per_model_seed_scores[model] = [\n",
    "        [((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v]\n",
    "        for k, v in commonsense_qa_all_scores_test_per_model[model].items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_label = commonsense_qa_test_df.true_label.mode()[0]\n",
    "majority_baseline_pred = [majority_label for _ in range(len(commonsense_qa_test_df))]\n",
    "\n",
    "commonsense_qa_maj_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=commonsense_qa_test_df[\"true_label\"],\n",
    "    y_pred=majority_baseline_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "\n",
    "commonsense_qa_maj_baseline_precision_macro_averaged_test = (\n",
    "    commonsense_qa_maj_baseline_scores[0][0] + commonsense_qa_maj_baseline_scores[0][1]\n",
    ") / 2\n",
    "commonsense_qa_maj_baseline_recall_macro_averaged_test = (\n",
    "    commonsense_qa_maj_baseline_scores[1][0] + commonsense_qa_maj_baseline_scores[1][1]\n",
    ") / 2\n",
    "commonsense_qa_maj_baseline_f1_macro_averaged_test = (\n",
    "    commonsense_qa_maj_baseline_scores[2][0] + commonsense_qa_maj_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    commonsense_qa_maj_baseline_precision_macro_averaged_test,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    commonsense_qa_maj_baseline_recall_macro_averaged_test,\n",
    ")\n",
    "print(\n",
    "    \"F1 (macro) (over all seeds):\", commonsense_qa_maj_baseline_f1_macro_averaged_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline_pred = np.random.randint(2, size=len(commonsense_qa_test_df))\n",
    "\n",
    "commonsense_qa_random_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=commonsense_qa_test_df[\"true_label\"],\n",
    "    y_pred=random_baseline_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "\n",
    "commonsense_qa_random_baseline_precision_macro_averaged = (\n",
    "    commonsense_qa_random_baseline_scores[0][0]\n",
    "    + commonsense_qa_random_baseline_scores[0][1]\n",
    ") / 2\n",
    "commonsense_qa_random_baseline_recall_macro_averaged = (\n",
    "    commonsense_qa_random_baseline_scores[1][0]\n",
    "    + commonsense_qa_random_baseline_scores[1][1]\n",
    ") / 2\n",
    "commonsense_qa_random_baseline_f1_macro_averaged = (\n",
    "    commonsense_qa_random_baseline_scores[2][0]\n",
    "    + commonsense_qa_random_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    commonsense_qa_random_baseline_precision_macro_averaged,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    commonsense_qa_random_baseline_recall_macro_averaged,\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", commonsense_qa_random_baseline_f1_macro_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle\n",
    "\n",
    "_Always chooses the correct label, if it was predicted by either of the compositions; only chooses the wrong label if no composition predicted the correct label_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa_oracle_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Compiling oracle predictions\n",
    "    oracle_predictions_test = pd.DataFrame()\n",
    "    for seed in RANDOM_SEED:\n",
    "        all_seed_predictions = pd.DataFrame()\n",
    "        for (\n",
    "            composition,\n",
    "            df,\n",
    "        ) in commonsense_qa_predictions_per_composition_test_per_model[model].items():\n",
    "            if \"input\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"input\"] = df[\"input\"]\n",
    "            if \"true_label\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"true_label\"] = df[\"true_label\"]\n",
    "\n",
    "            try:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[f\"output_{seed}\"]\n",
    "            except KeyError:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        if \"input\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"input\"] = all_seed_predictions[\"input\"]\n",
    "        if \"true_label\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"true_label\"] = all_seed_predictions[\"true_label\"]\n",
    "\n",
    "        # For each sample, choose the true_label if at least one prediction is the true label\n",
    "        # Since the true label is in this dataframe the first value, we check if it exists in all other columns\n",
    "        # If yes, we use the true_label as oracle prediction and otherwise the value of the first column (as\n",
    "        # this should be the wrong label, similar to all other columns)\n",
    "        oracle_predictions_test[f\"output_{seed}\"] = all_seed_predictions.loc[\n",
    "            :, [i for i in all_seed_predictions.columns if i not in [\"input\"]]\n",
    "        ].apply(\n",
    "            lambda row: (\n",
    "                row[\"true_label\"]\n",
    "                if row[\"true_label\"] in row.values[1:]\n",
    "                else row.values[1]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Calculating scores\n",
    "    oracle_seed_scores_test = [\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=oracle_predictions_test[\"true_label\"],\n",
    "            y_pred=oracle_predictions_test[f\"output_{seed}\"],\n",
    "            pos_label=1,\n",
    "        )\n",
    "        for seed in RANDOM_SEED\n",
    "    ]\n",
    "\n",
    "    commonsense_qa_oracle_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Oracle Precision (macro):\",\n",
    "        commonsense_qa_oracle_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Oracle Recall (macro):\",\n",
    "        commonsense_qa_oracle_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Oracle F1 (macro):\",\n",
    "        commonsense_qa_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No technique\n",
    "\n",
    "_Task description and input text only_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa_no_technique_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    no_technique_i = list(commonsense_qa_all_scores_test_per_model[model].keys()).index(\n",
    "        \"task-description-only\"\n",
    "    )\n",
    "\n",
    "    commonsense_qa_no_technique_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"No technique Precision (macro):\",\n",
    "        commonsense_qa_no_technique_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique Recall (macro):\",\n",
    "        commonsense_qa_no_technique_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique F1 (macro):\",\n",
    "        commonsense_qa_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal composition\n",
    "\n",
    "_Best composition on the validation set in terms of f1 macro score, evaluated on the test set for precision, recall and f1_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa_optimal_composition_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    commonsense_qa_optimal_precision_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in commonsense_qa_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    commonsense_qa_optimal_recall_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in commonsense_qa_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    commonsense_qa_optimal_f1_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in commonsense_qa_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    # Test split\n",
    "    commonsense_qa_optimal_precision_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in commonsense_qa_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    commonsense_qa_optimal_recall_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in commonsense_qa_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    commonsense_qa_optimal_f1_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in commonsense_qa_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "\n",
    "    # Find optimal model scores on test set\n",
    "    commonsense_qa_optimal_composition_val_f1_macro_i = np.argmax(\n",
    "        commonsense_qa_optimal_f1_macro_averaged_scores_val\n",
    "    )\n",
    "    commonsense_qa_optimal_composition_name = list(\n",
    "        commonsense_qa_all_scores_val_per_model[model].keys()\n",
    "    )[commonsense_qa_optimal_composition_val_f1_macro_i]\n",
    "\n",
    "    commonsense_qa_optimal_composition_scores_per_model[model] = {\n",
    "        \"composition_name\": list(commonsense_qa_all_scores_val_per_model[model].keys())[\n",
    "            commonsense_qa_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision\": commonsense_qa_optimal_precision_macro_averaged_scores_test[\n",
    "            commonsense_qa_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                commonsense_qa_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": commonsense_qa_optimal_recall_macro_averaged_scores_test[\n",
    "            commonsense_qa_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                commonsense_qa_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": commonsense_qa_optimal_f1_macro_averaged_scores_test[\n",
    "            commonsense_qa_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in commonsense_qa_all_scores_test_per_model[model][\n",
    "                commonsense_qa_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Optimal validation composition:\",\n",
    "        commonsense_qa_optimal_composition_scores_per_model[model][\"composition_name\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Precision (macro):\",\n",
    "        commonsense_qa_optimal_composition_scores_per_model[model][\n",
    "            \"test_macro_precision\"\n",
    "        ],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Recall (macro):\",\n",
    "        commonsense_qa_optimal_composition_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition F1 (macro):\",\n",
    "        commonsense_qa_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa_ensemble_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    all_seed_scores = []\n",
    "\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "\n",
    "        seed_df = pd.DataFrame()\n",
    "\n",
    "        for (\n",
    "            composition_name,\n",
    "            comp_preds,\n",
    "        ) in commonsense_qa_predictions_per_composition_test_per_model[model].items():\n",
    "            if \"input\" not in seed_df.columns:\n",
    "                seed_df[\"input\"] = comp_preds[\"input\"]\n",
    "                seed_df[\"post_id\"] = comp_preds[\"post_id\"]\n",
    "                seed_df[\"true_label\"] = comp_preds[\"true_label\"]\n",
    "            try:\n",
    "                seed_df = pd.merge(\n",
    "                    seed_df,\n",
    "                    comp_preds[[\"post_id\", f\"output_{seed}\"]].rename(\n",
    "                        columns={f\"output_{seed}\": f\"{composition_name}_{seed}\"}\n",
    "                    ),\n",
    "                    on=\"post_id\",\n",
    "                    how=\"left\",\n",
    "                )\n",
    "            except KeyError:\n",
    "                seed_df = pd.merge(\n",
    "                    seed_df,\n",
    "                    comp_preds[[\"post_id\", f\"output_{RANDOM_SEED[0]}\"]].rename(\n",
    "                        columns={\n",
    "                            f\"output_{RANDOM_SEED[0]}\": f\"{composition_name}_{seed}\"\n",
    "                        }\n",
    "                    ),\n",
    "                    on=\"post_id\",\n",
    "                    how=\"left\",\n",
    "                )\n",
    "\n",
    "        mode = seed_df.loc[\n",
    "            :,\n",
    "            [c for c in seed_df.columns if c not in [\"input\", \"post_id\", \"true_label\"]],\n",
    "        ].mode(axis=1)\n",
    "        # If there is a tie, use a random value between 0 and 1\n",
    "        seed_df[\"majority\"] = np.where(mode[1].isna(), mode[0], np.random.randint(2))\n",
    "\n",
    "        y_true_seed = seed_df[\"true_label\"]\n",
    "        y_pred_seed = seed_df[\"majority\"]\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores.append(scores)\n",
    "\n",
    "    commonsense_qa_ensemble_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        commonsense_qa_ensemble_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        commonsense_qa_ensemble_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        commonsense_qa_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "\n",
    "all_seed_scores = []\n",
    "for seed in RANDOM_SEED:\n",
    "    model_name = f\"deberta-v3-large-finetune_20241010181021_commonsense_qa-seed{seed}\"\n",
    "    seed_df = pd.read_parquet(\n",
    "        path.join(\n",
    "            output_dir,\n",
    "            f\"commonsense_qa-test_predictions-{model_name}.parquet\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seed_df = pd.merge(commonsense_qa_test_df, seed_df, on=\"md5_hash\", how=\"left\")\n",
    "\n",
    "    scores = precision_recall_fscore_support(\n",
    "        y_true=seed_df[\"true_label\"],\n",
    "        y_pred=seed_df[f\"prediction_{model_name}\"],\n",
    "        pos_label=1,\n",
    "    )\n",
    "    all_seed_scores.append(scores)\n",
    "\n",
    "commonsense_qa_finetune_scores = {\n",
    "    \"test_macro_precision\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_precision_seed_scores\": [\n",
    "        ((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_recall\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_recall_seed_scores\": [\n",
    "        ((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_f1\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_f1_seed_scores\": [\n",
    "        ((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    commonsense_qa_finetune_scores[\"test_macro_precision\"],\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    commonsense_qa_finetune_scores[\"test_macro_recall\"],\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", commonsense_qa_finetune_scores[\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare best-on-test data\n",
    "commonsense_qa_best_on_test_scores = {}\n",
    "for model in MODELS:\n",
    "    commonsense_qa_best_composition = np.argmax(\n",
    "        commonsense_qa_all_f1_scores_test_per_model[model]\n",
    "    )\n",
    "    commonsense_qa_best_on_test_scores[model] = {\n",
    "        \"test_macro_f1_seed_scores\": commonsense_qa_all_f1_scores_test_per_model_seed_scores[\n",
    "            model\n",
    "        ][\n",
    "            commonsense_qa_best_composition\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "target_dataset_to_evaluate = \"commonsense_qa\"\n",
    "\n",
    "baselines = [\n",
    "    (\"BaseComposition\", commonsense_qa_no_technique_scores_per_model),\n",
    "    (\"BestOnVal\", commonsense_qa_optimal_composition_scores_per_model),\n",
    "    (\"BestOnTest\", commonsense_qa_best_on_test_scores),\n",
    "    (\"Finetune\", commonsense_qa_finetune_scores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    approach_name = \"CompositionPrediction\"\n",
    "    approach_scores = commonsense_qa_composition_prediction_scores_per_model[\n",
    "        f\"{model}__{target_dataset_to_evaluate}\"\n",
    "    ][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "    for baseline_name, baseline_scores_dict in baselines:\n",
    "        if baseline_name == \"Finetune\":\n",
    "            baseline_scores = baseline_scores_dict[\"test_macro_f1_seed_scores\"]\n",
    "        else:\n",
    "            baseline_scores = baseline_scores_dict[model][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "        if not np.mean(approach_scores) > np.mean(baseline_scores):\n",
    "            print(\n",
    "                f\"Skipped {approach_name} vs. {baseline_name} ({np.mean(approach_scores)} vs. {np.mean(baseline_scores)})\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        t_results = ttest_function(baseline_scores, approach_scores)\n",
    "        # correct for one sided test, according to Hitchhiker's guide\n",
    "        p_value = t_results[1] / 2\n",
    "\n",
    "        if p_value <= alpha:\n",
    "            print(\n",
    "                f\"{approach_name} is significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"{approach_name} is NOT significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_ALL_BASELINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "box = ax.boxplot(\n",
    "    [\n",
    "        all_model_scores\n",
    "        for model, all_model_scores in commonsense_qa_all_f1_scores_test_per_model.items()\n",
    "    ],\n",
    "    labels=MODELS,\n",
    ")\n",
    "legend_entities_handlers_per_model = {}\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    print(\"\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"Median:\", np.median(commonsense_qa_all_f1_scores_test_per_model[model]))\n",
    "\n",
    "    best_composition_i = np.argmax(commonsense_qa_all_f1_scores_test_per_model[model])\n",
    "    worst_composition_i = np.argmin(commonsense_qa_all_f1_scores_test_per_model[model])\n",
    "    # Add best score as scatter\n",
    "    best_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        commonsense_qa_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        commonsense_qa_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        # (\n",
    "        #     f\"Best-on-test \"\n",
    "        #     f\"({np.round(commonsense_qa_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"A\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"A ->\",\n",
    "        (\n",
    "            f\"Best-on-test \"\n",
    "            f\"({np.round(commonsense_qa_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add worst score as scatter\n",
    "    worst_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        commonsense_qa_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        commonsense_qa_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        # (\n",
    "        #     f\"Worst-on-test \"\n",
    "        #     f\"({np.round(commonsense_qa_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"B\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"B ->\",\n",
    "        (\n",
    "            f\"Worst-on-test \"\n",
    "            f\"({np.round(commonsense_qa_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add no-technique as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        commonsense_qa_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        commonsense_qa_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\n",
    "        #     f\"Base composition \"\n",
    "        #     f\"({np.round(commonsense_qa_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        # ),\n",
    "        \"C\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"C ->\",\n",
    "        (\n",
    "            f\"Base composition \"\n",
    "            f\"({np.round(commonsense_qa_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add optimal model as scatter\n",
    "    optimal_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        commonsense_qa_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"olive\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        commonsense_qa_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Best-on-validation \" f\"({np.round(commonsense_qa_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"D\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"olive\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"D ->\",\n",
    "        (\n",
    "            \"Best-on-validation \"\n",
    "            f\"({np.round(commonsense_qa_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add naive ensemble as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        commonsense_qa_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        commonsense_qa_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Majority ensemble \" f\"({np.round(commonsense_qa_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"E\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"E ->\",\n",
    "        (\n",
    "            \"Majority ensemble \"\n",
    "            f\"({np.round(commonsense_qa_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add composition predictions as scatters\n",
    "    for k, training_dataset in enumerate(TRAINING_DATASETS):\n",
    "        composition_prediction_score = np.round(\n",
    "            commonsense_qa_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            decimals=3,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            commonsense_qa_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"green\",\n",
    "            marker=\"*\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            commonsense_qa_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            # f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "            f\"F{k}\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"green\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            f\"F{k} ->\",\n",
    "            f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "        )\n",
    "\n",
    "    if SHOW_ALL_BASELINES:\n",
    "        # Add oracle as scatter\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            commonsense_qa_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"black\",\n",
    "            marker=\"x\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            commonsense_qa_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            # (\n",
    "            #     \"Oracle \" f\"({np.round(commonsense_qa_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            # ),\n",
    "            \"G\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"black\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            \"G ->\",\n",
    "            (\n",
    "                \"Oracle \"\n",
    "                f\"({np.round(commonsense_qa_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Always in the following order: best-on-test, worst-on-test, best-on-validation\n",
    "    legend_entities_handlers_per_model[model] = dict(\n",
    "        zip(\n",
    "            [\n",
    "                f\"bot: {list(commonsense_qa_all_scores_test_per_model[model].keys())[best_composition_i]}\",\n",
    "                f\"wot: {list(commonsense_qa_all_scores_test_per_model[model].keys())[worst_composition_i]}\",\n",
    "                f\"bov: {commonsense_qa_optimal_composition_scores_per_model[model]['composition_name']}\",\n",
    "            ],\n",
    "            [\n",
    "                best_composition_handle,\n",
    "                worst_composition_handle,\n",
    "                optimal_composition_handle,\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Add finetune model as scatter\n",
    "plt.axhline(\n",
    "    commonsense_qa_finetune_scores[\"test_macro_f1\"],\n",
    "    color=\"black\",\n",
    "    linestyle=\"dashed\",\n",
    "    alpha=0.4,\n",
    "    zorder=3,\n",
    ")\n",
    "plt.text(\n",
    "    0,\n",
    "    commonsense_qa_finetune_scores[\"test_macro_f1\"],\n",
    "    f\"DeBERTa-v3-large (finetuned) ({np.round(commonsense_qa_finetune_scores['test_macro_f1'], decimals=3)})\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    "    color=\"black\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Conditionally show outlier baselines\n",
    "if SHOW_ALL_BASELINES:\n",
    "    # Add majority label baseline as scatter\n",
    "    plt.axhline(\n",
    "        commonsense_qa_maj_baseline_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        commonsense_qa_maj_baseline_f1_macro_averaged_test,\n",
    "        f\"Majority label baseline ({np.round(commonsense_qa_maj_baseline_f1_macro_averaged_test, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    # Add random baseline as scatter\n",
    "    plt.axhline(\n",
    "        commonsense_qa_random_baseline_f1_macro_averaged,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        commonsense_qa_random_baseline_f1_macro_averaged,\n",
    "        f\"Random baseline ({np.round(commonsense_qa_random_baseline_f1_macro_averaged, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "handlers = []\n",
    "labels = []\n",
    "for model in MODELS:\n",
    "    # Add handlers, with first being dummy handler\n",
    "    handlers.append(\n",
    "        plt.scatter([0], [0], marker=\"None\", linestyle=\"None\", label=f\"dummy-{model}\")\n",
    "    )\n",
    "    handlers.extend(list(legend_entities_handlers_per_model[model].values()))\n",
    "\n",
    "    # Add labels, with first being model label\n",
    "    labels.append(model)\n",
    "    labels.extend(list(legend_entities_handlers_per_model[model].keys()))\n",
    "\n",
    "legend = fig.legend(handlers, labels, ncol=len(MODELS), loc=\"outside lower center\")\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "plt.title(\"commonsense_qa data\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1 (macro) (over all seeds)\")\n",
    "\n",
    "plt.savefig(\"outputs/figures/commonsense_qa__performance-box-plot.pdf\")\n",
    "plt.savefig(\"outputs/figures/commonsense_qa__performance-box-plot.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition frequency\n",
    "\n",
    "_How often was each composition chosen (bar chart with box plot), how often was each technique and combination of technqiues chosen (heatmap)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa_composition_counts_per_seed_per_model = {}\n",
    "\n",
    "for model in commonsense_qa_composition_predictions_test_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "\n",
    "    commonsense_qa_composition_counts_per_seed_per_model[model] = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        comp_count = Counter(\n",
    "            commonsense_qa_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]\n",
    "        )\n",
    "        for composition in commonsense_qa_predictions_per_composition_test_per_model[\n",
    "            model_name_without_data\n",
    "        ]:\n",
    "            if (\n",
    "                composition\n",
    "                not in commonsense_qa_composition_counts_per_seed_per_model[\n",
    "                    model\n",
    "                ].keys()\n",
    "            ):\n",
    "                commonsense_qa_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ] = []\n",
    "\n",
    "            if composition in comp_count.keys():\n",
    "                commonsense_qa_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ].append(comp_count[composition])\n",
    "            else:\n",
    "                commonsense_qa_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ].append(0)\n",
    "\n",
    "    # Calculate bar heights (mean) and error bars (standard deviation)\n",
    "    compositions = list(\n",
    "        commonsense_qa_composition_counts_per_seed_per_model[model].keys()\n",
    "    )\n",
    "    values = [\n",
    "        np.mean(commonsense_qa_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    lower_errors = [\n",
    "        np.mean(commonsense_qa_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.min(commonsense_qa_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    upper_errors = [\n",
    "        np.max(commonsense_qa_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.mean(commonsense_qa_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "\n",
    "    # Combine the data into a list of tuples and sort by values (mean)\n",
    "    sorted_data = sorted(\n",
    "        zip(values, lower_errors, upper_errors, compositions),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=False,\n",
    "    )\n",
    "\n",
    "    # Unpack the sorted data\n",
    "    values, lower_errors, upper_errors, compositions = zip(*sorted_data)\n",
    "\n",
    "    # Create asymmetric error arrays\n",
    "    asymmetric_errors = [lower_errors, upper_errors]\n",
    "\n",
    "    # Bar chart positions\n",
    "    x_pos = np.arange(len(compositions))\n",
    "\n",
    "    # Plot bars\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    # bars = plt.bar(x_pos, values, yerr=asymmetric_errors, align=\"center\", alpha=0.7, capsize=0)\n",
    "    bars = plt.barh(x_pos, values, align=\"center\", alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.yticks(x_pos, compositions, ha=\"right\")\n",
    "    plt.ylabel(\"Compositions\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.title(f\"{model}: Composition counts (over five random seeds) on commonsense_qa\")\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/commonsense_qa__{model}__composition-frequency.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/commonsense_qa__{model}__composition-frequency.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composition frequency latex tables\n",
    "target_dataset = \"commonsense_qa\"\n",
    "target_dataset_models = [\n",
    "    m\n",
    "    for m in commonsense_qa_composition_counts_per_seed_per_model.keys()\n",
    "    if m.endswith(target_dataset)\n",
    "]\n",
    "\n",
    "composition_counts_mean_per_target_dataset_models = {}\n",
    "\n",
    "for model in target_dataset_models:\n",
    "    for (\n",
    "        composition_name,\n",
    "        composition_counts,\n",
    "    ) in commonsense_qa_composition_counts_per_seed_per_model[model].items():\n",
    "        mean_counts = np.mean(composition_counts)\n",
    "\n",
    "        if (\n",
    "            composition_name\n",
    "            not in composition_counts_mean_per_target_dataset_models.keys()\n",
    "        ):\n",
    "            composition_counts_mean_per_target_dataset_models[composition_name] = {}\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            model\n",
    "        ] = mean_counts\n",
    "\n",
    "\n",
    "# Make composition names nicer for final table\n",
    "counts_with_updated_composition_names = {}\n",
    "for (\n",
    "    composition_name,\n",
    "    composition_counts,\n",
    ") in composition_counts_mean_per_target_dataset_models.items():\n",
    "    composition_name_reformat_rules = {\n",
    "        \"cot\": \"Reasoning steps\",\n",
    "        \"category-few-shot\": \"In-context (category)\",\n",
    "        \"random-few-shot\": \"In-context (random)\",\n",
    "        \"similar-few-shot\": \"In-context (similar)\",\n",
    "        \"definitions\": \"Definitions\",\n",
    "        \"directional-stimulus\": \"Dir. stimulus\",\n",
    "        \"system-prompts\": \"Persona\",\n",
    "        \"task-description-only\": \"Base composition\",\n",
    "    }\n",
    "    composition_name_reformat = \", \".join(\n",
    "        [\n",
    "            composition_name_reformat_rules[comp].capitalize()\n",
    "            for comp in composition_name.split(\"_\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    counts_with_updated_composition_names[composition_name_reformat] = (\n",
    "        composition_counts\n",
    "    )\n",
    "\n",
    "composition_frequency_output_file = path.join(\n",
    "    f\"outputs/tables/composition-frequencies-{target_dataset}.csv\"\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=counts_with_updated_composition_names\n",
    ").transpose().sort_index().to_csv(composition_frequency_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = [\n",
    "    # \"cateogory-few-shot\",\n",
    "    \"cot\",\n",
    "    \"definitions\",\n",
    "    \"directional-stimulus\",\n",
    "    \"random-few-shot\",\n",
    "    \"similar-few-shot\",\n",
    "    \"system-prompts\",\n",
    "    \"task-description-only\",\n",
    "]\n",
    "for model in commonsense_qa_composition_predictions_test_per_model.keys():\n",
    "    all_cooccurrences = []\n",
    "    for t_outer in techniques:\n",
    "        t_cooccurrences = []\n",
    "        for seed in RANDOM_SEED:\n",
    "            seed_cooccurrences = np.zeros(len(techniques))\n",
    "            for pred in commonsense_qa_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]:\n",
    "                if t_outer in pred:\n",
    "                    for t_inner in techniques:\n",
    "                        if t_inner in pred and t_inner != t_outer:\n",
    "                            seed_cooccurrences[techniques.index(t_inner)] += 1\n",
    "            t_cooccurrences.append(seed_cooccurrences)\n",
    "        all_cooccurrences.append(t_cooccurrences)\n",
    "\n",
    "    average_cooccurrences = np.array(\n",
    "        [\n",
    "            [np.mean(coocc) for coocc in list(zip(*per_seed_occurrences))]\n",
    "            for per_seed_occurrences in all_cooccurrences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # mask = np.triu(np.ones_like(average_cooccurrences, dtype=bool))\n",
    "    # masked_data = np.ma.masked_array(average_cooccurrences, mask)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(average_cooccurrences, cmap=\"plasma\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(techniques)), labels=techniques)\n",
    "    ax.set_yticks(np.arange(len(techniques)), labels=techniques)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(techniques)):\n",
    "        for j in range(len(techniques)):\n",
    "            text = ax.text(\n",
    "                j, i, average_cooccurrences[i, j], ha=\"center\", va=\"center\", color=\"w\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{model}: Average (over all seeds) cooccurrence for predicted compositions on commonsense_qa\"\n",
    "    )\n",
    "    plt.savefig(f\"outputs/figures/commonsense_qa__{model}__technique-cooccurrences.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESNLI corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"mistral-7b-instruct-v2\",\n",
    "    \"command-r-v01\",\n",
    "    \"llama3-70b-instruct\",\n",
    "]\n",
    "\n",
    "TRAINING_DATASETS = [\"esnli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(datasets_to_load=[\"esnli\"])\n",
    "esnli_train_df = data_handler.esnli_data[\"train\"].rename(\n",
    "    columns={\"label\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "esnli_val_df = data_handler.esnli_data[\"dev\"].rename(columns={\"label\": \"true_label\"})[\n",
    "    [\"md5_hash\", \"true_label\"]\n",
    "]\n",
    "esnli_test_df = data_handler.esnli_data[\"test\"].rename(columns={\"label\": \"true_label\"})[\n",
    "    [\"md5_hash\", \"true_label\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_composition_predictions_val_per_model = {}\n",
    "esnli_composition_predictions_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    for training_dataset in TRAINING_DATASETS:\n",
    "        # Validation set\n",
    "        composition_predictions_val = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"esnli_val_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_val.columns:\n",
    "                composition_predictions_val[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_val[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            composition_predictions_val = pd.merge(\n",
    "                composition_predictions_val,\n",
    "                df.loc[\n",
    "                    :,\n",
    "                    [\n",
    "                        i\n",
    "                        for i in df.columns\n",
    "                        if i not in [\"input\", \"index\", \"pred_probabilities\"]\n",
    "                    ],\n",
    "                ],\n",
    "                on=\"post_id\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        esnli_composition_predictions_val_per_model[f\"{model}__{training_dataset}\"] = (\n",
    "            composition_predictions_val\n",
    "        )\n",
    "\n",
    "        # Test set\n",
    "        composition_predictions_test = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"esnli_test_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_test.columns:\n",
    "                composition_predictions_test[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_test[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            # Since we have duplicates, we need to sort them first, then merge them (cannot use\n",
    "            # df.merge properly)\n",
    "            df_sorted = df.sort_values(by=\"post_id\")\n",
    "            composition_predictions_test = composition_predictions_test.sort_values(\n",
    "                by=\"post_id\"\n",
    "            )\n",
    "\n",
    "            composition_predictions_test[f\"pred_best_composition_seed{seed}\"] = (\n",
    "                df_sorted[f\"pred_best_composition_seed{seed}\"]\n",
    "            )\n",
    "\n",
    "        esnli_composition_predictions_test_per_model[f\"{model}__{training_dataset}\"] = (\n",
    "            composition_predictions_test\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load composition-specific prediction files\n",
    "esnli_output_dir = \"outputs/prompt-predictions/esnli\"\n",
    "esnli_predictions_per_composition_val_per_model = {}\n",
    "esnli_predictions_per_composition_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation set\n",
    "    composition_files_val = [\n",
    "        f for f in sorted(listdir(esnli_output_dir)) if \"dev\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_val = {}\n",
    "\n",
    "    for f in composition_files_val:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"esnli-cot-greedy-dev_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(f\"esnli-greedy-dev_{model}_\", \"\").replace(\n",
    "                \".parquet\", \"\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(path.join(esnli_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_val[composition_name] = df\n",
    "\n",
    "    esnli_predictions_per_composition_val_per_model[model] = (\n",
    "        predictions_per_composition_val\n",
    "    )\n",
    "\n",
    "    # Test set\n",
    "    composition_files_test = [\n",
    "        f for f in sorted(listdir(esnli_output_dir)) if \"test\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_test = {}\n",
    "\n",
    "    for f in composition_files_test:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"esnli-cot-greedy-test_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(f\"esnli-greedy-test_{model}_\", \"\").replace(\n",
    "                \".parquet\", \"\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(path.join(esnli_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_test[composition_name] = df\n",
    "\n",
    "    esnli_predictions_per_composition_test_per_model[model] = (\n",
    "        predictions_per_composition_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Training split\")\n",
    "positive_instances_train = len(esnli_train_df[esnli_train_df.true_label == 1])\n",
    "negative_instances_train = len(esnli_train_df[esnli_train_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_train} ({np.round(positive_instances_train / len(esnli_train_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_train} ({np.round(negative_instances_train / len(esnli_train_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Validation split\")\n",
    "positive_instances_val = len(esnli_val_df[esnli_val_df.true_label == 1])\n",
    "negative_instances_val = len(esnli_val_df[esnli_val_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_val} ({np.round(positive_instances_val / len(esnli_val_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_val} ({np.round(negative_instances_val / len(esnli_val_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Test split\")\n",
    "positive_instances_test = len(esnli_test_df[esnli_test_df.true_label == 1])\n",
    "negative_instances_test = len(esnli_test_df[esnli_test_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_test} ({np.round(positive_instances_test / len(esnli_test_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_test} ({np.round(negative_instances_test / len(esnli_test_df), decimals=3)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive prediction evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_composition_prediction_scores_per_model = {}\n",
    "\n",
    "for model in esnli_composition_predictions_val_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "    # Validation split\n",
    "    all_seed_scores_val = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in esnli_composition_predictions_val_per_model[model].iterrows():\n",
    "            preds = esnli_predictions_per_composition_val_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "            if f\"output_{seed}\" in preds.columns:\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                )\n",
    "            else:\n",
    "                # If we don't have predictions for other seeds, use the primary seed\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                )\n",
    "            y_true_seed.append(\n",
    "                preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "            )\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_val.append(scores)\n",
    "\n",
    "    # Test split\n",
    "    all_seed_scores_test = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in esnli_composition_predictions_test_per_model[model].iterrows():\n",
    "            preds = esnli_predictions_per_composition_test_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "\n",
    "            try:\n",
    "                if f\"output_{seed}\" in preds.columns:\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                    )\n",
    "                else:\n",
    "                    # If we don't have predictions for other seeds, use the primary seed\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                    )\n",
    "                y_true_seed.append(\n",
    "                    preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "                )\n",
    "            except IndexError:\n",
    "                # print(f\"No post found for id {row.post_id} in predictions. Skipping for now.\")\n",
    "                pass\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_test.append(scores)\n",
    "\n",
    "    esnli_composition_prediction_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        esnli_composition_prediction_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        esnli_composition_prediction_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        esnli_composition_prediction_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_all_scores_val_per_model = {}\n",
    "esnli_all_scores_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    all_scores_val = {}\n",
    "    for name, predictions in esnli_predictions_per_composition_val_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_val[name] = scores\n",
    "\n",
    "    esnli_all_scores_val_per_model[model] = all_scores_val\n",
    "\n",
    "    # Test split\n",
    "    all_scores_test = {}\n",
    "    for name, predictions in esnli_predictions_per_composition_test_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_test[name] = scores\n",
    "\n",
    "    esnli_all_scores_test_per_model[model] = all_scores_test\n",
    "\n",
    "esnli_all_f1_scores_test_per_model = {}\n",
    "esnli_all_f1_scores_test_per_model_seed_scores = {}\n",
    "for model in MODELS:\n",
    "    esnli_all_f1_scores_test_per_model[model] = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in esnli_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    esnli_all_f1_scores_test_per_model_seed_scores[model] = [\n",
    "        [((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v]\n",
    "        for k, v in esnli_all_scores_test_per_model[model].items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_label = esnli_test_df.true_label.mode()[0]\n",
    "majority_baseline_pred = [majority_label for _ in range(len(esnli_test_df))]\n",
    "\n",
    "esnli_maj_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=esnli_test_df[\"true_label\"], y_pred=majority_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "esnli_maj_baseline_precision_macro_averaged_test = (\n",
    "    esnli_maj_baseline_scores[0][0] + esnli_maj_baseline_scores[0][1]\n",
    ") / 2\n",
    "esnli_maj_baseline_recall_macro_averaged_test = (\n",
    "    esnli_maj_baseline_scores[1][0] + esnli_maj_baseline_scores[1][1]\n",
    ") / 2\n",
    "esnli_maj_baseline_f1_macro_averaged_test = (\n",
    "    esnli_maj_baseline_scores[2][0] + esnli_maj_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    esnli_maj_baseline_precision_macro_averaged_test,\n",
    ")\n",
    "print(\"Recall (macro) (over all seeds):\", esnli_maj_baseline_recall_macro_averaged_test)\n",
    "print(\"F1 (macro) (over all seeds):\", esnli_maj_baseline_f1_macro_averaged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline_pred = np.random.randint(2, size=len(esnli_test_df))\n",
    "\n",
    "esnli_random_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=esnli_test_df[\"true_label\"], y_pred=random_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "esnli_random_baseline_precision_macro_averaged = (\n",
    "    esnli_random_baseline_scores[0][0] + esnli_random_baseline_scores[0][1]\n",
    ") / 2\n",
    "esnli_random_baseline_recall_macro_averaged = (\n",
    "    esnli_random_baseline_scores[1][0] + esnli_random_baseline_scores[1][1]\n",
    ") / 2\n",
    "esnli_random_baseline_f1_macro_averaged = (\n",
    "    esnli_random_baseline_scores[2][0] + esnli_random_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    esnli_random_baseline_precision_macro_averaged,\n",
    ")\n",
    "print(\"Recall (macro) (over all seeds):\", esnli_random_baseline_recall_macro_averaged)\n",
    "print(\"F1 (macro) (over all seeds):\", esnli_random_baseline_f1_macro_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle\n",
    "\n",
    "_Always chooses the correct label, if it was predicted by either of the compositions; only chooses the wrong label if no composition predicted the correct label_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_oracle_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Compiling oracle predictions\n",
    "    oracle_predictions_test = pd.DataFrame()\n",
    "    for seed in RANDOM_SEED:\n",
    "        all_seed_predictions = pd.DataFrame()\n",
    "        for composition, df in esnli_predictions_per_composition_test_per_model[\n",
    "            model\n",
    "        ].items():\n",
    "            if \"input\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"input\"] = df[\"input\"]\n",
    "            if \"true_label\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"true_label\"] = df[\"true_label\"]\n",
    "\n",
    "            try:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[f\"output_{seed}\"]\n",
    "            except KeyError:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        if \"input\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"input\"] = all_seed_predictions[\"input\"]\n",
    "        if \"true_label\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"true_label\"] = all_seed_predictions[\"true_label\"]\n",
    "\n",
    "        # For each sample, choose the true_label if at least one prediction is the true label\n",
    "        # Since the true label is in this dataframe the first value, we check if it exists in all other columns\n",
    "        # If yes, we use the true_label as oracle prediction and otherwise the value of the first column (as\n",
    "        # this should be the wrong label, similar to all other columns)\n",
    "        oracle_predictions_test[f\"output_{seed}\"] = all_seed_predictions.loc[\n",
    "            :, [i for i in all_seed_predictions.columns if i not in [\"input\"]]\n",
    "        ].apply(\n",
    "            lambda row: (\n",
    "                row[\"true_label\"]\n",
    "                if row[\"true_label\"] in row.values[1:]\n",
    "                else row.values[1]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Calculating scores\n",
    "    oracle_seed_scores_test = [\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=oracle_predictions_test[\"true_label\"],\n",
    "            y_pred=oracle_predictions_test[f\"output_{seed}\"],\n",
    "            pos_label=1,\n",
    "        )\n",
    "        for seed in RANDOM_SEED\n",
    "    ]\n",
    "\n",
    "    esnli_oracle_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Oracle Precision (macro):\",\n",
    "        esnli_oracle_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Oracle Recall (macro):\",\n",
    "        esnli_oracle_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\"Oracle F1 (macro):\", esnli_oracle_scores_per_model[model][\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No technique\n",
    "\n",
    "_Task description and input text only_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_no_technique_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    no_technique_i = list(esnli_all_scores_test_per_model[model].keys()).index(\n",
    "        \"task-description-only\"\n",
    "    )\n",
    "\n",
    "    esnli_no_technique_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"No technique Precision (macro):\",\n",
    "        esnli_no_technique_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique Recall (macro):\",\n",
    "        esnli_no_technique_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique F1 (macro):\",\n",
    "        esnli_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal composition\n",
    "\n",
    "_Best composition on the validation set in terms of f1 macro score, evaluated on the test set for precision, recall and f1_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_optimal_composition_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    esnli_optimal_precision_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in esnli_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    esnli_optimal_recall_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in esnli_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    esnli_optimal_f1_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in esnli_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    # Test split\n",
    "    esnli_optimal_precision_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in esnli_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    esnli_optimal_recall_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in esnli_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    esnli_optimal_f1_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in esnli_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "\n",
    "    # Find optimal model scores on test set\n",
    "    esnli_optimal_composition_val_f1_macro_i = np.argmax(\n",
    "        esnli_optimal_f1_macro_averaged_scores_val\n",
    "    )\n",
    "    esnli_optimal_composition_name = list(esnli_all_scores_val_per_model[model].keys())[\n",
    "        esnli_optimal_composition_val_f1_macro_i\n",
    "    ]\n",
    "\n",
    "    esnli_optimal_composition_scores_per_model[model] = {\n",
    "        \"composition_name\": list(esnli_all_scores_val_per_model[model].keys())[\n",
    "            esnli_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision\": esnli_optimal_precision_macro_averaged_scores_test[\n",
    "            esnli_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                esnli_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": esnli_optimal_recall_macro_averaged_scores_test[\n",
    "            esnli_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                esnli_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": esnli_optimal_f1_macro_averaged_scores_test[\n",
    "            esnli_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in esnli_all_scores_test_per_model[model][\n",
    "                esnli_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Optimal validation composition:\",\n",
    "        esnli_optimal_composition_scores_per_model[model][\"composition_name\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Precision (macro):\",\n",
    "        esnli_optimal_composition_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Recall (macro):\",\n",
    "        esnli_optimal_composition_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition F1 (macro):\",\n",
    "        esnli_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_ensemble_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    all_seed_scores = []\n",
    "\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "\n",
    "        seed_df = pd.DataFrame()\n",
    "\n",
    "        for (\n",
    "            composition_name,\n",
    "            comp_preds,\n",
    "        ) in esnli_predictions_per_composition_test_per_model[model].items():\n",
    "            # print(composition_name)\n",
    "            if \"input\" not in seed_df.columns:\n",
    "                seed_df[\"input\"] = comp_preds[\"input\"]\n",
    "                seed_df[\"post_id\"] = comp_preds[\"post_id\"]\n",
    "                seed_df[\"true_label\"] = comp_preds[\"true_label\"]\n",
    "            # print(\"merging\")\n",
    "\n",
    "            # We need to work on sorted dictionaries, due to duplicates...\n",
    "            comp_preds_sorted = comp_preds.sort_values(by=\"post_id\")\n",
    "            seed_df = seed_df.sort_values(by=\"post_id\")\n",
    "\n",
    "            try:\n",
    "                seed_df[f\"{composition_name}_{seed}\"] = comp_preds_sorted[\n",
    "                    f\"output_{seed}\"\n",
    "                ]\n",
    "            except KeyError:\n",
    "                seed_df[f\"{composition_name}_{seed}\"] = comp_preds_sorted[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        mode = seed_df.loc[\n",
    "            :,\n",
    "            [c for c in seed_df.columns if c not in [\"input\", \"post_id\", \"true_label\"]],\n",
    "        ].mode(axis=1)\n",
    "        # If there are no ties, we mode[1] does not exist\n",
    "        if len(mode.columns) == 1:\n",
    "            # No tie exists\n",
    "            seed_df[\"majority\"] = mode[0]\n",
    "        else:\n",
    "            # If there is a tie, use a random value between 0 and 1\n",
    "            seed_df[\"majority\"] = np.where(\n",
    "                mode[1].isna(), mode[0], np.random.randint(2)\n",
    "            )\n",
    "\n",
    "        y_true_seed = seed_df[\"true_label\"]\n",
    "        y_pred_seed = seed_df[\"majority\"]\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores.append(scores)\n",
    "\n",
    "    esnli_ensemble_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        esnli_ensemble_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        esnli_ensemble_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        esnli_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "\n",
    "all_seed_scores = []\n",
    "for seed in RANDOM_SEED:\n",
    "    model_name = f\"deberta-v3-large-finetune_20241009160411_esnli-seed{seed}\"\n",
    "    seed_df = pd.read_parquet(\n",
    "        path.join(\n",
    "            output_dir,\n",
    "            f\"esnli-test_predictions-{model_name}.parquet\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seed_df = pd.merge(esnli_test_df, seed_df, on=\"md5_hash\", how=\"left\")\n",
    "\n",
    "    scores = precision_recall_fscore_support(\n",
    "        y_true=seed_df[\"true_label\"],\n",
    "        y_pred=seed_df[f\"prediction_{model_name}\"],\n",
    "        pos_label=1,\n",
    "    )\n",
    "    all_seed_scores.append(scores)\n",
    "\n",
    "esnli_finetune_scores = {\n",
    "    \"test_macro_precision\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_precision_seed_scores\": [\n",
    "        ((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_recall\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_recall_seed_scores\": [\n",
    "        ((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_f1\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_f1_seed_scores\": [\n",
    "        ((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\", esnli_finetune_scores[\"test_macro_precision\"]\n",
    ")\n",
    "print(\"Recall (macro) (over all seeds):\", esnli_finetune_scores[\"test_macro_recall\"])\n",
    "print(\"F1 (macro) (over all seeds):\", esnli_finetune_scores[\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare best-on-test data\n",
    "esnli_best_on_test_scores = {}\n",
    "for model in MODELS:\n",
    "    esnli_best_composition = np.argmax(esnli_all_f1_scores_test_per_model[model])\n",
    "    esnli_best_on_test_scores[model] = {\n",
    "        \"test_macro_f1_seed_scores\": esnli_all_f1_scores_test_per_model_seed_scores[\n",
    "            model\n",
    "        ][esnli_best_composition]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "target_dataset_to_evaluate = \"esnli\"\n",
    "\n",
    "baselines = [\n",
    "    (\"BaseComposition\", esnli_no_technique_scores_per_model),\n",
    "    (\"BestOnVal\", esnli_optimal_composition_scores_per_model),\n",
    "    (\"BestOnTest\", esnli_best_on_test_scores),\n",
    "    (\"Finetune\", esnli_finetune_scores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    approach_name = \"CompositionPrediction\"\n",
    "    approach_scores = esnli_composition_prediction_scores_per_model[\n",
    "        f\"{model}__{target_dataset_to_evaluate}\"\n",
    "    ][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "    for baseline_name, baseline_scores_dict in baselines:\n",
    "        if baseline_name == \"Finetune\":\n",
    "            baseline_scores = baseline_scores_dict[\"test_macro_f1_seed_scores\"]\n",
    "        else:\n",
    "            baseline_scores = baseline_scores_dict[model][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "        if not np.mean(approach_scores) > np.mean(baseline_scores):\n",
    "            print(\n",
    "                f\"Skipped {approach_name} vs. {baseline_name} ({np.mean(approach_scores)} vs. {np.mean(baseline_scores)})\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        t_results = ttest_function(baseline_scores, approach_scores)\n",
    "        # correct for one sided test, according to Hitchhiker's guide\n",
    "        p_value = t_results[1] / 2\n",
    "\n",
    "        if p_value <= alpha:\n",
    "            print(\n",
    "                f\"{approach_name} is significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"{approach_name} is NOT significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_ALL_BASELINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "box = ax.boxplot(\n",
    "    [\n",
    "        all_model_scores\n",
    "        for model, all_model_scores in esnli_all_f1_scores_test_per_model.items()\n",
    "    ],\n",
    "    labels=MODELS,\n",
    ")\n",
    "legend_entities_handlers_per_model = {}\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    print(\"\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"Median:\", np.median(esnli_all_f1_scores_test_per_model[model]))\n",
    "\n",
    "    best_composition_i = np.argmax(esnli_all_f1_scores_test_per_model[model])\n",
    "    worst_composition_i = np.argmin(esnli_all_f1_scores_test_per_model[model])\n",
    "    # Add best score as scatter\n",
    "    best_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        esnli_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        esnli_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        # (\n",
    "        #     f\"Best-on-test \"\n",
    "        #     f\"({np.round(esnli_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"A\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"A ->\",\n",
    "        (\n",
    "            f\"Best-on-test \"\n",
    "            f\"({np.round(esnli_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add worst score as scatter\n",
    "    worst_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        esnli_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        esnli_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        # (\n",
    "        #     f\"Worst-on-test \"\n",
    "        #     f\"({np.round(esnli_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"B\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"B ->\",\n",
    "        (\n",
    "            f\"Worst-on-test \"\n",
    "            f\"({np.round(esnli_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add no-technique as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        esnli_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        esnli_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\n",
    "        #     f\"Base composition \"\n",
    "        #     f\"({np.round(esnli_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        # ),\n",
    "        \"C\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"C ->\",\n",
    "        (\n",
    "            f\"Base composition \"\n",
    "            f\"({np.round(esnli_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add optimal model as scatter\n",
    "    optimal_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        esnli_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"olive\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        esnli_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Best-on-validation \"f\"({np.round(esnli_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"D\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"olive\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"D ->\",\n",
    "        (\n",
    "            \"Best-on-validation \"\n",
    "            f\"({np.round(esnli_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add naive ensemble as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        esnli_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        esnli_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Majority ensemble \" f\"({np.round(esnli_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"E\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"E -> \",\n",
    "        (\n",
    "            \"Majority ensemble \"\n",
    "            f\"({np.round(esnli_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add composition predictions as scatters\n",
    "    for k, training_dataset in enumerate(TRAINING_DATASETS):\n",
    "        composition_prediction_score = np.round(\n",
    "            esnli_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            decimals=3,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            esnli_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"green\",\n",
    "            marker=\"*\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            esnli_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            # f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "            f\"F{k}\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"green\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            f\"F{k} ->\",\n",
    "            f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "        )\n",
    "\n",
    "    if SHOW_ALL_BASELINES:\n",
    "        # Add oracle as scatter\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            esnli_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"black\",\n",
    "            marker=\"x\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            esnli_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            # (\n",
    "            #     \"Oracle \"\n",
    "            #     f\"({np.round(esnli_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            # ),\n",
    "            \"G\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"black\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            \"G ->\",\n",
    "            (\n",
    "                \"Oracle \"\n",
    "                f\"({np.round(esnli_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Always in the following order: best-on-test, worst-on-test, best-on-validation\n",
    "    legend_entities_handlers_per_model[model] = dict(\n",
    "        zip(\n",
    "            [\n",
    "                f\"bot: {list(esnli_all_scores_test_per_model[model].keys())[best_composition_i]}\",\n",
    "                f\"wot: {list(esnli_all_scores_test_per_model[model].keys())[worst_composition_i]}\",\n",
    "                f\"bov: {esnli_optimal_composition_scores_per_model[model]['composition_name']}\",\n",
    "            ],\n",
    "            [\n",
    "                best_composition_handle,\n",
    "                worst_composition_handle,\n",
    "                optimal_composition_handle,\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Add finetune model as scatter\n",
    "plt.axhline(\n",
    "    esnli_finetune_scores[\"test_macro_f1\"],\n",
    "    color=\"black\",\n",
    "    linestyle=\"dashed\",\n",
    "    alpha=0.4,\n",
    "    zorder=3,\n",
    ")\n",
    "plt.text(\n",
    "    0,\n",
    "    esnli_finetune_scores[\"test_macro_f1\"],\n",
    "    f\"DeBERTa-v3-large (finetuned) ({np.round(esnli_finetune_scores['test_macro_f1'], decimals=3)})\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    "    color=\"black\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Conditionally show outlier baselines\n",
    "if SHOW_ALL_BASELINES:\n",
    "    # Add majority label baseline as scatter\n",
    "    plt.axhline(\n",
    "        esnli_maj_baseline_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        esnli_maj_baseline_f1_macro_averaged_test,\n",
    "        f\"Majority label baseline ({np.round(esnli_maj_baseline_f1_macro_averaged_test, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    # Add random baseline as scatter\n",
    "    plt.axhline(\n",
    "        esnli_random_baseline_f1_macro_averaged,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        esnli_random_baseline_f1_macro_averaged,\n",
    "        f\"Random baseline ({np.round(esnli_random_baseline_f1_macro_averaged, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "handlers = []\n",
    "labels = []\n",
    "for model in MODELS:\n",
    "    # Add handlers, with first being dummy handler\n",
    "    handlers.append(\n",
    "        plt.scatter([0], [0], marker=\"None\", linestyle=\"None\", label=f\"dummy-{model}\")\n",
    "    )\n",
    "    handlers.extend(list(legend_entities_handlers_per_model[model].values()))\n",
    "\n",
    "    # Add labels, with first being model label\n",
    "    labels.append(model)\n",
    "    labels.extend(list(legend_entities_handlers_per_model[model].keys()))\n",
    "\n",
    "legend = fig.legend(handlers, labels, ncol=len(MODELS), loc=\"outside lower center\")\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "plt.title(\"ESNLI data\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1 (macro) (over all seeds)\")\n",
    "plt.savefig(\"outputs/figures/esnli__performance-box-plot.pdf\")\n",
    "plt.savefig(\"outputs/figures/esnli__performance-box-plot.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition frequency\n",
    "\n",
    "_How often was each composition chosen (bar chart with box plot), how often was each technique and combination of technqiues chosen (heatmap)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_composition_counts_per_seed_per_model = {}\n",
    "\n",
    "for model in esnli_composition_predictions_test_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "\n",
    "    esnli_composition_counts_per_seed_per_model[model] = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        comp_count = Counter(\n",
    "            esnli_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]\n",
    "        )\n",
    "        for composition in esnli_predictions_per_composition_test_per_model[\n",
    "            model_name_without_data\n",
    "        ]:\n",
    "            if (\n",
    "                composition\n",
    "                not in esnli_composition_counts_per_seed_per_model[model].keys()\n",
    "            ):\n",
    "                esnli_composition_counts_per_seed_per_model[model][composition] = []\n",
    "\n",
    "            if composition in comp_count.keys():\n",
    "                esnli_composition_counts_per_seed_per_model[model][composition].append(\n",
    "                    comp_count[composition]\n",
    "                )\n",
    "            else:\n",
    "                esnli_composition_counts_per_seed_per_model[model][composition].append(\n",
    "                    0\n",
    "                )\n",
    "\n",
    "    # Calculate bar heights (mean) and error bars (standard deviation)\n",
    "    compositions = list(esnli_composition_counts_per_seed_per_model[model].keys())\n",
    "    values = [\n",
    "        np.mean(esnli_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    lower_errors = [\n",
    "        np.mean(esnli_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.min(esnli_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    upper_errors = [\n",
    "        np.max(esnli_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.mean(esnli_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "\n",
    "    # Combine the data into a list of tuples and sort by values (mean)\n",
    "    sorted_data = sorted(\n",
    "        zip(values, lower_errors, upper_errors, compositions),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=False,\n",
    "    )\n",
    "\n",
    "    # Unpack the sorted data\n",
    "    values, lower_errors, upper_errors, compositions = zip(*sorted_data)\n",
    "\n",
    "    # Create asymmetric error arrays\n",
    "    asymmetric_errors = [lower_errors, upper_errors]\n",
    "\n",
    "    # Bar chart positions\n",
    "    x_pos = np.arange(len(compositions))\n",
    "\n",
    "    # Plot bars\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    # bars = plt.bar(x_pos, values, yerr=asymmetric_errors, align=\"center\", alpha=0.7, capsize=0)\n",
    "    bars = plt.barh(x_pos, values, align=\"center\", alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.yticks(x_pos, compositions, ha=\"right\")\n",
    "    plt.ylabel(\"Compositions\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.title(f\"{model}: Composition counts (over five random seeds) on esnli\")\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/esnli__{model}__composition-frequency.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/esnli__{model}__composition-frequency.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composition frequency latex tables\n",
    "target_dataset = \"esnli\"\n",
    "target_dataset_models = [\n",
    "    m\n",
    "    for m in esnli_composition_counts_per_seed_per_model.keys()\n",
    "    if m.endswith(target_dataset)\n",
    "]\n",
    "\n",
    "composition_counts_mean_per_target_dataset_models = {}\n",
    "\n",
    "for model in target_dataset_models:\n",
    "    for (\n",
    "        composition_name,\n",
    "        composition_counts,\n",
    "    ) in esnli_composition_counts_per_seed_per_model[model].items():\n",
    "        mean_counts = np.mean(composition_counts)\n",
    "\n",
    "        if (\n",
    "            composition_name\n",
    "            not in composition_counts_mean_per_target_dataset_models.keys()\n",
    "        ):\n",
    "            composition_counts_mean_per_target_dataset_models[composition_name] = {}\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            model\n",
    "        ] = mean_counts\n",
    "\n",
    "\n",
    "# Make composition names nicer for final table\n",
    "counts_with_updated_composition_names = {}\n",
    "for (\n",
    "    composition_name,\n",
    "    composition_counts,\n",
    ") in composition_counts_mean_per_target_dataset_models.items():\n",
    "    composition_name_reformat_rules = {\n",
    "        \"cot\": \"Reasoning steps\",\n",
    "        \"category-few-shot\": \"In-context (category)\",\n",
    "        \"random-few-shot\": \"In-context (random)\",\n",
    "        \"similar-few-shot\": \"In-context (similar)\",\n",
    "        \"definitions\": \"Definitions\",\n",
    "        \"directional-stimulus\": \"Dir. stimulus\",\n",
    "        \"system-prompts\": \"Persona\",\n",
    "        \"task-description-only\": \"Base composition\",\n",
    "    }\n",
    "    composition_name_reformat = \", \".join(\n",
    "        [\n",
    "            composition_name_reformat_rules[comp].capitalize()\n",
    "            for comp in composition_name.split(\"_\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    counts_with_updated_composition_names[composition_name_reformat] = (\n",
    "        composition_counts\n",
    "    )\n",
    "\n",
    "composition_frequency_output_file = path.join(\n",
    "    f\"outputs/tables/composition-frequencies-{target_dataset}.csv\"\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=counts_with_updated_composition_names\n",
    ").transpose().sort_index().to_csv(composition_frequency_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = [\n",
    "    \"cateogory-few-shot\",\n",
    "    \"cot\",\n",
    "    \"definitions\",\n",
    "    \"directional-stimulus\",\n",
    "    \"random-few-shot\",\n",
    "    \"similar-few-shot\",\n",
    "    \"system-prompts\",\n",
    "    \"task-description-only\",\n",
    "]\n",
    "for model in esnli_composition_predictions_test_per_model.keys():\n",
    "    all_cooccurrences = []\n",
    "    for t_outer in techniques:\n",
    "        t_cooccurrences = []\n",
    "        for seed in RANDOM_SEED:\n",
    "            seed_cooccurrences = np.zeros(len(techniques))\n",
    "            for pred in esnli_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]:\n",
    "                if t_outer in pred:\n",
    "                    for t_inner in techniques:\n",
    "                        if t_inner in pred and t_inner != t_outer:\n",
    "                            seed_cooccurrences[techniques.index(t_inner)] += 1\n",
    "            t_cooccurrences.append(seed_cooccurrences)\n",
    "        all_cooccurrences.append(t_cooccurrences)\n",
    "\n",
    "    average_cooccurrences = np.array(\n",
    "        [\n",
    "            [np.mean(coocc) for coocc in list(zip(*per_seed_occurrences))]\n",
    "            for per_seed_occurrences in all_cooccurrences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(average_cooccurrences, cmap=\"plasma\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(techniques)), labels=techniques)\n",
    "    ax.set_yticks(np.arange(len(techniques)), labels=techniques)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(techniques)):\n",
    "        for j in range(len(techniques)):\n",
    "            text = ax.text(\n",
    "                j, i, average_cooccurrences[i, j], ha=\"center\", va=\"center\", color=\"w\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{model}: Average (over all seeds) cooccurrence for predicted compositions on ESNLI\"\n",
    "    )\n",
    "    plt.savefig(f\"outputs/figures/esnli__{model}__technique-cooccurrences.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semeval corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"mistral-7b-instruct-v2\",\n",
    "    \"command-r-v01\",\n",
    "    \"llama3-70b-instruct\",\n",
    "]\n",
    "\n",
    "TRAINING_DATASETS = [\"semeval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(datasets_to_load=[\"semeval\"])\n",
    "semeval_train_df = data_handler.semeval_data[\"train\"].rename(\n",
    "    columns={\"label\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "semeval_val_df = data_handler.semeval_data[\"dev\"].rename(\n",
    "    columns={\"label\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "semeval_test_df = data_handler.semeval_data[\"test\"].rename(\n",
    "    columns={\"label\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_composition_predictions_val_per_model = {}\n",
    "semeval_composition_predictions_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    for training_dataset in TRAINING_DATASETS:\n",
    "        # Validation set\n",
    "        composition_predictions_val = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"semeval_val_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_val.columns:\n",
    "                composition_predictions_val[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_val[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            composition_predictions_val = pd.merge(\n",
    "                composition_predictions_val,\n",
    "                df.loc[\n",
    "                    :,\n",
    "                    [\n",
    "                        i\n",
    "                        for i in df.columns\n",
    "                        if i not in [\"input\", \"index\", \"pred_probabilities\"]\n",
    "                    ],\n",
    "                ],\n",
    "                on=\"post_id\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        semeval_composition_predictions_val_per_model[\n",
    "            f\"{model}__{training_dataset}\"\n",
    "        ] = composition_predictions_val\n",
    "\n",
    "        # Test set\n",
    "        composition_predictions_test = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"semeval_test_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_test.columns:\n",
    "                composition_predictions_test[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_test[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            # Since we have duplicates, we need to sort them first, then merge them (cannot use\n",
    "            # df.merge properly)\n",
    "            df_sorted = df.sort_values(by=\"post_id\")\n",
    "            composition_predictions_test = composition_predictions_test.sort_values(\n",
    "                by=\"post_id\"\n",
    "            )\n",
    "\n",
    "            composition_predictions_test[f\"pred_best_composition_seed{seed}\"] = (\n",
    "                df_sorted[f\"pred_best_composition_seed{seed}\"]\n",
    "            )\n",
    "\n",
    "        semeval_composition_predictions_test_per_model[\n",
    "            f\"{model}__{training_dataset}\"\n",
    "        ] = composition_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load composition-specific prediction files\n",
    "semeval_output_dir = \"outputs/prompt-predictions/semeval\"\n",
    "semeval_predictions_per_composition_val_per_model = {}\n",
    "semeval_predictions_per_composition_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation set\n",
    "    composition_files_val = [\n",
    "        f for f in sorted(listdir(semeval_output_dir)) if \"dev\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_val = {}\n",
    "\n",
    "    for f in composition_files_val:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"semeval-cot-greedy-dev_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(f\"semeval-greedy-dev_{model}_\", \"\").replace(\n",
    "                \".parquet\", \"\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(path.join(semeval_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_val[composition_name] = df\n",
    "\n",
    "    semeval_predictions_per_composition_val_per_model[model] = (\n",
    "        predictions_per_composition_val\n",
    "    )\n",
    "\n",
    "    # Test set\n",
    "    composition_files_test = [\n",
    "        f for f in sorted(listdir(semeval_output_dir)) if \"test\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_test = {}\n",
    "\n",
    "    for f in composition_files_test:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"semeval-cot-greedy-test_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(f\"semeval-greedy-test_{model}_\", \"\").replace(\n",
    "                \".parquet\", \"\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(path.join(semeval_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_test[composition_name] = df\n",
    "\n",
    "    semeval_predictions_per_composition_test_per_model[model] = (\n",
    "        predictions_per_composition_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Training split\")\n",
    "positive_instances_train = len(semeval_train_df[semeval_train_df.true_label == 1])\n",
    "negative_instances_train = len(semeval_train_df[semeval_train_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_train} ({np.round(positive_instances_train / len(semeval_train_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_train} ({np.round(negative_instances_train / len(semeval_train_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Validation split\")\n",
    "positive_instances_val = len(semeval_val_df[semeval_val_df.true_label == 1])\n",
    "negative_instances_val = len(semeval_val_df[semeval_val_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_val} ({np.round(positive_instances_val / len(semeval_val_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_val} ({np.round(negative_instances_val / len(semeval_val_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Test split\")\n",
    "positive_instances_test = len(semeval_test_df[semeval_test_df.true_label == 1])\n",
    "negative_instances_test = len(semeval_test_df[semeval_test_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_test} ({np.round(positive_instances_test / len(semeval_test_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_test} ({np.round(negative_instances_test / len(semeval_test_df), decimals=3)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive prompting evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_composition_prediction_scores_per_model = {}\n",
    "\n",
    "for model in semeval_composition_predictions_val_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "    # Validation split\n",
    "    all_seed_scores_val = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in semeval_composition_predictions_val_per_model[model].iterrows():\n",
    "            preds = semeval_predictions_per_composition_val_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "            if f\"output_{seed}\" in preds.columns:\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                )\n",
    "            else:\n",
    "                # If we don't have predictions for other seeds, use the primary seed\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                )\n",
    "            y_true_seed.append(\n",
    "                preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "            )\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_val.append(scores)\n",
    "\n",
    "    # Test split\n",
    "    all_seed_scores_test = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in semeval_composition_predictions_test_per_model[model].iterrows():\n",
    "            preds = semeval_predictions_per_composition_test_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "\n",
    "            # TODO: REMOVE; ONLY TEMPORARY FIX FOR BROKEN DATA\n",
    "            # (doesn't have any impact on compelete data, though)\n",
    "            try:\n",
    "                if f\"output_{seed}\" in preds.columns:\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                    )\n",
    "                else:\n",
    "                    # If we don't have predictions for other seeds, use the primary seed\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                    )\n",
    "                y_true_seed.append(\n",
    "                    preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "                )\n",
    "            except IndexError:\n",
    "                # print(f\"No post found for id {row.post_id} in predictions. Skipping for now.\")\n",
    "                pass\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_test.append(scores)\n",
    "\n",
    "    semeval_composition_prediction_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        semeval_composition_prediction_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        semeval_composition_prediction_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        semeval_composition_prediction_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_all_scores_val_per_model = {}\n",
    "semeval_all_scores_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    all_scores_val = {}\n",
    "    for name, predictions in semeval_predictions_per_composition_val_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_val[name] = scores\n",
    "\n",
    "    semeval_all_scores_val_per_model[model] = all_scores_val\n",
    "\n",
    "    # Test split\n",
    "    all_scores_test = {}\n",
    "    for name, predictions in semeval_predictions_per_composition_test_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_test[name] = scores\n",
    "\n",
    "    semeval_all_scores_test_per_model[model] = all_scores_test\n",
    "\n",
    "semeval_all_f1_scores_test_per_model = {}\n",
    "semeval_all_f1_scores_test_per_model_seed_scores = {}\n",
    "for model in MODELS:\n",
    "    semeval_all_f1_scores_test_per_model[model] = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in semeval_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    semeval_all_f1_scores_test_per_model_seed_scores[model] = [\n",
    "        [((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v]\n",
    "        for k, v in semeval_all_scores_test_per_model[model].items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_label = semeval_test_df.true_label.mode()[0]\n",
    "majority_baseline_pred = [majority_label for _ in range(len(semeval_test_df))]\n",
    "\n",
    "semeval_maj_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=semeval_test_df[\"true_label\"], y_pred=majority_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "semeval_maj_baseline_precision_macro_averaged_test = (\n",
    "    semeval_maj_baseline_scores[0][0] + semeval_maj_baseline_scores[0][1]\n",
    ") / 2\n",
    "semeval_maj_baseline_recall_macro_averaged_test = (\n",
    "    semeval_maj_baseline_scores[1][0] + semeval_maj_baseline_scores[1][1]\n",
    ") / 2\n",
    "semeval_maj_baseline_f1_macro_averaged_test = (\n",
    "    semeval_maj_baseline_scores[2][0] + semeval_maj_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    semeval_maj_baseline_precision_macro_averaged_test,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\", semeval_maj_baseline_recall_macro_averaged_test\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", semeval_maj_baseline_f1_macro_averaged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline_pred = np.random.randint(2, size=len(semeval_test_df))\n",
    "\n",
    "semeval_random_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=semeval_test_df[\"true_label\"], y_pred=random_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "semeval_random_baseline_precision_macro_averaged = (\n",
    "    semeval_random_baseline_scores[0][0] + semeval_random_baseline_scores[0][1]\n",
    ") / 2\n",
    "semeval_random_baseline_recall_macro_averaged = (\n",
    "    semeval_random_baseline_scores[1][0] + semeval_random_baseline_scores[1][1]\n",
    ") / 2\n",
    "semeval_random_baseline_f1_macro_averaged = (\n",
    "    semeval_random_baseline_scores[2][0] + semeval_random_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    semeval_random_baseline_precision_macro_averaged,\n",
    ")\n",
    "print(\"Recall (macro) (over all seeds):\", semeval_random_baseline_recall_macro_averaged)\n",
    "print(\"F1 (macro) (over all seeds):\", semeval_random_baseline_f1_macro_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle\n",
    "\n",
    "_Always chooses the correct label, if it was predicted by either of the compositions; only chooses the wrong label if no composition predicted the correct label_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_oracle_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Compiling oracle predictions\n",
    "    oracle_predictions_test = pd.DataFrame()\n",
    "    for seed in RANDOM_SEED:\n",
    "        all_seed_predictions = pd.DataFrame()\n",
    "        for composition, df in semeval_predictions_per_composition_test_per_model[\n",
    "            model\n",
    "        ].items():\n",
    "            if \"input\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"input\"] = df[\"input\"]\n",
    "            if \"true_label\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"true_label\"] = df[\"true_label\"]\n",
    "\n",
    "            try:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[f\"output_{seed}\"]\n",
    "            except KeyError:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        if \"input\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"input\"] = all_seed_predictions[\"input\"]\n",
    "        if \"true_label\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"true_label\"] = all_seed_predictions[\"true_label\"]\n",
    "\n",
    "        # For each sample, choose the true_label if at least one prediction is the true label\n",
    "        # Since the true label is in this dataframe the first value, we check if it exists in all other columns\n",
    "        # If yes, we use the true_label as oracle prediction and otherwise the value of the first column (as\n",
    "        # this should be the wrong label, similar to all other columns)\n",
    "        oracle_predictions_test[f\"output_{seed}\"] = all_seed_predictions.loc[\n",
    "            :, [i for i in all_seed_predictions.columns if i not in [\"input\"]]\n",
    "        ].apply(\n",
    "            lambda row: (\n",
    "                row[\"true_label\"]\n",
    "                if row[\"true_label\"] in row.values[1:]\n",
    "                else row.values[1]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Calculating scores\n",
    "    oracle_seed_scores_test = [\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=oracle_predictions_test[\"true_label\"],\n",
    "            y_pred=oracle_predictions_test[f\"output_{seed}\"],\n",
    "            pos_label=1,\n",
    "        )\n",
    "        for seed in RANDOM_SEED\n",
    "    ]\n",
    "\n",
    "    semeval_oracle_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Oracle Precision (macro):\",\n",
    "        semeval_oracle_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Oracle Recall (macro):\",\n",
    "        semeval_oracle_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\"Oracle F1 (macro):\", semeval_oracle_scores_per_model[model][\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No technique\n",
    "\n",
    "_Task description and input text only_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_no_technique_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    no_technique_i = list(semeval_all_scores_test_per_model[model].keys()).index(\n",
    "        \"task-description-only\"\n",
    "    )\n",
    "\n",
    "    semeval_no_technique_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"No technique Precision (macro):\",\n",
    "        semeval_no_technique_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique Recall (macro):\",\n",
    "        semeval_no_technique_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique F1 (macro):\",\n",
    "        semeval_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal composition\n",
    "\n",
    "_Best composition on the validation set in terms of f1 macro score, evaluated on the test set for precision, recall and f1_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_optimal_composition_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    semeval_optimal_precision_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in semeval_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    semeval_optimal_recall_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in semeval_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    semeval_optimal_f1_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in semeval_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    # Test split\n",
    "    semeval_optimal_precision_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in semeval_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    semeval_optimal_recall_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in semeval_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    semeval_optimal_f1_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in semeval_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "\n",
    "    # Find optimal model scores on test set\n",
    "    semeval_optimal_composition_val_f1_macro_i = np.argmax(\n",
    "        semeval_optimal_f1_macro_averaged_scores_val\n",
    "    )\n",
    "    semeval_optimal_composition_name = list(\n",
    "        semeval_all_scores_val_per_model[model].keys()\n",
    "    )[semeval_optimal_composition_val_f1_macro_i]\n",
    "\n",
    "    semeval_optimal_composition_scores_per_model[model] = {\n",
    "        \"composition_name\": list(semeval_all_scores_val_per_model[model].keys())[\n",
    "            semeval_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision\": semeval_optimal_precision_macro_averaged_scores_test[\n",
    "            semeval_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                semeval_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": semeval_optimal_recall_macro_averaged_scores_test[\n",
    "            semeval_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                semeval_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": semeval_optimal_f1_macro_averaged_scores_test[\n",
    "            semeval_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in semeval_all_scores_test_per_model[model][\n",
    "                semeval_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Optimal validation composition:\",\n",
    "        semeval_optimal_composition_scores_per_model[model][\"composition_name\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Precision (macro):\",\n",
    "        semeval_optimal_composition_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Recall (macro):\",\n",
    "        semeval_optimal_composition_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition F1 (macro):\",\n",
    "        semeval_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_ensemble_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    all_seed_scores = []\n",
    "\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "\n",
    "        seed_df = pd.DataFrame()\n",
    "\n",
    "        for (\n",
    "            composition_name,\n",
    "            comp_preds,\n",
    "        ) in semeval_predictions_per_composition_test_per_model[model].items():\n",
    "            # print(composition_name)\n",
    "            if \"input\" not in seed_df.columns:\n",
    "                seed_df[\"input\"] = comp_preds[\"input\"]\n",
    "                seed_df[\"post_id\"] = comp_preds[\"post_id\"]\n",
    "                seed_df[\"true_label\"] = comp_preds[\"true_label\"]\n",
    "\n",
    "            # We need to work on sorted dictionaries, due to duplicates...\n",
    "            comp_preds_sorted = comp_preds.sort_values(by=\"post_id\")\n",
    "            seed_df = seed_df.sort_values(by=\"post_id\")\n",
    "\n",
    "            try:\n",
    "                seed_df[f\"{composition_name}_{seed}\"] = comp_preds_sorted[\n",
    "                    f\"output_{seed}\"\n",
    "                ]\n",
    "            except KeyError:\n",
    "                seed_df[f\"{composition_name}_{seed}\"] = comp_preds_sorted[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        mode = seed_df.loc[\n",
    "            :,\n",
    "            [c for c in seed_df.columns if c not in [\"input\", \"post_id\", \"true_label\"]],\n",
    "        ].mode(axis=1)\n",
    "        # If there are no ties, we mode[1] does not exist\n",
    "        if len(mode.columns) == 1:\n",
    "            # No tie exists\n",
    "            seed_df[\"majority\"] = mode[0]\n",
    "        else:\n",
    "            # If there is a tie, use a random value between 0 and 1\n",
    "            seed_df[\"majority\"] = np.where(\n",
    "                mode[1].isna(), mode[0], np.random.randint(2)\n",
    "            )\n",
    "\n",
    "        y_true_seed = seed_df[\"true_label\"]\n",
    "        y_pred_seed = seed_df[\"majority\"]\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores.append(scores)\n",
    "\n",
    "    semeval_ensemble_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        semeval_ensemble_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        semeval_ensemble_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        semeval_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "\n",
    "all_seed_scores = []\n",
    "for seed in RANDOM_SEED:\n",
    "    model_name = f\"deberta-v3-large-finetune_20240925121914_semeval-seed{seed}\"\n",
    "    seed_df = pd.read_parquet(\n",
    "        path.join(\n",
    "            output_dir,\n",
    "            f\"semeval-test_predictions-{model_name}.parquet\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seed_df = pd.merge(semeval_test_df, seed_df, on=\"md5_hash\", how=\"left\")\n",
    "\n",
    "    scores = precision_recall_fscore_support(\n",
    "        y_true=seed_df[\"true_label\"],\n",
    "        y_pred=seed_df[f\"prediction_{model_name}\"],\n",
    "        pos_label=1,\n",
    "    )\n",
    "    all_seed_scores.append(scores)\n",
    "\n",
    "\n",
    "semeval_finetune_scores = {\n",
    "    \"test_macro_precision\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_precision_seed_scores\": [\n",
    "        ((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_recall\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_recall_seed_scores\": [\n",
    "        ((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_f1\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_f1_seed_scores\": [\n",
    "        ((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    semeval_finetune_scores[\"test_macro_precision\"],\n",
    ")\n",
    "print(\"Recall (macro) (over all seeds):\", semeval_finetune_scores[\"test_macro_recall\"])\n",
    "print(\"F1 (macro) (over all seeds):\", semeval_finetune_scores[\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare best-on-test data\n",
    "semeval_best_on_test_scores = {}\n",
    "for model in MODELS:\n",
    "    semeval_best_composition = np.argmax(semeval_all_f1_scores_test_per_model[model])\n",
    "    semeval_best_on_test_scores[model] = {\n",
    "        \"test_macro_f1_seed_scores\": semeval_all_f1_scores_test_per_model_seed_scores[\n",
    "            model\n",
    "        ][semeval_best_composition]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "target_dataset_to_evaluate = \"semeval\"\n",
    "\n",
    "baselines = [\n",
    "    (\"BaseComposition\", semeval_no_technique_scores_per_model),\n",
    "    (\"BestOnVal\", semeval_optimal_composition_scores_per_model),\n",
    "    (\"BestOnTest\", semeval_best_on_test_scores),\n",
    "    (\"Finetune\", semeval_finetune_scores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    approach_name = \"CompositionPrediction\"\n",
    "    approach_scores = semeval_composition_prediction_scores_per_model[\n",
    "        f\"{model}__{target_dataset_to_evaluate}\"\n",
    "    ][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "    for baseline_name, baseline_scores_dict in baselines:\n",
    "        if baseline_name == \"Finetune\":\n",
    "            baseline_scores = baseline_scores_dict[\"test_macro_f1_seed_scores\"]\n",
    "        else:\n",
    "            baseline_scores = baseline_scores_dict[model][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "        if not np.mean(approach_scores) > np.mean(baseline_scores):\n",
    "            print(\n",
    "                f\"Skipped {approach_name} vs. {baseline_name} ({np.mean(approach_scores)} vs. {np.mean(baseline_scores)})\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        t_results = ttest_function(baseline_scores, approach_scores)\n",
    "        # correct for one sided test, according to Hitchhiker's guide\n",
    "        p_value = t_results[1] / 2\n",
    "\n",
    "        if p_value <= alpha:\n",
    "            print(\n",
    "                f\"{approach_name} is significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"{approach_name} is NOT significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_ALL_BASELINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "box = ax.boxplot(\n",
    "    [\n",
    "        all_model_scores\n",
    "        for model, all_model_scores in semeval_all_f1_scores_test_per_model.items()\n",
    "    ],\n",
    "    labels=MODELS,\n",
    ")\n",
    "legend_entities_handlers_per_model = {}\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    print(\"\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"Median:\", np.median(semeval_all_f1_scores_test_per_model[model]))\n",
    "\n",
    "    best_composition_i = np.argmax(semeval_all_f1_scores_test_per_model[model])\n",
    "    worst_composition_i = np.argmin(semeval_all_f1_scores_test_per_model[model])\n",
    "    # Add best score as scatter\n",
    "    best_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        semeval_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        semeval_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        # (\n",
    "        #     f\"Best-on-test \"\n",
    "        #     f\"({np.round(semeval_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"A\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"A ->\",\n",
    "        (\n",
    "            f\"Best-on-test \"\n",
    "            f\"({np.round(semeval_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add worst score as scatter\n",
    "    worst_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        semeval_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        semeval_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        # (\n",
    "        #     f\"Worst-on-test \"\n",
    "        #     f\"({np.round(semeval_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"B\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"B ->\",\n",
    "        (\n",
    "            f\"Worst-on-test \"\n",
    "            f\"({np.round(semeval_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add no-technique as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        semeval_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        semeval_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\n",
    "        #     f\"Base composition \"\n",
    "        #     f\"({np.round(semeval_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        # ),\n",
    "        \"C\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"C ->\",\n",
    "        (\n",
    "            f\"Base composition \"\n",
    "            f\"({np.round(semeval_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add optimal model as scatter\n",
    "    optimal_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        semeval_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"olive\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        semeval_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Best-on-validation \"f\"({np.round(semeval_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"D\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"olive\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"D ->\",\n",
    "        (\n",
    "            \"Best-on-validation \"\n",
    "            f\"({np.round(semeval_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add naive ensemble as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        semeval_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        semeval_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Majority ensemble \" f\"({np.round(semeval_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"E\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"E -> \",\n",
    "        (\n",
    "            \"Majority ensemble \"\n",
    "            f\"({np.round(semeval_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add composition predictions as scatters\n",
    "    for k, training_dataset in enumerate(TRAINING_DATASETS):\n",
    "        composition_prediction_score = np.round(\n",
    "            semeval_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            decimals=3,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            semeval_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"green\",\n",
    "            marker=\"*\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            semeval_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            # f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "            f\"F{k}\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"green\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            f\"F{k} ->\",\n",
    "            f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "        )\n",
    "\n",
    "    if SHOW_ALL_BASELINES:\n",
    "        # Add oracle as scatter\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            semeval_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"black\",\n",
    "            marker=\"x\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            semeval_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            # (\n",
    "            #     \"Oracle \"\n",
    "            #     f\"({np.round(semeval_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            # ),\n",
    "            \"G\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"black\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            \"G ->\",\n",
    "            (\n",
    "                \"Oracle \"\n",
    "                f\"({np.round(semeval_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Always in the following order: best-on-test, worst-on-test, best-on-validation\n",
    "    legend_entities_handlers_per_model[model] = dict(\n",
    "        zip(\n",
    "            [\n",
    "                f\"bot: {list(semeval_all_scores_test_per_model[model].keys())[best_composition_i]}\",\n",
    "                f\"wot: {list(semeval_all_scores_test_per_model[model].keys())[worst_composition_i]}\",\n",
    "                f\"bov: {semeval_optimal_composition_scores_per_model[model]['composition_name']}\",\n",
    "            ],\n",
    "            [\n",
    "                best_composition_handle,\n",
    "                worst_composition_handle,\n",
    "                optimal_composition_handle,\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Add finetune model as scatter\n",
    "plt.axhline(\n",
    "    semeval_finetune_scores[\"test_macro_f1\"],\n",
    "    color=\"black\",\n",
    "    linestyle=\"dashed\",\n",
    "    alpha=0.4,\n",
    "    zorder=3,\n",
    ")\n",
    "plt.text(\n",
    "    0,\n",
    "    semeval_finetune_scores[\"test_macro_f1\"],\n",
    "    f\"DeBERTa-v3-large (finetuned) ({np.round(semeval_finetune_scores['test_macro_f1'], decimals=3)})\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    "    color=\"black\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Conditionally show outlier baselines\n",
    "if SHOW_ALL_BASELINES:\n",
    "    # Add majority label baseline as scatter\n",
    "    plt.axhline(\n",
    "        semeval_maj_baseline_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        semeval_maj_baseline_f1_macro_averaged_test,\n",
    "        f\"Majority label baseline ({np.round(semeval_maj_baseline_f1_macro_averaged_test, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    # Add random baseline as scatter\n",
    "    plt.axhline(\n",
    "        semeval_random_baseline_f1_macro_averaged,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        semeval_random_baseline_f1_macro_averaged,\n",
    "        f\"Random baseline ({np.round(semeval_random_baseline_f1_macro_averaged, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "handlers = []\n",
    "labels = []\n",
    "for model in MODELS:\n",
    "    # Add handlers, with first being dummy handler\n",
    "    handlers.append(\n",
    "        plt.scatter([0], [0], marker=\"None\", linestyle=\"None\", label=f\"dummy-{model}\")\n",
    "    )\n",
    "    handlers.extend(list(legend_entities_handlers_per_model[model].values()))\n",
    "\n",
    "    # Add labels, with first being model label\n",
    "    labels.append(model)\n",
    "    labels.extend(list(legend_entities_handlers_per_model[model].keys()))\n",
    "\n",
    "legend = fig.legend(handlers, labels, ncol=len(MODELS), loc=\"outside lower center\")\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "plt.title(\"SemEval data\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1 (macro) (over all seeds)\")\n",
    "plt.savefig(\"outputs/figures/semeval__performance-box-plot.pdf\")\n",
    "plt.savefig(\"outputs/figures/semeval__performance-box-plot.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition frequency\n",
    "\n",
    "_How often was each composition chosen (bar chart with box plot), how often was each technique and combination of technqiues chosen (heatmap)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_composition_counts_per_seed_per_model = {}\n",
    "\n",
    "for model in semeval_composition_predictions_test_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "\n",
    "    semeval_composition_counts_per_seed_per_model[model] = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        comp_count = Counter(\n",
    "            semeval_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]\n",
    "        )\n",
    "        for composition in semeval_predictions_per_composition_test_per_model[\n",
    "            model_name_without_data\n",
    "        ]:\n",
    "            if (\n",
    "                composition\n",
    "                not in semeval_composition_counts_per_seed_per_model[model].keys()\n",
    "            ):\n",
    "                semeval_composition_counts_per_seed_per_model[model][composition] = []\n",
    "\n",
    "            if composition in comp_count.keys():\n",
    "                semeval_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ].append(comp_count[composition])\n",
    "            else:\n",
    "                semeval_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ].append(0)\n",
    "\n",
    "    # Calculate bar heights (mean) and error bars (standard deviation)\n",
    "    compositions = list(semeval_composition_counts_per_seed_per_model[model].keys())\n",
    "    values = [\n",
    "        np.mean(semeval_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    lower_errors = [\n",
    "        np.mean(semeval_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.min(semeval_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    upper_errors = [\n",
    "        np.max(semeval_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.mean(semeval_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "\n",
    "    # Combine the data into a list of tuples and sort by values (mean)\n",
    "    sorted_data = sorted(\n",
    "        zip(values, lower_errors, upper_errors, compositions),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=False,\n",
    "    )\n",
    "\n",
    "    # Unpack the sorted data\n",
    "    values, lower_errors, upper_errors, compositions = zip(*sorted_data)\n",
    "\n",
    "    # Create asymmetric error arrays\n",
    "    asymmetric_errors = [lower_errors, upper_errors]\n",
    "\n",
    "    # Bar chart positions\n",
    "    x_pos = np.arange(len(compositions))\n",
    "\n",
    "    # Plot bars\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    # bars = plt.bar(x_pos, values, yerr=asymmetric_errors, align=\"center\", alpha=0.7, capsize=0)\n",
    "    bars = plt.barh(x_pos, values, align=\"center\", alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.yticks(x_pos, compositions, ha=\"right\")\n",
    "    plt.ylabel(\"Compositions\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.title(f\"{model}: Composition counts (over five random seeds) on SemEval\")\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/semeval__{model}__composition-frequency.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/semeval__{model}__composition-frequency.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composition frequency latex tables\n",
    "target_dataset = \"semeval\"\n",
    "target_dataset_models = [\n",
    "    m\n",
    "    for m in semeval_composition_counts_per_seed_per_model.keys()\n",
    "    if m.endswith(target_dataset)\n",
    "]\n",
    "\n",
    "composition_counts_mean_per_target_dataset_models = {}\n",
    "\n",
    "for model in target_dataset_models:\n",
    "    for (\n",
    "        composition_name,\n",
    "        composition_counts,\n",
    "    ) in semeval_composition_counts_per_seed_per_model[model].items():\n",
    "        mean_counts = np.mean(composition_counts)\n",
    "\n",
    "        if (\n",
    "            composition_name\n",
    "            not in composition_counts_mean_per_target_dataset_models.keys()\n",
    "        ):\n",
    "            composition_counts_mean_per_target_dataset_models[composition_name] = {}\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            model\n",
    "        ] = mean_counts\n",
    "\n",
    "\n",
    "# Make composition names nicer for final table\n",
    "counts_with_updated_composition_names = {}\n",
    "for (\n",
    "    composition_name,\n",
    "    composition_counts,\n",
    ") in composition_counts_mean_per_target_dataset_models.items():\n",
    "    composition_name_reformat_rules = {\n",
    "        \"cot\": \"Reasoning steps\",\n",
    "        \"category-few-shot\": \"In-context (category)\",\n",
    "        \"random-few-shot\": \"In-context (random)\",\n",
    "        \"similar-few-shot\": \"In-context (similar)\",\n",
    "        \"definitions\": \"Definitions\",\n",
    "        \"directional-stimulus\": \"Dir. stimulus\",\n",
    "        \"system-prompts\": \"Persona\",\n",
    "        \"task-description-only\": \"Base composition\",\n",
    "    }\n",
    "    composition_name_reformat = \", \".join(\n",
    "        [\n",
    "            composition_name_reformat_rules[comp].capitalize()\n",
    "            for comp in composition_name.split(\"_\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    counts_with_updated_composition_names[composition_name_reformat] = (\n",
    "        composition_counts\n",
    "    )\n",
    "\n",
    "composition_frequency_output_file = path.join(\n",
    "    f\"outputs/tables/composition-frequencies-{target_dataset}.csv\"\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=counts_with_updated_composition_names\n",
    ").transpose().sort_index().to_csv(composition_frequency_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = [\n",
    "    \"cateogory-few-shot\",\n",
    "    \"cot\",\n",
    "    \"definitions\",\n",
    "    \"directional-stimulus\",\n",
    "    \"random-few-shot\",\n",
    "    \"similar-few-shot\",\n",
    "    \"system-prompts\",\n",
    "    \"task-description-only\",\n",
    "]\n",
    "for model in semeval_composition_predictions_test_per_model.keys():\n",
    "    all_cooccurrences = []\n",
    "    for t_outer in techniques:\n",
    "        t_cooccurrences = []\n",
    "        for seed in RANDOM_SEED:\n",
    "            seed_cooccurrences = np.zeros(len(techniques))\n",
    "            for pred in semeval_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]:\n",
    "                if t_outer in pred:\n",
    "                    for t_inner in techniques:\n",
    "                        if t_inner in pred and t_inner != t_outer:\n",
    "                            seed_cooccurrences[techniques.index(t_inner)] += 1\n",
    "            t_cooccurrences.append(seed_cooccurrences)\n",
    "        all_cooccurrences.append(t_cooccurrences)\n",
    "\n",
    "    average_cooccurrences = np.array(\n",
    "        [\n",
    "            [np.mean(coocc) for coocc in list(zip(*per_seed_occurrences))]\n",
    "            for per_seed_occurrences in all_cooccurrences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(average_cooccurrences, cmap=\"plasma\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(techniques)), labels=techniques)\n",
    "    ax.set_yticks(np.arange(len(techniques)), labels=techniques)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(techniques)):\n",
    "        for j in range(len(techniques)):\n",
    "            text = ax.text(\n",
    "                j, i, average_cooccurrences[i, j], ha=\"center\", va=\"center\", color=\"w\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{model}: Average (over all seeds) cooccurrence for predicted compositions on SemEval\"\n",
    "    )\n",
    "    plt.savefig(f\"outputs/figures/semeval__{model}__technique-cooccurrences.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cobra frames corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"mistral-7b-instruct-v2\", \"command-r-v01\", \"llama3-70b-instruct\"]\n",
    "\n",
    "TRAINING_DATASETS = [\"sbic\", \"stereoset\", \"cobra_frames\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(datasets_to_load=[\"cobra_frames\"])\n",
    "cobra_frames_train_df = data_handler.cobra_frames[\"train\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\", \"text_hash\": \"md5_hash\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "cobra_frames_val_df = data_handler.cobra_frames[\"dev\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\", \"text_hash\": \"md5_hash\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "cobra_frames_test_df = data_handler.cobra_frames[\"test\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\", \"text_hash\": \"md5_hash\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_frames_composition_predictions_val_per_model = {}\n",
    "cobra_frames_composition_predictions_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    for training_dataset in TRAINING_DATASETS:\n",
    "        # Validation set\n",
    "        composition_predictions_val = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"cobra_frames_val_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_val.columns:\n",
    "                composition_predictions_val[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_val[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            composition_predictions_val = pd.merge(\n",
    "                composition_predictions_val,\n",
    "                df.loc[\n",
    "                    :,\n",
    "                    [\n",
    "                        i\n",
    "                        for i in df.columns\n",
    "                        if i not in [\"input\", \"index\", \"pred_probabilities\"]\n",
    "                    ],\n",
    "                ],\n",
    "                on=\"post_id\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        cobra_frames_composition_predictions_val_per_model[\n",
    "            f\"{model}__{training_dataset}\"\n",
    "        ] = composition_predictions_val\n",
    "\n",
    "        # Test set\n",
    "        composition_predictions_test = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"cobra_frames_test_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_test.columns:\n",
    "                composition_predictions_test[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_test[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            # Since we have duplicates, we need to sort them first, then merge them (cannot use\n",
    "            # df.merge properly)\n",
    "            df_sorted = df.sort_values(by=\"post_id\")\n",
    "            composition_predictions_test = composition_predictions_test.sort_values(\n",
    "                by=\"post_id\"\n",
    "            )\n",
    "\n",
    "            composition_predictions_test[f\"pred_best_composition_seed{seed}\"] = (\n",
    "                df_sorted[f\"pred_best_composition_seed{seed}\"]\n",
    "            )\n",
    "\n",
    "        cobra_frames_composition_predictions_test_per_model[\n",
    "            f\"{model}__{training_dataset}\"\n",
    "        ] = composition_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load composition-specific prediction files\n",
    "cobra_frames_output_dir = \"outputs/prompt-predictions/cobra_frames\"\n",
    "cobra_frames_predictions_per_composition_val_per_model = {}\n",
    "cobra_frames_predictions_per_composition_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation set\n",
    "    composition_files_val = [\n",
    "        f for f in sorted(listdir(cobra_frames_output_dir)) if \"dev\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_val = {}\n",
    "\n",
    "    for f in composition_files_val:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"cobra_frames-cot-greedy-dev_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(\n",
    "                f\"cobra_frames-greedy-dev_{model}_\", \"\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "\n",
    "        df = pd.read_parquet(path.join(cobra_frames_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_val[composition_name] = df\n",
    "\n",
    "    cobra_frames_predictions_per_composition_val_per_model[model] = (\n",
    "        predictions_per_composition_val\n",
    "    )\n",
    "\n",
    "    # Test set\n",
    "    composition_files_test = [\n",
    "        f\n",
    "        for f in sorted(listdir(cobra_frames_output_dir))\n",
    "        if \"test\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_test = {}\n",
    "\n",
    "    for f in composition_files_test:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"cobra_frames-cot-greedy-test_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(\n",
    "                f\"cobra_frames-greedy-test_{model}_\", \"\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "\n",
    "        df = pd.read_parquet(path.join(cobra_frames_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_test[composition_name] = df\n",
    "\n",
    "    cobra_frames_predictions_per_composition_test_per_model[model] = (\n",
    "        predictions_per_composition_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Training split\")\n",
    "positive_instances_train = len(\n",
    "    cobra_frames_train_df[cobra_frames_train_df.true_label == 1]\n",
    ")\n",
    "negative_instances_train = len(\n",
    "    cobra_frames_train_df[cobra_frames_train_df.true_label == 0]\n",
    ")\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_train} ({np.round(positive_instances_train / len(cobra_frames_train_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_train} ({np.round(negative_instances_train / len(cobra_frames_train_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Validation split\")\n",
    "positive_instances_val = len(cobra_frames_val_df[cobra_frames_val_df.true_label == 1])\n",
    "negative_instances_val = len(cobra_frames_val_df[cobra_frames_val_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_val} ({np.round(positive_instances_val / len(cobra_frames_val_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_val} ({np.round(negative_instances_val / len(cobra_frames_val_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Test split\")\n",
    "positive_instances_test = len(\n",
    "    cobra_frames_test_df[cobra_frames_test_df.true_label == 1]\n",
    ")\n",
    "negative_instances_test = len(\n",
    "    cobra_frames_test_df[cobra_frames_test_df.true_label == 0]\n",
    ")\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_test} ({np.round(positive_instances_test / len(cobra_frames_test_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_test} ({np.round(negative_instances_test / len(cobra_frames_test_df), decimals=3)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting compositions performance evaluation\n",
    "\n",
    "_aka. how well can the encoder model predict a composition that is correct_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = \"cobra_frames\"\n",
    "\n",
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    output_dir = path.join(\"outputs/composition-predictions\")\n",
    "    prompt_compositions = cobra_frames_predictions_per_composition_test_per_model[\n",
    "        model\n",
    "    ].keys()\n",
    "\n",
    "    scores_per_seed = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        # Load predictions of the adaptive prompting model for each text instance and composition\n",
    "        seed_dir = list(\n",
    "            filter(\n",
    "                lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{target_data}\"\n",
    "                in x\n",
    "                and f\"seed{seed}\" in x,\n",
    "                sorted(listdir(output_dir)),\n",
    "            )\n",
    "        )[0]\n",
    "        df = pd.read_parquet(\n",
    "            path.join(output_dir, seed_dir, f\"{target_data}_test_results.parquet\")\n",
    "        )\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "\n",
    "        # COBRAFRAMES-specific fix\n",
    "        # Remove posts that are not present in filtered cobraframe dataset above\n",
    "        # (due to a bug in cluster) (should only be five posts)\n",
    "        df = df[\n",
    "            df[\"post_id\"].isin(\n",
    "                cobra_frames_predictions_per_composition_test_per_model[model][\n",
    "                    list(prompt_compositions)[0]\n",
    "                ][\"post_id\"]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Load the id2component map to ensure that the predictions are in the same ordering as above\n",
    "        with open(path.join(output_dir, seed_dir, \"id2component_map.json\"), \"r\") as f:\n",
    "            id2component_map = json.load(f)\n",
    "        component2id_map = {value: int(key) for key, value in id2component_map.items()}\n",
    "\n",
    "        # Re-order the predicted probabilities per text instance according the to id2component map\n",
    "        # loaded above\n",
    "        adaptive_prompting_correctness_per_sample = []\n",
    "        for i, row in df.iterrows():\n",
    "            comp_df = cobra_frames_predictions_per_composition_test_per_model[model][\n",
    "                row[\"pred_best_composition\"]\n",
    "            ]\n",
    "            row_comp_df = comp_df[comp_df[\"post_id\"] == row[\"post_id\"]].iloc[0]\n",
    "            try:\n",
    "                adaptive_prompting_correctness_per_sample.append(\n",
    "                    (row_comp_df[f\"output_{seed}\"] == row_comp_df[\"true_label\"]).astype(\n",
    "                        int\n",
    "                    )\n",
    "                )\n",
    "            except KeyError:\n",
    "                adaptive_prompting_correctness_per_sample.append(\n",
    "                    (row_comp_df[f\"output_23\"] == row_comp_df[\"true_label\"]).astype(int)\n",
    "                )\n",
    "        scores_per_seed[seed] = sum(adaptive_prompting_correctness_per_sample) / len(\n",
    "            adaptive_prompting_correctness_per_sample\n",
    "        )\n",
    "\n",
    "    print(scores_per_seed)\n",
    "    print(np.mean(list(scores_per_seed.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive prompting evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_frames_composition_prediction_scores_per_model = {}\n",
    "\n",
    "for model in cobra_frames_composition_predictions_val_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "    # Validation split\n",
    "    all_seed_scores_val = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in cobra_frames_composition_predictions_val_per_model[\n",
    "            model\n",
    "        ].iterrows():\n",
    "            preds = cobra_frames_predictions_per_composition_val_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "            if f\"output_{seed}\" in preds.columns:\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                )\n",
    "            else:\n",
    "                # If we don't have predictions for other seeds, use the primary seed\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                )\n",
    "            y_true_seed.append(\n",
    "                preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "            )\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_val.append(scores)\n",
    "\n",
    "    # Test split\n",
    "    all_seed_scores_test = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in cobra_frames_composition_predictions_test_per_model[\n",
    "            model\n",
    "        ].iterrows():\n",
    "            preds = cobra_frames_predictions_per_composition_test_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "\n",
    "            # TODO: REMOVE; ONLY TEMPORARY FIX FOR BROKEN DATA\n",
    "            # (doesn't have any impact on compelete data, though)\n",
    "            try:\n",
    "                if f\"output_{seed}\" in preds.columns:\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                    )\n",
    "                else:\n",
    "                    # If we don't have predictions for other seeds, use the primary seed\n",
    "                    y_pred_seed.append(\n",
    "                        (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                    )\n",
    "                y_true_seed.append(\n",
    "                    preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "                )\n",
    "            except IndexError:\n",
    "                # print(f\"No post found for id {row.post_id} in predictions. Skipping for now.\")\n",
    "                pass\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_test.append(scores)\n",
    "\n",
    "    cobra_frames_composition_prediction_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        cobra_frames_composition_prediction_scores_per_model[model][\n",
    "            \"test_macro_precision\"\n",
    "        ],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        cobra_frames_composition_prediction_scores_per_model[model][\n",
    "            \"test_macro_recall\"\n",
    "        ],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        cobra_frames_composition_prediction_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_frames_all_scores_val_per_model = {}\n",
    "cobra_frames_all_scores_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    all_scores_val = {}\n",
    "    for name, predictions in cobra_frames_predictions_per_composition_val_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_val[name] = scores\n",
    "\n",
    "    cobra_frames_all_scores_val_per_model[model] = all_scores_val\n",
    "\n",
    "    # Test split\n",
    "    all_scores_test = {}\n",
    "    for name, predictions in cobra_frames_predictions_per_composition_test_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_test[name] = scores\n",
    "\n",
    "    cobra_frames_all_scores_test_per_model[model] = all_scores_test\n",
    "\n",
    "cobra_frames_all_f1_scores_test_per_model = {}\n",
    "cobra_frames_all_f1_scores_test_per_model_seed_scores = {}\n",
    "for model in MODELS:\n",
    "    cobra_frames_all_f1_scores_test_per_model[model] = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in cobra_frames_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    cobra_frames_all_f1_scores_test_per_model_seed_scores[model] = [\n",
    "        [((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v]\n",
    "        for k, v in cobra_frames_all_scores_test_per_model[model].items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Trivial baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_label = cobra_frames_test_df.true_label.mode()[0]\n",
    "majority_baseline_pred = [majority_label for _ in range(len(cobra_frames_test_df))]\n",
    "\n",
    "cobra_frames_maj_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=cobra_frames_test_df[\"true_label\"],\n",
    "    y_pred=majority_baseline_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "\n",
    "cobra_frames_maj_baseline_precision_macro_averaged_test = (\n",
    "    cobra_frames_maj_baseline_scores[0][0] + cobra_frames_maj_baseline_scores[0][1]\n",
    ") / 2\n",
    "cobra_frames_maj_baseline_recall_macro_averaged_test = (\n",
    "    cobra_frames_maj_baseline_scores[1][0] + cobra_frames_maj_baseline_scores[1][1]\n",
    ") / 2\n",
    "cobra_frames_maj_baseline_f1_macro_averaged_test = (\n",
    "    cobra_frames_maj_baseline_scores[2][0] + cobra_frames_maj_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    cobra_frames_maj_baseline_precision_macro_averaged_test,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    cobra_frames_maj_baseline_recall_macro_averaged_test,\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", cobra_frames_maj_baseline_f1_macro_averaged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline_pred = np.random.randint(2, size=len(cobra_frames_test_df))\n",
    "\n",
    "cobra_frames_random_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=cobra_frames_test_df[\"true_label\"], y_pred=random_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "cobra_frames_random_baseline_precision_macro_averaged = (\n",
    "    cobra_frames_random_baseline_scores[0][0]\n",
    "    + cobra_frames_random_baseline_scores[0][1]\n",
    ") / 2\n",
    "cobra_frames_random_baseline_recall_macro_averaged = (\n",
    "    cobra_frames_random_baseline_scores[1][0]\n",
    "    + cobra_frames_random_baseline_scores[1][1]\n",
    ") / 2\n",
    "cobra_frames_random_baseline_f1_macro_averaged = (\n",
    "    cobra_frames_random_baseline_scores[2][0]\n",
    "    + cobra_frames_random_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    cobra_frames_random_baseline_precision_macro_averaged,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    cobra_frames_random_baseline_recall_macro_averaged,\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", cobra_frames_random_baseline_f1_macro_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Oracle\n",
    "\n",
    "_Always chooses the correct label, if it was predicted by either of the compositions; only chooses the wrong label if no composition predicted the correct label_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_frames_oracle_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Compiling oracle predictions\n",
    "    oracle_predictions_test = pd.DataFrame()\n",
    "    for seed in RANDOM_SEED:\n",
    "        all_seed_predictions = pd.DataFrame()\n",
    "        for composition, df in cobra_frames_predictions_per_composition_test_per_model[\n",
    "            model\n",
    "        ].items():\n",
    "            if \"input\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"input\"] = df[\"input\"]\n",
    "            if \"true_label\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"true_label\"] = df[\"true_label\"]\n",
    "\n",
    "            try:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[f\"output_{seed}\"]\n",
    "            except KeyError:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        if \"input\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"input\"] = all_seed_predictions[\"input\"]\n",
    "        if \"true_label\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"true_label\"] = all_seed_predictions[\"true_label\"]\n",
    "\n",
    "        # For each sample, choose the true_label if at least one prediction is the true label\n",
    "        # Since the true label is in this dataframe the first value, we check if it exists in all other columns\n",
    "        # If yes, we use the true_label as oracle prediction and otherwise the value of the first column (as\n",
    "        # this should be the wrong label, similar to all other columns)\n",
    "        oracle_predictions_test[f\"output_{seed}\"] = all_seed_predictions.loc[\n",
    "            :, [i for i in all_seed_predictions.columns if i not in [\"input\"]]\n",
    "        ].apply(\n",
    "            lambda row: (\n",
    "                row[\"true_label\"]\n",
    "                if row[\"true_label\"] in row.values[1:]\n",
    "                else row.values[1]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Calculating scores\n",
    "    oracle_seed_scores_test = [\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=oracle_predictions_test[\"true_label\"],\n",
    "            y_pred=oracle_predictions_test[f\"output_{seed}\"],\n",
    "            pos_label=1,\n",
    "        )\n",
    "        for seed in RANDOM_SEED\n",
    "    ]\n",
    "\n",
    "    cobra_frames_oracle_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Oracle Precision (macro):\",\n",
    "        cobra_frames_oracle_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Oracle Recall (macro):\",\n",
    "        cobra_frames_oracle_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Oracle F1 (macro):\",\n",
    "        cobra_frames_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### No technique\n",
    "\n",
    "_Task description and input text only_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_frames_no_technique_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    no_technique_i = list(cobra_frames_all_scores_test_per_model[model].keys()).index(\n",
    "        \"task-description-only\"\n",
    "    )\n",
    "\n",
    "    cobra_frames_no_technique_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"No technique Precision (macro):\",\n",
    "        cobra_frames_no_technique_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique Recall (macro):\",\n",
    "        cobra_frames_no_technique_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique F1 (macro):\",\n",
    "        cobra_frames_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Optimal composition\n",
    "\n",
    "_Best composition on the validation set in terms of f1 macro score, evaluated on the test set for precision, recall and f1_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_frames_optimal_composition_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    cobra_frames_optimal_precision_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in cobra_frames_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    cobra_frames_optimal_recall_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in cobra_frames_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    cobra_frames_optimal_f1_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in cobra_frames_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    # Test split\n",
    "    cobra_frames_optimal_precision_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in cobra_frames_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    cobra_frames_optimal_recall_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in cobra_frames_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    cobra_frames_optimal_f1_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in cobra_frames_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "\n",
    "    # Find optimal model scores on test set\n",
    "    cobra_frames_optimal_composition_val_f1_macro_i = np.argmax(\n",
    "        cobra_frames_optimal_f1_macro_averaged_scores_val\n",
    "    )\n",
    "    cobra_frames_optimal_composition_name = list(\n",
    "        cobra_frames_all_scores_val_per_model[model].keys()\n",
    "    )[cobra_frames_optimal_composition_val_f1_macro_i]\n",
    "\n",
    "    cobra_frames_optimal_composition_scores_per_model[model] = {\n",
    "        \"composition_name\": list(cobra_frames_all_scores_val_per_model[model].keys())[\n",
    "            cobra_frames_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision\": cobra_frames_optimal_precision_macro_averaged_scores_test[\n",
    "            cobra_frames_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                cobra_frames_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": cobra_frames_optimal_recall_macro_averaged_scores_test[\n",
    "            cobra_frames_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                cobra_frames_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": cobra_frames_optimal_f1_macro_averaged_scores_test[\n",
    "            cobra_frames_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in cobra_frames_all_scores_test_per_model[model][\n",
    "                cobra_frames_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Optimal validation composition:\",\n",
    "        cobra_frames_optimal_composition_scores_per_model[model][\"composition_name\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Precision (macro):\",\n",
    "        cobra_frames_optimal_composition_scores_per_model[model][\n",
    "            \"test_macro_precision\"\n",
    "        ],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Recall (macro):\",\n",
    "        cobra_frames_optimal_composition_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition F1 (macro):\",\n",
    "        cobra_frames_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_frames_ensemble_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # print(model)\n",
    "    all_seed_scores = []\n",
    "\n",
    "    for seed in RANDOM_SEED:\n",
    "        # print(seed)\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "\n",
    "        seed_df = pd.DataFrame()\n",
    "\n",
    "        for (\n",
    "            composition_name,\n",
    "            comp_preds,\n",
    "        ) in cobra_frames_predictions_per_composition_test_per_model[model].items():\n",
    "            # print(composition_name)\n",
    "            if \"input\" not in seed_df.columns:\n",
    "                seed_df[\"input\"] = comp_preds[\"input\"]\n",
    "                seed_df[\"post_id\"] = comp_preds[\"post_id\"]\n",
    "                seed_df[\"true_label\"] = comp_preds[\"true_label\"]\n",
    "            # print(\"merging\")\n",
    "\n",
    "            # We need to work on sorted dictionaries, due to duplicates...\n",
    "            comp_preds_sorted = comp_preds.sort_values(by=\"post_id\")\n",
    "            seed_df = seed_df.sort_values(by=\"post_id\")\n",
    "\n",
    "            try:\n",
    "                seed_df[f\"{composition_name}_{seed}\"] = comp_preds_sorted[\n",
    "                    f\"output_{seed}\"\n",
    "                ]\n",
    "            except KeyError:\n",
    "                seed_df[f\"{composition_name}_{seed}\"] = comp_preds_sorted[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        # print(\"moding\")\n",
    "        mode = seed_df.loc[\n",
    "            :,\n",
    "            [c for c in seed_df.columns if c not in [\"input\", \"post_id\", \"true_label\"]],\n",
    "        ].mode(axis=1)\n",
    "        # If there is a tie, use a random value between 0 and 1\n",
    "        seed_df[\"majority\"] = np.where(mode[1].isna(), mode[0], np.random.randint(2))\n",
    "\n",
    "        y_true_seed = seed_df[\"true_label\"]\n",
    "        y_pred_seed = seed_df[\"majority\"]\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores.append(scores)\n",
    "\n",
    "    cobra_frames_ensemble_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        cobra_frames_ensemble_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        cobra_frames_ensemble_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        cobra_frames_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "\n",
    "all_seed_scores = []\n",
    "for seed in RANDOM_SEED:\n",
    "    model_name = f\"deberta-v3-large-finetune_20240730162242_cobra_frames-seed{seed}\"\n",
    "    seed_df = pd.read_parquet(\n",
    "        path.join(\n",
    "            output_dir,\n",
    "            f\"cobra_frames-test_predictions-{model_name}.parquet\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seed_df = pd.merge(cobra_frames_test_df, seed_df, on=\"md5_hash\", how=\"left\")\n",
    "\n",
    "    scores = precision_recall_fscore_support(\n",
    "        y_true=seed_df[\"true_label\"],\n",
    "        y_pred=seed_df[f\"prediction_{model_name}\"],\n",
    "        pos_label=1,\n",
    "    )\n",
    "    all_seed_scores.append(scores)\n",
    "\n",
    "cobra_frames_finetune_scores = {\n",
    "    \"test_macro_precision\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_precision_seed_scores\": [\n",
    "        ((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_recall\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_recall_seed_scores\": [\n",
    "        ((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_f1\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_f1_seed_scores\": [\n",
    "        ((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    cobra_frames_finetune_scores[\"test_macro_precision\"],\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    cobra_frames_finetune_scores[\"test_macro_recall\"],\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", cobra_frames_finetune_scores[\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-diagnosis baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "model_name = \"self-diagnosis\"\n",
    "\n",
    "sd_results_df = pd.read_parquet(\n",
    "    path.join(\n",
    "        output_dir, model_name, \"baseline_self_diagnosis_cobra_frames_test.parquet\"\n",
    "    )\n",
    ")\n",
    "\n",
    "sd_scores = precision_recall_fscore_support(\n",
    "    y_true=sd_results_df[\"true_label\"], y_pred=sd_results_df[\"output_23\"], pos_label=1\n",
    ")\n",
    "\n",
    "cobra_frames_self_diagnosis_precision_macro_averaged_test = (\n",
    "    sd_scores[0][0] + sd_scores[0][1]\n",
    ") / 2\n",
    "cobra_frames_self_diagnosis_recall_macro_averaged_test = (\n",
    "    sd_scores[1][0] + sd_scores[1][1]\n",
    ") / 2\n",
    "cobra_frames_self_diagnosis_f1_macro_averaged_test = (\n",
    "    sd_scores[2][0] + sd_scores[2][1]\n",
    ") / 2\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    cobra_frames_self_diagnosis_precision_macro_averaged_test,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    cobra_frames_self_diagnosis_recall_macro_averaged_test,\n",
    ")\n",
    "print(\n",
    "    \"F1 (macro) (over all seeds):\", cobra_frames_self_diagnosis_f1_macro_averaged_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare best-on-test data\n",
    "cobra_frames_best_on_test_scores = {}\n",
    "for model in MODELS:\n",
    "    cobra_frames_best_composition = np.argmax(\n",
    "        cobra_frames_all_f1_scores_test_per_model[model]\n",
    "    )\n",
    "    cobra_frames_best_on_test_scores[model] = {\n",
    "        \"test_macro_f1_seed_scores\": cobra_frames_all_f1_scores_test_per_model_seed_scores[\n",
    "            model\n",
    "        ][\n",
    "            cobra_frames_best_composition\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "target_dataset_to_evaluate = \"cobra_frames\"\n",
    "\n",
    "baselines = [\n",
    "    (\"BaseComposition\", cobra_frames_no_technique_scores_per_model),\n",
    "    (\"BestOnVal\", cobra_frames_optimal_composition_scores_per_model),\n",
    "    (\"BestOnTest\", cobra_frames_best_on_test_scores),\n",
    "    (\"Finetune\", cobra_frames_finetune_scores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    approach_name = \"CompositionPrediction\"\n",
    "    approach_scores = cobra_frames_composition_prediction_scores_per_model[\n",
    "        f\"{model}__{target_dataset_to_evaluate}\"\n",
    "    ][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "    for baseline_name, baseline_scores_dict in baselines:\n",
    "        if baseline_name == \"Finetune\":\n",
    "            baseline_scores = baseline_scores_dict[\"test_macro_f1_seed_scores\"]\n",
    "        else:\n",
    "            baseline_scores = baseline_scores_dict[model][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "        if not np.mean(approach_scores) > np.mean(baseline_scores):\n",
    "            print(\n",
    "                f\"Skipped {approach_name} vs. {baseline_name} ({np.mean(approach_scores)} vs. {np.mean(baseline_scores)})\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        t_results = ttest_function(baseline_scores, approach_scores)\n",
    "        # correct for one sided test, according to Hitchhiker's guide\n",
    "        p_value = t_results[1] / 2\n",
    "\n",
    "        if p_value <= alpha:\n",
    "            print(\n",
    "                f\"{approach_name} is significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"{approach_name} is NOT significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_ALL_BASELINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "box = ax.boxplot(\n",
    "    [\n",
    "        all_model_scores\n",
    "        for model, all_model_scores in cobra_frames_all_f1_scores_test_per_model.items()\n",
    "    ],\n",
    "    labels=MODELS,\n",
    ")\n",
    "legend_entities_handlers_per_model = {}\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    print(\"\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"Median:\", np.median(cobra_frames_all_f1_scores_test_per_model[model]))\n",
    "\n",
    "    best_composition_i = np.argmax(cobra_frames_all_f1_scores_test_per_model[model])\n",
    "    worst_composition_i = np.argmin(cobra_frames_all_f1_scores_test_per_model[model])\n",
    "    # Add best score as scatter\n",
    "    best_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        cobra_frames_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        cobra_frames_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        # (\n",
    "        #     f\"Best-on-test \"\n",
    "        #     f\"({np.round(cobra_frames_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"A\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"A ->\",\n",
    "        (\n",
    "            f\"Best-on-test \"\n",
    "            f\"({np.round(cobra_frames_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add worst score as scatter\n",
    "    worst_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        cobra_frames_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        cobra_frames_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        # (\n",
    "        #     f\"Worst-on-test \"\n",
    "        #     f\"({np.round(cobra_frames_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"B\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"B ->\",\n",
    "        (\n",
    "            f\"Worst-on-test \"\n",
    "            f\"({np.round(cobra_frames_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add no-technique as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        cobra_frames_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        cobra_frames_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\n",
    "        #     f\"Base composition \"\n",
    "        #     f\"({np.round(cobra_frames_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        # ),\n",
    "        \"C\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"C ->\",\n",
    "        (\n",
    "            f\"Base composition \"\n",
    "            f\"({np.round(cobra_frames_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add optimal model as scatter\n",
    "    optimal_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        cobra_frames_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"olive\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        cobra_frames_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Best-on-validation \"f\"({np.round(cobra_frames_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"D\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"olive\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"D ->\",\n",
    "        (\n",
    "            \"Best-on-validation \"\n",
    "            f\"({np.round(cobra_frames_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add naive ensemble as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        cobra_frames_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        cobra_frames_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Majority ensemble \" f\"({np.round(cobra_frames_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"E\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"E -> \",\n",
    "        (\n",
    "            \"Majority ensemble \"\n",
    "            f\"({np.round(cobra_frames_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add composition predictions as scatters\n",
    "    for k, training_dataset in enumerate(TRAINING_DATASETS):\n",
    "        composition_prediction_score = np.round(\n",
    "            cobra_frames_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            decimals=3,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            cobra_frames_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"green\",\n",
    "            marker=\"*\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            cobra_frames_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            # f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "            f\"F{k}\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"green\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            f\"F{k} ->\",\n",
    "            f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "        )\n",
    "\n",
    "    if SHOW_ALL_BASELINES:\n",
    "        # Add oracle as scatter\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            cobra_frames_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"black\",\n",
    "            marker=\"x\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            cobra_frames_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            # (\n",
    "            #     \"Oracle \"\n",
    "            #     f\"({np.round(cobra_frames_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            # ),\n",
    "            \"G\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"black\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            \"G ->\",\n",
    "            (\n",
    "                \"Oracle \"\n",
    "                f\"({np.round(cobra_frames_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Always in the following order: best-on-test, worst-on-test, best-on-validation\n",
    "    legend_entities_handlers_per_model[model] = dict(\n",
    "        zip(\n",
    "            [\n",
    "                f\"bot: {list(cobra_frames_all_scores_test_per_model[model].keys())[best_composition_i]}\",\n",
    "                f\"wot: {list(cobra_frames_all_scores_test_per_model[model].keys())[worst_composition_i]}\",\n",
    "                f\"bov: {cobra_frames_optimal_composition_scores_per_model[model]['composition_name']}\",\n",
    "            ],\n",
    "            [\n",
    "                best_composition_handle,\n",
    "                worst_composition_handle,\n",
    "                optimal_composition_handle,\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Add finetune model as scatter\n",
    "plt.axhline(\n",
    "    cobra_frames_finetune_scores[\"test_macro_f1\"],\n",
    "    color=\"black\",\n",
    "    linestyle=\"dashed\",\n",
    "    alpha=0.4,\n",
    "    zorder=3,\n",
    ")\n",
    "plt.text(\n",
    "    0,\n",
    "    cobra_frames_finetune_scores[\"test_macro_f1\"],\n",
    "    f\"DeBERTa-v3-large (finetuned) ({np.round(cobra_frames_finetune_scores['test_macro_f1'], decimals=3)})\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    "    color=\"black\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Conditionally show outlier baselines\n",
    "if SHOW_ALL_BASELINES:\n",
    "    # Add majority label baseline as scatter\n",
    "    plt.axhline(\n",
    "        cobra_frames_maj_baseline_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        cobra_frames_maj_baseline_f1_macro_averaged_test,\n",
    "        f\"Majority label baseline ({np.round(cobra_frames_maj_baseline_f1_macro_averaged_test, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    # Add random baseline as scatter\n",
    "    plt.axhline(\n",
    "        cobra_frames_random_baseline_f1_macro_averaged,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        cobra_frames_random_baseline_f1_macro_averaged,\n",
    "        f\"Random baseline ({np.round(cobra_frames_random_baseline_f1_macro_averaged, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    # Add self-diagnosis as horizontal line scatter\n",
    "    plt.axhline(\n",
    "        cobra_frames_self_diagnosis_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        cobra_frames_self_diagnosis_f1_macro_averaged_test,\n",
    "        f\"Self-diagnosis baseline ({np.round(cobra_frames_self_diagnosis_f1_macro_averaged_test, decimals=3)})\",\n",
    "        # \"I\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "handlers = []\n",
    "labels = []\n",
    "for model in MODELS:\n",
    "    # Add handlers, with first being dummy handler\n",
    "    handlers.append(\n",
    "        plt.scatter([0], [0], marker=\"None\", linestyle=\"None\", label=f\"dummy-{model}\")\n",
    "    )\n",
    "    handlers.extend(list(legend_entities_handlers_per_model[model].values()))\n",
    "\n",
    "    # Add labels, with first being model label\n",
    "    labels.append(model)\n",
    "    labels.extend(list(legend_entities_handlers_per_model[model].keys()))\n",
    "\n",
    "legend = fig.legend(handlers, labels, ncol=len(MODELS), loc=\"outside lower center\")\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "plt.title(\"Cobra Frames data\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1 (macro) (over all seeds)\")\n",
    "plt.savefig(\"outputs/figures/cobraframes__performance-box-plot.pdf\")\n",
    "plt.savefig(\"outputs/figures/cobraframes__performance-box-plot.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition frequency\n",
    "\n",
    "_How often was each composition chosen (bar chart with box plot), how often was each technique and combination of technqiues chosen (heatmap)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_frames_composition_counts_per_seed_per_model = {}\n",
    "\n",
    "for model in cobra_frames_composition_predictions_test_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "\n",
    "    cobra_frames_composition_counts_per_seed_per_model[model] = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        comp_count = Counter(\n",
    "            cobra_frames_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]\n",
    "        )\n",
    "        for composition in cobra_frames_predictions_per_composition_test_per_model[\n",
    "            model_name_without_data\n",
    "        ]:\n",
    "            if (\n",
    "                composition\n",
    "                not in cobra_frames_composition_counts_per_seed_per_model[model].keys()\n",
    "            ):\n",
    "                cobra_frames_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ] = []\n",
    "\n",
    "            if composition in comp_count.keys():\n",
    "                cobra_frames_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ].append(comp_count[composition])\n",
    "            else:\n",
    "                cobra_frames_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ].append(0)\n",
    "\n",
    "    # Calculate bar heights (mean) and error bars (standard deviation)\n",
    "    compositions = list(\n",
    "        cobra_frames_composition_counts_per_seed_per_model[model].keys()\n",
    "    )\n",
    "    values = [\n",
    "        np.mean(cobra_frames_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    lower_errors = [\n",
    "        np.mean(cobra_frames_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.min(cobra_frames_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    upper_errors = [\n",
    "        np.max(cobra_frames_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.mean(cobra_frames_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "\n",
    "    # Combine the data into a list of tuples and sort by values (mean)\n",
    "    sorted_data = sorted(\n",
    "        zip(values, lower_errors, upper_errors, compositions),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=False,\n",
    "    )\n",
    "\n",
    "    # Unpack the sorted data\n",
    "    values, lower_errors, upper_errors, compositions = zip(*sorted_data)\n",
    "\n",
    "    # Create asymmetric error arrays\n",
    "    asymmetric_errors = [lower_errors, upper_errors]\n",
    "\n",
    "    # Bar chart positions\n",
    "    x_pos = np.arange(len(compositions))\n",
    "\n",
    "    # Plot bars\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    # bars = plt.bar(x_pos, values, yerr=asymmetric_errors, align=\"center\", alpha=0.7, capsize=0)\n",
    "    bars = plt.barh(x_pos, values, align=\"center\", alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.yticks(x_pos, compositions, ha=\"right\")\n",
    "    plt.ylabel(\"Compositions\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.title(f\"{model}: Composition counts (over five random seeds) on CobraFrames\")\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/cobraframes__{model}__composition-frequency.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/cobraframes__{model}__composition-frequency.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each composition produces a correct prediction for each split\n",
    "target_dataset = \"cobra_frames\"\n",
    "\n",
    "cobra_frames_train_correct_prediction_counts_per_seed_per_model = {}\n",
    "cobra_frames_val_correct_prediction_counts_per_seed_per_model = {}\n",
    "cobra_frames_test_correct_prediction_counts_per_seed_per_model = {}\n",
    "\n",
    "# Train dataset (needs slightly different loading)\n",
    "# We use the composition names form the test data loaded further above, but then load the\n",
    "# predictions on the train data below from file\n",
    "for (\n",
    "    model,\n",
    "    composition_predictions,\n",
    ") in cobra_frames_predictions_per_composition_test_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in cobra_frames_train_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        cobra_frames_train_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        # Load composition predictions for trainin dataset\n",
    "        if \"cot\" in composition:\n",
    "            comp_no_cot = composition.replace(\"cot_\", \"\")\n",
    "            model_composition_df_train = pd.read_parquet(\n",
    "                f\"outputs/prompt-predictions/{target_dataset}/{target_dataset}-cot-greedy-train_{model}_{comp_no_cot}.parquet\"\n",
    "            )\n",
    "        else:\n",
    "            model_composition_df_train = pd.read_parquet(\n",
    "                f\"outputs/prompt-predictions/{target_dataset}/{target_dataset}-greedy-train_{model}_{composition}.parquet\"\n",
    "            )\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df_train[\n",
    "                        model_composition_df_train[\"true_label\"]\n",
    "                        == model_composition_df_train[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df_train[\n",
    "                        model_composition_df_train[\"true_label\"]\n",
    "                        == model_composition_df_train[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        cobra_frames_train_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ][composition] = correct_predictions_per_seed\n",
    "\n",
    "# Val dataset\n",
    "for (\n",
    "    model,\n",
    "    composition_predictions,\n",
    ") in cobra_frames_predictions_per_composition_val_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in cobra_frames_val_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        cobra_frames_val_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            model_composition_df = (\n",
    "                cobra_frames_predictions_per_composition_val_per_model[model][\n",
    "                    composition\n",
    "                ]\n",
    "            )\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        cobra_frames_val_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ][composition] = correct_predictions_per_seed\n",
    "\n",
    "# Test dataset\n",
    "for (\n",
    "    model,\n",
    "    composition_predictions,\n",
    ") in cobra_frames_predictions_per_composition_test_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in cobra_frames_test_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        cobra_frames_test_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            model_composition_df = (\n",
    "                cobra_frames_predictions_per_composition_test_per_model[model][\n",
    "                    composition\n",
    "                ]\n",
    "            )\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        cobra_frames_test_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ][composition] = correct_predictions_per_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composition frequency tables\n",
    "target_dataset = \"cobra_frames\"\n",
    "target_dataset_models = [\n",
    "    m\n",
    "    for m in cobra_frames_composition_counts_per_seed_per_model.keys()\n",
    "    if m.endswith(target_dataset)\n",
    "]\n",
    "\n",
    "composition_counts_mean_per_target_dataset_models = {}\n",
    "\n",
    "for model in target_dataset_models:\n",
    "    for (\n",
    "        composition_name,\n",
    "        composition_counts,\n",
    "    ) in cobra_frames_composition_counts_per_seed_per_model[model].items():\n",
    "        # Calculate average composition frequencies for current model over seeds\n",
    "        mean_frequency_counts = np.mean(composition_counts)\n",
    "\n",
    "        # Calculate standard deviation of composition frequencies over seeds\n",
    "        stddev_frequency_counts = np.std(composition_counts)\n",
    "\n",
    "        # Calculate how often each composition results in the correct prediction per split\n",
    "        mean_correct_prediction = np.mean(\n",
    "            cobra_frames_train_correct_prediction_counts_per_seed_per_model[model][\n",
    "                composition_name\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate standard deviation of correct predictions per composition over seeds\n",
    "        stddev_correct_prediction = np.std(\n",
    "            cobra_frames_train_correct_prediction_counts_per_seed_per_model[model][\n",
    "                composition_name\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            composition_name\n",
    "            not in composition_counts_mean_per_target_dataset_models.keys()\n",
    "        ):\n",
    "            composition_counts_mean_per_target_dataset_models[composition_name] = {}\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            f\"{model}__mean_frequencies_test\"\n",
    "        ] = f\"{mean_frequency_counts} (+- {np.round(stddev_frequency_counts, decimals=2):0.2f})\"\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            f\"{model}__mean_correct_prediction_train\"\n",
    "        ] = f\"{mean_correct_prediction} (+- {np.round(stddev_correct_prediction, decimals=2):0.2f})\"\n",
    "\n",
    "\n",
    "# Make composition names nicer for final table\n",
    "counts_with_updated_composition_names = {}\n",
    "for (\n",
    "    composition_name,\n",
    "    composition_counts,\n",
    ") in composition_counts_mean_per_target_dataset_models.items():\n",
    "    composition_name_reformat_rules = {\n",
    "        \"cot\": \"Reasoning steps\",\n",
    "        \"category-few-shot\": \"In-context (category)\",\n",
    "        \"random-few-shot\": \"In-context (random)\",\n",
    "        \"similar-few-shot\": \"In-context (similar)\",\n",
    "        \"definitions\": \"Definitions\",\n",
    "        \"directional-stimulus\": \"Dir. stimulus\",\n",
    "        \"system-prompts\": \"Persona\",\n",
    "        \"task-description-only\": \"Base composition\",\n",
    "    }\n",
    "    composition_name_reformat = \", \".join(\n",
    "        [\n",
    "            composition_name_reformat_rules[comp].capitalize()\n",
    "            for comp in composition_name.split(\"_\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    counts_with_updated_composition_names[composition_name_reformat] = (\n",
    "        composition_counts\n",
    "    )\n",
    "\n",
    "composition_frequency_output_file = path.join(\n",
    "    f\"outputs/tables/composition-frequencies-{target_dataset}.csv\"\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=counts_with_updated_composition_names\n",
    ").transpose().sort_index().to_csv(composition_frequency_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = [\n",
    "    \"cateogory-few-shot\",\n",
    "    \"cot\",\n",
    "    \"definitions\",\n",
    "    \"directional-stimulus\",\n",
    "    \"random-few-shot\",\n",
    "    \"similar-few-shot\",\n",
    "    \"system-prompts\",\n",
    "    \"task-description-only\",\n",
    "]\n",
    "for model in cobra_frames_composition_predictions_test_per_model.keys():\n",
    "    all_cooccurrences = []\n",
    "    for t_outer in techniques:\n",
    "        t_cooccurrences = []\n",
    "        for seed in RANDOM_SEED:\n",
    "            seed_cooccurrences = np.zeros(len(techniques))\n",
    "            for pred in cobra_frames_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]:\n",
    "                if t_outer in pred:\n",
    "                    for t_inner in techniques:\n",
    "                        if t_inner in pred and t_inner != t_outer:\n",
    "                            seed_cooccurrences[techniques.index(t_inner)] += 1\n",
    "            t_cooccurrences.append(seed_cooccurrences)\n",
    "        all_cooccurrences.append(t_cooccurrences)\n",
    "\n",
    "    average_cooccurrences = np.array(\n",
    "        [\n",
    "            [np.mean(coocc) for coocc in list(zip(*per_seed_occurrences))]\n",
    "            for per_seed_occurrences in all_cooccurrences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(average_cooccurrences, cmap=\"plasma\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(techniques)), labels=techniques)\n",
    "    ax.set_yticks(np.arange(len(techniques)), labels=techniques)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(techniques)):\n",
    "        for j in range(len(techniques)):\n",
    "            text = ax.text(\n",
    "                j, i, average_cooccurrences[i, j], ha=\"center\", va=\"center\", color=\"w\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{model}: Average (over all seeds) cooccurrence for predicted compositions on CobraFrames\"\n",
    "    )\n",
    "    plt.savefig(f\"outputs/figures/cobraframes__{model}__technique-cooccurrences.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Stereoset corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"mistral-7b-instruct-v2\", \"command-r-v01\", \"llama3-70b-instruct\"]\n",
    "\n",
    "TRAINING_DATASETS = [\"sbic\", \"stereoset\", \"cobra_frames\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(datasets_to_load=[\"stereoset\"])\n",
    "stereoset_train_df = data_handler.stereoset_data[\"train\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\", \"text_hash\": \"md5_hash\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "stereoset_val_df = data_handler.stereoset_data[\"dev\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\", \"text_hash\": \"md5_hash\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "stereoset_test_df = data_handler.stereoset_data[\"test\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\", \"text_hash\": \"md5_hash\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_composition_predictions_val_per_model = {}\n",
    "stereoset_composition_predictions_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    for training_dataset in TRAINING_DATASETS:\n",
    "        # Validation set\n",
    "        composition_predictions_val = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"stereoset_val_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_val.columns:\n",
    "                composition_predictions_val[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_val[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            composition_predictions_val = pd.merge(\n",
    "                composition_predictions_val,\n",
    "                df.loc[\n",
    "                    :,\n",
    "                    [\n",
    "                        i\n",
    "                        for i in df.columns\n",
    "                        if i not in [\"input\", \"index\", \"pred_probabilities\"]\n",
    "                    ],\n",
    "                ],\n",
    "                on=\"post_id\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        stereoset_composition_predictions_val_per_model[\n",
    "            f\"{model}__{training_dataset}\"\n",
    "        ] = composition_predictions_val\n",
    "\n",
    "        # Test set\n",
    "        composition_predictions_test = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"stereoset_test_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_test.columns:\n",
    "                composition_predictions_test[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_test[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            composition_predictions_test = pd.merge(\n",
    "                composition_predictions_test,\n",
    "                df.loc[\n",
    "                    :,\n",
    "                    [\n",
    "                        i\n",
    "                        for i in df.columns\n",
    "                        if i not in [\"input\", \"index\", \"pred_probabilities\"]\n",
    "                    ],\n",
    "                ],\n",
    "                on=\"post_id\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        stereoset_composition_predictions_test_per_model[\n",
    "            f\"{model}__{training_dataset}\"\n",
    "        ] = composition_predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load composition-specific prediction files\n",
    "stereoset_output_dir = \"outputs/prompt-predictions/stereoset\"\n",
    "stereoset_predictions_per_composition_val_per_model = {}\n",
    "stereoset_predictions_per_composition_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation set\n",
    "    composition_files_val = [\n",
    "        f for f in sorted(listdir(stereoset_output_dir)) if \"dev\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_val = {}\n",
    "\n",
    "    for f in composition_files_val:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"stereoset-cot-greedy-dev_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(f\"stereoset-greedy-dev_{model}_\", \"\").replace(\n",
    "                \".parquet\", \"\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(path.join(stereoset_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_val[composition_name] = df\n",
    "\n",
    "    stereoset_predictions_per_composition_val_per_model[model] = (\n",
    "        predictions_per_composition_val\n",
    "    )\n",
    "\n",
    "    # Test set\n",
    "    composition_files_test = [\n",
    "        f for f in sorted(listdir(stereoset_output_dir)) if \"test\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_test = {}\n",
    "\n",
    "    for f in composition_files_test:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"stereoset-cot-greedy-test_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(f\"stereoset-greedy-test_{model}_\", \"\").replace(\n",
    "                \".parquet\", \"\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(path.join(stereoset_output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_test[composition_name] = df\n",
    "\n",
    "    stereoset_predictions_per_composition_test_per_model[model] = (\n",
    "        predictions_per_composition_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Training split\")\n",
    "positive_instances_train = len(stereoset_train_df[stereoset_train_df.true_label == 1])\n",
    "negative_instances_train = len(stereoset_train_df[stereoset_train_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_train} ({np.round(positive_instances_train / len(stereoset_train_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_train} ({np.round(negative_instances_train / len(stereoset_train_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Validation split\")\n",
    "positive_instances_val = len(stereoset_val_df[stereoset_val_df.true_label == 1])\n",
    "negative_instances_val = len(stereoset_val_df[stereoset_val_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_val} ({np.round(positive_instances_val / len(stereoset_val_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_val} ({np.round(negative_instances_val / len(stereoset_val_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Test split\")\n",
    "positive_instances_test = len(stereoset_test_df[stereoset_test_df.true_label == 1])\n",
    "negative_instances_test = len(stereoset_test_df[stereoset_test_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_test} ({np.round(positive_instances_test / len(stereoset_test_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_test} ({np.round(negative_instances_test / len(stereoset_test_df), decimals=3)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting compositions performance evaluation\n",
    "\n",
    "_aka. how well can the encoder model predict a composition that is correct_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = \"stereoset\"\n",
    "\n",
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    output_dir = path.join(\"outputs/composition-predictions\")\n",
    "    prompt_compositions = stereoset_predictions_per_composition_test_per_model[\n",
    "        model\n",
    "    ].keys()\n",
    "\n",
    "    scores_per_seed = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        # Load predictions of the adaptive prompting model for each text instance and composition\n",
    "        seed_dir = list(\n",
    "            filter(\n",
    "                lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{target_data}\"\n",
    "                in x\n",
    "                and f\"seed{seed}\" in x,\n",
    "                sorted(listdir(output_dir)),\n",
    "            )\n",
    "        )[0]\n",
    "        df = pd.read_parquet(\n",
    "            path.join(output_dir, seed_dir, f\"{target_data}_test_results.parquet\")\n",
    "        )\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "\n",
    "        # COBRAFRAMES-specific fix\n",
    "        # Remove posts that are not present in filtered cobraframe dataset above\n",
    "        # (due to a bug in cluster) (should only be five posts)\n",
    "        df = df[\n",
    "            df[\"post_id\"].isin(\n",
    "                stereoset_predictions_per_composition_test_per_model[model][\n",
    "                    list(prompt_compositions)[0]\n",
    "                ][\"post_id\"]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Load the id2component map to ensure that the predictions are in the same ordering as above\n",
    "        with open(path.join(output_dir, seed_dir, \"id2component_map.json\"), \"r\") as f:\n",
    "            id2component_map = json.load(f)\n",
    "        component2id_map = {value: int(key) for key, value in id2component_map.items()}\n",
    "\n",
    "        # Re-order the predicted probabilities per text instance according the to id2component map\n",
    "        # loaded above\n",
    "        adaptive_prompting_correctness_per_sample = []\n",
    "        for i, row in df.iterrows():\n",
    "            comp_df = stereoset_predictions_per_composition_test_per_model[model][\n",
    "                row[\"pred_best_composition\"]\n",
    "            ]\n",
    "            row_comp_df = comp_df[comp_df[\"post_id\"] == row[\"post_id\"]].iloc[0]\n",
    "            try:\n",
    "                adaptive_prompting_correctness_per_sample.append(\n",
    "                    (row_comp_df[f\"output_{seed}\"] == row_comp_df[\"true_label\"]).astype(\n",
    "                        int\n",
    "                    )\n",
    "                )\n",
    "            except KeyError:\n",
    "                adaptive_prompting_correctness_per_sample.append(\n",
    "                    (row_comp_df[f\"output_23\"] == row_comp_df[\"true_label\"]).astype(int)\n",
    "                )\n",
    "        scores_per_seed[seed] = sum(adaptive_prompting_correctness_per_sample) / len(\n",
    "            adaptive_prompting_correctness_per_sample\n",
    "        )\n",
    "\n",
    "    print(scores_per_seed)\n",
    "    print(np.mean(list(scores_per_seed.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive prompting evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_composition_prediction_scores_per_model = {}\n",
    "\n",
    "for model in stereoset_composition_predictions_val_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "\n",
    "    # Validation split\n",
    "    all_seed_scores_val = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in stereoset_composition_predictions_val_per_model[model].iterrows():\n",
    "            preds = stereoset_predictions_per_composition_val_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "            if f\"output_{seed}\" in preds.columns:\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                )\n",
    "            else:\n",
    "                # If we don't have predictions for other seeds, use the primary seed\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                )\n",
    "            y_true_seed.append(\n",
    "                preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "            )\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_val.append(scores)\n",
    "\n",
    "    # Test split\n",
    "    all_seed_scores_test = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in stereoset_composition_predictions_test_per_model[\n",
    "            model\n",
    "        ].iterrows():\n",
    "            preds = stereoset_predictions_per_composition_test_per_model[\n",
    "                model_name_without_data\n",
    "            ][row[f\"pred_best_composition_seed{seed}\"]]\n",
    "            if f\"output_{seed}\" in preds.columns:\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                )\n",
    "            else:\n",
    "                # If we don't have predictions for other seeds, use the primary seed\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                )\n",
    "            y_true_seed.append(\n",
    "                preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "            )\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_test.append(scores)\n",
    "\n",
    "    stereoset_composition_prediction_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        stereoset_composition_prediction_scores_per_model[model][\n",
    "            \"test_macro_precision\"\n",
    "        ],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        stereoset_composition_prediction_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        stereoset_composition_prediction_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_all_scores_val_per_model = {}\n",
    "stereoset_all_scores_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    all_scores_val = {}\n",
    "    for name, predictions in stereoset_predictions_per_composition_val_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_val[name] = scores\n",
    "\n",
    "    stereoset_all_scores_val_per_model[model] = all_scores_val\n",
    "\n",
    "    # Test split\n",
    "    all_scores_test = {}\n",
    "    for name, predictions in stereoset_predictions_per_composition_test_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_test[name] = scores\n",
    "\n",
    "    stereoset_all_scores_test_per_model[model] = all_scores_test\n",
    "\n",
    "stereoset_all_f1_scores_test_per_model = {}\n",
    "stereoset_all_f1_scores_test_per_model_seed_scores = {}\n",
    "for model in MODELS:\n",
    "    stereoset_all_f1_scores_test_per_model[model] = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in stereoset_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "\n",
    "    stereoset_all_f1_scores_test_per_model_seed_scores[model] = [\n",
    "        [((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v]\n",
    "        for k, v in stereoset_all_scores_test_per_model[model].items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_label = stereoset_test_df.true_label.mode()[0]\n",
    "majority_baseline_pred = [majority_label for _ in range(len(stereoset_test_df))]\n",
    "\n",
    "stereoset_maj_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=stereoset_test_df[\"true_label\"], y_pred=majority_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "stereoset_maj_baseline_precision_macro_averaged_test = (\n",
    "    stereoset_maj_baseline_scores[0][0] + stereoset_maj_baseline_scores[0][1]\n",
    ") / 2\n",
    "stereoset_maj_baseline_recall_macro_averaged_test = (\n",
    "    stereoset_maj_baseline_scores[1][0] + stereoset_maj_baseline_scores[1][1]\n",
    ") / 2\n",
    "stereoset_maj_baseline_f1_macro_averaged_test = (\n",
    "    stereoset_maj_baseline_scores[2][0] + stereoset_maj_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    stereoset_maj_baseline_precision_macro_averaged_test,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    stereoset_maj_baseline_recall_macro_averaged_test,\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", stereoset_maj_baseline_f1_macro_averaged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline_pred = np.random.randint(2, size=len(stereoset_test_df))\n",
    "\n",
    "stereoset_random_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=stereoset_test_df[\"true_label\"], y_pred=random_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "stereoset_random_baseline_precision_macro_averaged = (\n",
    "    stereoset_random_baseline_scores[0][0] + stereoset_random_baseline_scores[0][1]\n",
    ") / 2\n",
    "stereoset_random_baseline_recall_macro_averaged = (\n",
    "    stereoset_random_baseline_scores[1][0] + stereoset_random_baseline_scores[1][1]\n",
    ") / 2\n",
    "stereoset_random_baseline_f1_macro_averaged = (\n",
    "    stereoset_random_baseline_scores[2][0] + stereoset_random_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    stereoset_random_baseline_precision_macro_averaged,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\", stereoset_random_baseline_recall_macro_averaged\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", stereoset_random_baseline_f1_macro_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle\n",
    "\n",
    "_Always chooses the correct label, if it was predicted by either of the compositions; only chooses the wrong label if no composition predicted the correct label_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_oracle_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Compiling oracle predictions\n",
    "    oracle_predictions_test = pd.DataFrame()\n",
    "    for seed in RANDOM_SEED:\n",
    "        all_seed_predictions = pd.DataFrame()\n",
    "        for composition, df in stereoset_predictions_per_composition_test_per_model[\n",
    "            model\n",
    "        ].items():\n",
    "            if \"input\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"input\"] = df[\"input\"]\n",
    "            if \"true_label\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"true_label\"] = df[\"true_label\"]\n",
    "\n",
    "            try:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[f\"output_{seed}\"]\n",
    "            except KeyError:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        if \"input\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"input\"] = all_seed_predictions[\"input\"]\n",
    "        if \"true_label\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"true_label\"] = all_seed_predictions[\"true_label\"]\n",
    "\n",
    "        # For each sample, choose the true_label if at least one prediction is the true label\n",
    "        # Since the true label is in this dataframe the first value, we check if it exists in all other columns\n",
    "        # If yes, we use the true_label as oracle prediction and otherwise the value of the first column (as\n",
    "        # this should be the wrong label, similar to all other columns)\n",
    "        oracle_predictions_test[f\"output_{seed}\"] = all_seed_predictions.loc[\n",
    "            :, [i for i in all_seed_predictions.columns if i not in [\"input\"]]\n",
    "        ].apply(\n",
    "            lambda row: (\n",
    "                row[\"true_label\"]\n",
    "                if row[\"true_label\"] in row.values[1:]\n",
    "                else row.values[1]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Calculating scores\n",
    "    oracle_seed_scores_test = [\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=oracle_predictions_test[\"true_label\"],\n",
    "            y_pred=oracle_predictions_test[f\"output_{seed}\"],\n",
    "            pos_label=1,\n",
    "        )\n",
    "        for seed in RANDOM_SEED\n",
    "    ]\n",
    "\n",
    "    stereoset_oracle_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Oracle Precision (macro):\",\n",
    "        stereoset_oracle_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Oracle Recall (macro):\",\n",
    "        stereoset_oracle_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Oracle F1 (macro):\", stereoset_oracle_scores_per_model[model][\"test_macro_f1\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No technique\n",
    "\n",
    "_Task description and input text only_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_no_technique_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    no_technique_i = list(stereoset_all_scores_test_per_model[model].keys()).index(\n",
    "        \"task-description-only\"\n",
    "    )\n",
    "\n",
    "    stereoset_no_technique_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                \"task-description-only\"\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"No technique Precision (macro):\",\n",
    "        stereoset_no_technique_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique Recall (macro):\",\n",
    "        stereoset_no_technique_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique F1 (macro):\",\n",
    "        stereoset_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal composition\n",
    "\n",
    "_Best composition on the validation set in terms of f1 macro score, evaluated on the test set for precision, recall and f1_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_optimal_composition_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    stereoset_optimal_precision_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in stereoset_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    stereoset_optimal_recall_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in stereoset_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    stereoset_optimal_f1_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in stereoset_all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    # Test split\n",
    "    stereoset_optimal_precision_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in stereoset_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    stereoset_optimal_recall_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in stereoset_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    stereoset_optimal_f1_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in stereoset_all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "\n",
    "    # Find optimal model scores on test set\n",
    "    stereoset_optimal_composition_val_f1_macro_i = np.argmax(\n",
    "        stereoset_optimal_f1_macro_averaged_scores_val\n",
    "    )\n",
    "    stereoset_optimal_composition_name = list(\n",
    "        stereoset_all_scores_val_per_model[model].keys()\n",
    "    )[stereoset_optimal_composition_val_f1_macro_i]\n",
    "\n",
    "    stereoset_optimal_composition_scores_per_model[model] = {\n",
    "        \"composition_name\": list(stereoset_all_scores_val_per_model[model].keys())[\n",
    "            stereoset_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision\": stereoset_optimal_precision_macro_averaged_scores_test[\n",
    "            stereoset_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                stereoset_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": stereoset_optimal_recall_macro_averaged_scores_test[\n",
    "            stereoset_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                stereoset_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": stereoset_optimal_f1_macro_averaged_scores_test[\n",
    "            stereoset_optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in stereoset_all_scores_test_per_model[model][\n",
    "                stereoset_optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Optimal validation composition:\",\n",
    "        stereoset_optimal_composition_scores_per_model[model][\"composition_name\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Precision (macro):\",\n",
    "        stereoset_optimal_composition_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Recall (macro):\",\n",
    "        stereoset_optimal_composition_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition F1 (macro):\",\n",
    "        stereoset_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_ensemble_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    all_seed_scores = []\n",
    "\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "\n",
    "        seed_df = pd.DataFrame()\n",
    "\n",
    "        for (\n",
    "            composition_name,\n",
    "            comp_preds,\n",
    "        ) in stereoset_predictions_per_composition_test_per_model[model].items():\n",
    "            if \"input\" not in seed_df.columns:\n",
    "                seed_df[\"input\"] = comp_preds[\"input\"]\n",
    "                seed_df[\"post_id\"] = comp_preds[\"post_id\"]\n",
    "                seed_df[\"true_label\"] = comp_preds[\"true_label\"]\n",
    "            try:\n",
    "                seed_df = pd.merge(\n",
    "                    seed_df,\n",
    "                    comp_preds[[\"post_id\", f\"output_{seed}\"]].rename(\n",
    "                        columns={f\"output_{seed}\": f\"{composition_name}_{seed}\"}\n",
    "                    ),\n",
    "                    on=\"post_id\",\n",
    "                    how=\"left\",\n",
    "                )\n",
    "            except KeyError:\n",
    "                seed_df = pd.merge(\n",
    "                    seed_df,\n",
    "                    comp_preds[[\"post_id\", f\"output_{RANDOM_SEED[0]}\"]].rename(\n",
    "                        columns={\n",
    "                            f\"output_{RANDOM_SEED[0]}\": f\"{composition_name}_{seed}\"\n",
    "                        }\n",
    "                    ),\n",
    "                    on=\"post_id\",\n",
    "                    how=\"left\",\n",
    "                )\n",
    "\n",
    "        mode = seed_df.loc[\n",
    "            :,\n",
    "            [c for c in seed_df.columns if c not in [\"input\", \"post_id\", \"true_label\"]],\n",
    "        ].mode(axis=1)\n",
    "        # If there is a tie, use a random value between 0 and 1\n",
    "        seed_df[\"majority\"] = np.where(mode[1].isna(), mode[0], np.random.randint(2))\n",
    "\n",
    "        y_true_seed = seed_df[\"true_label\"]\n",
    "        y_pred_seed = seed_df[\"majority\"]\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores.append(scores)\n",
    "\n",
    "    stereoset_ensemble_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        stereoset_ensemble_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        stereoset_ensemble_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        stereoset_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "\n",
    "all_seed_scores = []\n",
    "for seed in RANDOM_SEED:\n",
    "    model_name = f\"deberta-v3-large-finetune_20240607232518_stereoset-seed{seed}\"\n",
    "    seed_df = pd.read_parquet(\n",
    "        path.join(\n",
    "            output_dir,\n",
    "            f\"stereoset-test_predictions-{model_name}.parquet\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seed_df = pd.merge(stereoset_test_df, seed_df, on=\"md5_hash\", how=\"left\")\n",
    "\n",
    "    scores = precision_recall_fscore_support(\n",
    "        y_true=seed_df[\"true_label\"],\n",
    "        y_pred=seed_df[f\"prediction_{model_name}\"],\n",
    "        pos_label=1,\n",
    "    )\n",
    "    all_seed_scores.append(scores)\n",
    "\n",
    "stereoset_finetune_scores = {\n",
    "    \"test_macro_precision\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_precision_seed_scores\": [\n",
    "        ((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_recall\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_recall_seed_scores\": [\n",
    "        ((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_f1\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_f1_seed_scores\": [\n",
    "        ((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    stereoset_finetune_scores[\"test_macro_precision\"],\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\", stereoset_finetune_scores[\"test_macro_recall\"]\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", stereoset_finetune_scores[\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-diagnosis baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "model_name = \"self-diagnosis\"\n",
    "\n",
    "sd_results_df = pd.read_parquet(\n",
    "    path.join(output_dir, model_name, \"baseline_self_diagnosis_stereoset_test.parquet\")\n",
    ")\n",
    "\n",
    "sd_scores = precision_recall_fscore_support(\n",
    "    y_true=sd_results_df[\"true_label\"], y_pred=sd_results_df[\"output_23\"], pos_label=1\n",
    ")\n",
    "\n",
    "stereoset_self_diagnosis_precision_macro_averaged_test = (\n",
    "    sd_scores[0][0] + sd_scores[0][1]\n",
    ") / 2\n",
    "stereoset_self_diagnosis_recall_macro_averaged_test = (\n",
    "    sd_scores[1][0] + sd_scores[1][1]\n",
    ") / 2\n",
    "stereoset_self_diagnosis_f1_macro_averaged_test = (\n",
    "    sd_scores[2][0] + sd_scores[2][1]\n",
    ") / 2\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\",\n",
    "    stereoset_self_diagnosis_precision_macro_averaged_test,\n",
    ")\n",
    "print(\n",
    "    \"Recall (macro) (over all seeds):\",\n",
    "    stereoset_self_diagnosis_recall_macro_averaged_test,\n",
    ")\n",
    "print(\"F1 (macro) (over all seeds):\", stereoset_self_diagnosis_f1_macro_averaged_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare best-on-test data\n",
    "stereoset_best_on_test_scores = {}\n",
    "for model in MODELS:\n",
    "    stereoset_best_composition = np.argmax(\n",
    "        stereoset_all_f1_scores_test_per_model[model]\n",
    "    )\n",
    "    stereoset_best_on_test_scores[model] = {\n",
    "        \"test_macro_f1_seed_scores\": stereoset_all_f1_scores_test_per_model_seed_scores[\n",
    "            model\n",
    "        ][stereoset_best_composition]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "target_dataset_to_evaluate = \"stereoset\"\n",
    "\n",
    "baselines = [\n",
    "    (\"BaseComposition\", stereoset_no_technique_scores_per_model),\n",
    "    (\"BestOnVal\", stereoset_optimal_composition_scores_per_model),\n",
    "    (\"BestOnTest\", stereoset_best_on_test_scores),\n",
    "    (\"Finetune\", stereoset_finetune_scores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    approach_name = \"CompositionPrediction\"\n",
    "    approach_scores = stereoset_composition_prediction_scores_per_model[\n",
    "        f\"{model}__{target_dataset_to_evaluate}\"\n",
    "    ][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "    for baseline_name, baseline_scores_dict in baselines:\n",
    "        if baseline_name == \"Finetune\":\n",
    "            baseline_scores = baseline_scores_dict[\"test_macro_f1_seed_scores\"]\n",
    "        else:\n",
    "            baseline_scores = baseline_scores_dict[model][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "        if not np.mean(approach_scores) > np.mean(baseline_scores):\n",
    "            print(\n",
    "                f\"Skipped {approach_name} vs. {baseline_name} ({np.mean(approach_scores)} vs. {np.mean(baseline_scores)})\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        t_results = ttest_function(baseline_scores, approach_scores)\n",
    "        # correct for one sided test, according to Hitchhiker's guide\n",
    "        p_value = t_results[1] / 2\n",
    "\n",
    "        if p_value <= alpha:\n",
    "            print(\n",
    "                f\"{approach_name} is significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"{approach_name} is NOT significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_ALL_BASELINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "box = ax.boxplot(\n",
    "    [\n",
    "        all_model_scores\n",
    "        for model, all_model_scores in stereoset_all_f1_scores_test_per_model.items()\n",
    "    ],\n",
    "    labels=MODELS,\n",
    ")\n",
    "legend_entities_handlers_per_model = {}\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    print(\"\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"Median:\", np.median(stereoset_all_f1_scores_test_per_model[model]))\n",
    "\n",
    "    best_composition_i = np.argmax(stereoset_all_f1_scores_test_per_model[model])\n",
    "    worst_composition_i = np.argmin(stereoset_all_f1_scores_test_per_model[model])\n",
    "    # Add best score as scatter\n",
    "    best_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        stereoset_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        stereoset_all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        # (\n",
    "        #     f\"Best-on-test \"\n",
    "        #     f\"({np.round(stereoset_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"A\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"A ->\",\n",
    "        (\n",
    "            f\"Best-on-test \"\n",
    "            f\"({np.round(stereoset_all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add worst score as scatter\n",
    "    worst_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        stereoset_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        stereoset_all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        # (\n",
    "        #     f\"Worst-on-test \"\n",
    "        #     f\"({np.round(stereoset_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"B\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"B ->\",\n",
    "        (\n",
    "            f\"Worst-on-test \"\n",
    "            f\"({np.round(stereoset_all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add no-technique as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        stereoset_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        stereoset_no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\n",
    "        #     f\"Base composition \"\n",
    "        #     f\"({np.round(stereoset_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        # ),\n",
    "        \"C\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"C ->\",\n",
    "        (\n",
    "            f\"Base composition \"\n",
    "            f\"({np.round(stereoset_no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add optimal model as scatter\n",
    "    optimal_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        stereoset_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"olive\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        stereoset_optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Best-on-validation \" f\"({np.round(stereoset_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"D\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"olive\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"D ->\",\n",
    "        (\n",
    "            \"Best-on-validation \"\n",
    "            f\"({np.round(stereoset_optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add naive ensemble as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        stereoset_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        stereoset_ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\"Majority ensemble \" f\"({np.round(stereoset_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "        \"E\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"E ->\",\n",
    "        (\n",
    "            \"Majority ensemble \"\n",
    "            f\"({np.round(stereoset_ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add composition predictions as scatters\n",
    "    for k, training_dataset in enumerate(TRAINING_DATASETS):\n",
    "        composition_prediction_score = np.round(\n",
    "            stereoset_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            decimals=3,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            stereoset_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"green\",\n",
    "            marker=\"*\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            stereoset_composition_prediction_scores_per_model[\n",
    "                f\"{model}__{training_dataset}\"\n",
    "            ][\"test_macro_f1\"],\n",
    "            # f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "            f\"F{k}\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"green\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            f\"F{k} ->\",\n",
    "            f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "        )\n",
    "\n",
    "    if SHOW_ALL_BASELINES:\n",
    "        # Add oracle as scatter\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            stereoset_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"black\",\n",
    "            marker=\"x\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            stereoset_oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            # (\n",
    "            #     \"Oracle \" f\"({np.round(stereoset_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            # ),\n",
    "            \"G\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"black\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            \"G ->\",\n",
    "            (\n",
    "                \"Oracle \"\n",
    "                f\"({np.round(stereoset_oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Always in the following order: best-on-test, worst-on-test, best-on-validation\n",
    "    legend_entities_handlers_per_model[model] = dict(\n",
    "        zip(\n",
    "            [\n",
    "                f\"bot: {list(stereoset_all_scores_test_per_model[model].keys())[best_composition_i]}\",\n",
    "                f\"wot: {list(stereoset_all_scores_test_per_model[model].keys())[worst_composition_i]}\",\n",
    "                f\"bov: {stereoset_optimal_composition_scores_per_model[model]['composition_name']}\",\n",
    "            ],\n",
    "            [\n",
    "                best_composition_handle,\n",
    "                worst_composition_handle,\n",
    "                optimal_composition_handle,\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Add finetune model as scatter\n",
    "plt.axhline(\n",
    "    stereoset_finetune_scores[\"test_macro_f1\"],\n",
    "    color=\"black\",\n",
    "    linestyle=\"dashed\",\n",
    "    alpha=0.4,\n",
    "    zorder=3,\n",
    ")\n",
    "plt.text(\n",
    "    0,\n",
    "    stereoset_finetune_scores[\"test_macro_f1\"],\n",
    "    f\"DeBERTa-v3-large (finetuned) ({np.round(stereoset_finetune_scores['test_macro_f1'], decimals=3)})\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    "    color=\"black\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Conditionally show outlier baselines\n",
    "if SHOW_ALL_BASELINES:\n",
    "    # Add majority label baseline as scatter\n",
    "    plt.axhline(\n",
    "        stereoset_maj_baseline_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        stereoset_maj_baseline_f1_macro_averaged_test,\n",
    "        f\"Majority label baseline ({np.round(stereoset_maj_baseline_f1_macro_averaged_test, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    # Add random baseline as scatter\n",
    "    plt.axhline(\n",
    "        stereoset_random_baseline_f1_macro_averaged,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        stereoset_random_baseline_f1_macro_averaged,\n",
    "        f\"Random baseline ({np.round(stereoset_random_baseline_f1_macro_averaged, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "    # Add self-diagnosis as horizontal line scatter\n",
    "    plt.axhline(\n",
    "        stereoset_self_diagnosis_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        stereoset_self_diagnosis_f1_macro_averaged_test,\n",
    "        f\"Self-diagnosis baseline ({np.round(stereoset_self_diagnosis_f1_macro_averaged_test, decimals=3)})\",\n",
    "        # \"I\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "handlers = []\n",
    "labels = []\n",
    "for model in MODELS:\n",
    "    # Add handlers, with first being dummy handler\n",
    "    handlers.append(\n",
    "        plt.scatter([0], [0], marker=\"None\", linestyle=\"None\", label=f\"dummy-{model}\")\n",
    "    )\n",
    "    handlers.extend(list(legend_entities_handlers_per_model[model].values()))\n",
    "\n",
    "    # Add labels, with first being model label\n",
    "    labels.append(model)\n",
    "    labels.extend(list(legend_entities_handlers_per_model[model].keys()))\n",
    "\n",
    "legend = fig.legend(handlers, labels, ncol=len(MODELS), loc=\"outside lower center\")\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "plt.title(\"Stereoset data\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1 (macro) (over all seeds)\")\n",
    "\n",
    "plt.savefig(\"outputs/figures/stereoset__performance-box-plot.pdf\")\n",
    "plt.savefig(\"outputs/figures/stereoset__performance-box-plot.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition frequency\n",
    "\n",
    "_How often was each composition chosen (bar chart with box plot), how often was each technique and combination of technqiues chosen (heatmap)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset_composition_counts_per_seed_per_model = {}\n",
    "\n",
    "for model in stereoset_composition_predictions_test_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "\n",
    "    stereoset_composition_counts_per_seed_per_model[model] = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        # Calculate how often each composition is used\n",
    "        comp_count = Counter(\n",
    "            stereoset_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]\n",
    "        )\n",
    "        for composition in stereoset_predictions_per_composition_test_per_model[\n",
    "            model_name_without_data\n",
    "        ]:\n",
    "            if (\n",
    "                composition\n",
    "                not in stereoset_composition_counts_per_seed_per_model[model].keys()\n",
    "            ):\n",
    "                stereoset_composition_counts_per_seed_per_model[model][composition] = []\n",
    "\n",
    "            if composition in comp_count.keys():\n",
    "                stereoset_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ].append(comp_count[composition])\n",
    "            else:\n",
    "                stereoset_composition_counts_per_seed_per_model[model][\n",
    "                    composition\n",
    "                ].append(0)\n",
    "\n",
    "    # Calculate bar heights (mean) and error bars (standard deviation)\n",
    "    compositions = list(stereoset_composition_counts_per_seed_per_model[model].keys())\n",
    "    values = [\n",
    "        np.mean(stereoset_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    lower_errors = [\n",
    "        np.mean(stereoset_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.min(stereoset_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    upper_errors = [\n",
    "        np.max(stereoset_composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.mean(stereoset_composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "\n",
    "    # Combine the data into a list of tuples and sort by values (mean)\n",
    "    sorted_data = sorted(\n",
    "        zip(values, lower_errors, upper_errors, compositions),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=False,\n",
    "    )\n",
    "\n",
    "    # Unpack the sorted data\n",
    "    values, lower_errors, upper_errors, compositions = zip(*sorted_data)\n",
    "\n",
    "    # Create asymmetric error arrays\n",
    "    asymmetric_errors = [lower_errors, upper_errors]\n",
    "\n",
    "    # Bar chart positions\n",
    "    x_pos = np.arange(len(compositions))\n",
    "\n",
    "    # Plot bars\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    # bars = plt.bar(x_pos, values, yerr=asymmetric_errors, align=\"center\", alpha=0.7, capsize=0)\n",
    "    bars = plt.barh(x_pos, values, align=\"center\", alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.yticks(x_pos, compositions, ha=\"right\")\n",
    "    plt.ylabel(\"Compositions\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.title(f\"{model}: Composition counts (over five random seeds) on Stereoset\")\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/stereoset__{model}__composition-frequency.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/stereoset__{model}__composition-frequency.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each composition produces a correct prediction for each split\n",
    "target_dataset = \"stereoset\"\n",
    "\n",
    "stereoset_train_correct_prediction_counts_per_seed_per_model = {}\n",
    "stereoset_val_correct_prediction_counts_per_seed_per_model = {}\n",
    "stereoset_test_correct_prediction_counts_per_seed_per_model = {}\n",
    "\n",
    "# Train dataset (needs slightly different loading)\n",
    "# We use the composition names form the test data loaded further above, but then load the\n",
    "# predictions on the train data below from file\n",
    "for (\n",
    "    model,\n",
    "    composition_predictions,\n",
    ") in stereoset_predictions_per_composition_test_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in stereoset_train_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        stereoset_train_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        # Load composition predictions for trainin dataset\n",
    "        if \"cot\" in composition:\n",
    "            comp_no_cot = composition.replace(\"cot_\", \"\")\n",
    "            model_composition_df_train = pd.read_parquet(\n",
    "                f\"outputs/prompt-predictions/{target_dataset}/{target_dataset}-cot-greedy-train_{model}_{comp_no_cot}.parquet\"\n",
    "            )\n",
    "        else:\n",
    "            model_composition_df_train = pd.read_parquet(\n",
    "                f\"outputs/prompt-predictions/{target_dataset}/{target_dataset}-greedy-train_{model}_{composition}.parquet\"\n",
    "            )\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df_train[\n",
    "                        model_composition_df_train[\"true_label\"]\n",
    "                        == model_composition_df_train[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df_train[\n",
    "                        model_composition_df_train[\"true_label\"]\n",
    "                        == model_composition_df_train[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        stereoset_train_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ][composition] = correct_predictions_per_seed\n",
    "\n",
    "# Val dataset\n",
    "for (\n",
    "    model,\n",
    "    composition_predictions,\n",
    ") in stereoset_predictions_per_composition_val_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in stereoset_val_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        stereoset_val_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            model_composition_df = stereoset_predictions_per_composition_val_per_model[\n",
    "                model\n",
    "            ][composition]\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        stereoset_val_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ][composition] = correct_predictions_per_seed\n",
    "\n",
    "# Test dataset\n",
    "for (\n",
    "    model,\n",
    "    composition_predictions,\n",
    ") in stereoset_predictions_per_composition_test_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in stereoset_test_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        stereoset_test_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            model_composition_df = stereoset_predictions_per_composition_test_per_model[\n",
    "                model\n",
    "            ][composition]\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        stereoset_test_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ][composition] = correct_predictions_per_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composition frequency tables\n",
    "target_dataset = \"stereoset\"\n",
    "target_dataset_models = [\n",
    "    m\n",
    "    for m in stereoset_composition_counts_per_seed_per_model.keys()\n",
    "    if m.endswith(target_dataset)\n",
    "]\n",
    "\n",
    "composition_counts_mean_per_target_dataset_models = {}\n",
    "\n",
    "for model in target_dataset_models:\n",
    "    for (\n",
    "        composition_name,\n",
    "        composition_counts,\n",
    "    ) in stereoset_composition_counts_per_seed_per_model[model].items():\n",
    "        # Calculate average composition frequencies for current model over seeds\n",
    "        mean_frequency_counts = np.mean(composition_counts)\n",
    "\n",
    "        # Calculate standard deviation of composition frequencies over seeds\n",
    "        stddev_frequency_counts = np.std(composition_counts)\n",
    "\n",
    "        # Calculate how often each composition results in the correct prediction per split\n",
    "        mean_correct_prediction = np.mean(\n",
    "            stereoset_train_correct_prediction_counts_per_seed_per_model[model][\n",
    "                composition_name\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate standard deviation of correct predictions per composition over seeds\n",
    "        stddev_correct_prediction = np.std(\n",
    "            stereoset_train_correct_prediction_counts_per_seed_per_model[model][\n",
    "                composition_name\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            composition_name\n",
    "            not in composition_counts_mean_per_target_dataset_models.keys()\n",
    "        ):\n",
    "            composition_counts_mean_per_target_dataset_models[composition_name] = {}\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            f\"{model}__mean_frequencies_test\"\n",
    "        ] = f\"{mean_frequency_counts} (+- {np.round(stddev_frequency_counts, decimals=2):0.2f})\"\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            f\"{model}__mean_correct_prediction_train\"\n",
    "        ] = f\"{mean_correct_prediction} (+- {np.round(stddev_correct_prediction, decimals=2):0.2f})\"\n",
    "\n",
    "\n",
    "# Make composition names nicer for final table\n",
    "counts_with_updated_composition_names = {}\n",
    "for (\n",
    "    composition_name,\n",
    "    composition_counts,\n",
    ") in composition_counts_mean_per_target_dataset_models.items():\n",
    "    composition_name_reformat_rules = {\n",
    "        \"cot\": \"Reasoning steps\",\n",
    "        \"category-few-shot\": \"In-context (category)\",\n",
    "        \"random-few-shot\": \"In-context (random)\",\n",
    "        \"similar-few-shot\": \"In-context (similar)\",\n",
    "        \"definitions\": \"Definitions\",\n",
    "        \"directional-stimulus\": \"Dir. stimulus\",\n",
    "        \"system-prompts\": \"Persona\",\n",
    "        \"task-description-only\": \"Base composition\",\n",
    "    }\n",
    "    composition_name_reformat = \", \".join(\n",
    "        [\n",
    "            composition_name_reformat_rules[comp].capitalize()\n",
    "            for comp in composition_name.split(\"_\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    counts_with_updated_composition_names[composition_name_reformat] = (\n",
    "        composition_counts\n",
    "    )\n",
    "\n",
    "composition_frequency_output_file = path.join(\n",
    "    f\"outputs/tables/composition-frequencies-{target_dataset}.csv\"\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=counts_with_updated_composition_names\n",
    ").transpose().sort_index().to_csv(composition_frequency_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = [\n",
    "    \"cateogory-few-shot\",\n",
    "    \"cot\",\n",
    "    \"definitions\",\n",
    "    \"directional-stimulus\",\n",
    "    \"random-few-shot\",\n",
    "    \"similar-few-shot\",\n",
    "    \"system-prompts\",\n",
    "    \"task-description-only\",\n",
    "]\n",
    "for model in stereoset_composition_predictions_test_per_model.keys():\n",
    "    all_cooccurrences = []\n",
    "    for t_outer in techniques:\n",
    "        t_cooccurrences = []\n",
    "        for seed in RANDOM_SEED:\n",
    "            seed_cooccurrences = np.zeros(len(techniques))\n",
    "            for pred in stereoset_composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]:\n",
    "                if t_outer in pred:\n",
    "                    for t_inner in techniques:\n",
    "                        if t_inner in pred and t_inner != t_outer:\n",
    "                            seed_cooccurrences[techniques.index(t_inner)] += 1\n",
    "            t_cooccurrences.append(seed_cooccurrences)\n",
    "        all_cooccurrences.append(t_cooccurrences)\n",
    "\n",
    "    average_cooccurrences = np.array(\n",
    "        [\n",
    "            [np.mean(coocc) for coocc in list(zip(*per_seed_occurrences))]\n",
    "            for per_seed_occurrences in all_cooccurrences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # mask = np.triu(np.ones_like(average_cooccurrences, dtype=bool))\n",
    "    # masked_data = np.ma.masked_array(average_cooccurrences, mask)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(average_cooccurrences, cmap=\"plasma\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(techniques)), labels=techniques)\n",
    "    ax.set_yticks(np.arange(len(techniques)), labels=techniques)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(techniques)):\n",
    "        for j in range(len(techniques)):\n",
    "            text = ax.text(\n",
    "                j, i, average_cooccurrences[i, j], ha=\"center\", va=\"center\", color=\"w\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{model}: Average (over all seeds) cooccurrence for predicted compositions on Stereoset\"\n",
    "    )\n",
    "    plt.savefig(f\"outputs/figures/stereoset__{model}__technique-cooccurrences.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SBIC corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"mistral-7b-instruct-v2\", \"command-r-v01\", \"llama3-70b-instruct\"]\n",
    "\n",
    "TRAINING_DATASETS = [\"sbic\", \"stereoset\", \"cobra_frames\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(datasets_to_load=[\"sbic\"])\n",
    "sbic_train_df = data_handler.sbic_data[\"train_sub_split_balanced\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "sbic_val_df = data_handler.sbic_data[\"dev\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]\n",
    "sbic_test_df = data_handler.sbic_data[\"test\"].rename(\n",
    "    columns={\"hasBiasedImplication\": \"true_label\"}\n",
    ")[[\"md5_hash\", \"true_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composition_predictions_val_per_model = {}\n",
    "composition_predictions_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    for training_dataset in TRAINING_DATASETS:\n",
    "        # Validation set\n",
    "        composition_predictions_val = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"sbic_val_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_val.columns:\n",
    "                composition_predictions_val[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_val[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            composition_predictions_val = pd.merge(\n",
    "                composition_predictions_val,\n",
    "                df.loc[\n",
    "                    :,\n",
    "                    [\n",
    "                        i\n",
    "                        for i in df.columns\n",
    "                        if i not in [\"input\", \"index\", \"pred_probabilities\"]\n",
    "                    ],\n",
    "                ],\n",
    "                on=\"post_id\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        composition_predictions_val_per_model[f\"{model}__{training_dataset}\"] = (\n",
    "            composition_predictions_val\n",
    "        )\n",
    "\n",
    "        # Test set\n",
    "        composition_predictions_test = pd.DataFrame()\n",
    "        for seed in RANDOM_SEED:\n",
    "            output_dir = path.join(\"outputs/composition-predictions\")\n",
    "            seed_dir = list(\n",
    "                filter(\n",
    "                    lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{training_dataset}\"\n",
    "                    in x\n",
    "                    and f\"seed{seed}\" in x,\n",
    "                    sorted(listdir(output_dir)),\n",
    "                )\n",
    "            )[0]\n",
    "            df = pd.read_parquet(\n",
    "                path.join(output_dir, seed_dir, \"sbic_test_results.parquet\")\n",
    "            )\n",
    "            df[\"post_id\"] = df.input.apply(\n",
    "                lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "            )\n",
    "            df = df.rename(\n",
    "                columns={\"pred_best_composition\": f\"pred_best_composition_seed{seed}\"}\n",
    "            )\n",
    "\n",
    "            if \"input\" not in composition_predictions_test.columns:\n",
    "                composition_predictions_test[\"input\"] = df[\"input\"]\n",
    "                composition_predictions_test[\"post_id\"] = df[\"post_id\"]\n",
    "\n",
    "            composition_predictions_test = pd.merge(\n",
    "                composition_predictions_test,\n",
    "                df.loc[\n",
    "                    :,\n",
    "                    [\n",
    "                        i\n",
    "                        for i in df.columns\n",
    "                        if i not in [\"input\", \"index\", \"pred_probabilities\"]\n",
    "                    ],\n",
    "                ],\n",
    "                on=\"post_id\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        composition_predictions_test_per_model[f\"{model}__{training_dataset}\"] = (\n",
    "            composition_predictions_test\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load composition-specific prediction files\n",
    "output_dir = \"outputs/prompt-predictions/sbic\"\n",
    "predictions_per_composition_val_per_model = {}\n",
    "predictions_per_composition_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation set\n",
    "    composition_files_val = [\n",
    "        f for f in sorted(listdir(output_dir)) if \"dev\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_val = {}\n",
    "\n",
    "    for f in composition_files_val:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"sbic-cot-greedy-dev_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(f\"sbic-greedy-dev_{model}_\", \"\").replace(\n",
    "                \".parquet\", \"\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(path.join(output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_val[composition_name] = df\n",
    "\n",
    "    predictions_per_composition_val_per_model[model] = predictions_per_composition_val\n",
    "\n",
    "    # Test set\n",
    "    composition_files_test = [\n",
    "        f for f in sorted(listdir(output_dir)) if \"test\" in f and model in f\n",
    "    ]\n",
    "    predictions_per_composition_test = {}\n",
    "\n",
    "    for f in composition_files_test:\n",
    "        if \"cot\" in f:\n",
    "            composition_name = f.replace(\n",
    "                f\"sbic-cot-greedy-test_{model}_\", \"cot_\"\n",
    "            ).replace(\".parquet\", \"\")\n",
    "        else:\n",
    "            composition_name = f.replace(f\"sbic-greedy-test_{model}_\", \"\").replace(\n",
    "                \".parquet\", \"\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(path.join(output_dir, f))\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        predictions_per_composition_test[composition_name] = df\n",
    "\n",
    "    predictions_per_composition_test_per_model[model] = predictions_per_composition_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Training split\")\n",
    "positive_instances_train = len(sbic_train_df[sbic_train_df.true_label == 1])\n",
    "negative_instances_train = len(sbic_train_df[sbic_train_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_train} ({np.round(positive_instances_train / len(sbic_train_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_train} ({np.round(negative_instances_train / len(sbic_train_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Validation split\")\n",
    "positive_instances_val = len(sbic_val_df[sbic_val_df.true_label == 1])\n",
    "negative_instances_val = len(sbic_val_df[sbic_val_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_val} ({np.round(positive_instances_val / len(sbic_val_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_val} ({np.round(negative_instances_val / len(sbic_val_df), decimals=3)})\"\n",
    ")\n",
    "\n",
    "print(\"## Test split\")\n",
    "positive_instances_test = len(sbic_test_df[sbic_test_df.true_label == 1])\n",
    "negative_instances_test = len(sbic_test_df[sbic_test_df.true_label == 0])\n",
    "print(\n",
    "    f\"Positive label: {positive_instances_test} ({np.round(positive_instances_test / len(sbic_test_df), decimals=3)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Negative label: {negative_instances_test} ({np.round(negative_instances_test / len(sbic_test_df), decimals=3)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting compositions performance evaluation\n",
    "\n",
    "_aka. how well can the encoder model predict a composition that is correct_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = \"sbic\"\n",
    "\n",
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    output_dir = path.join(\"outputs/composition-predictions\")\n",
    "    prompt_compositions = predictions_per_composition_test_per_model[model].keys()\n",
    "\n",
    "    scores_per_seed = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        # Load predictions of the adaptive prompting model for each text instance and composition\n",
    "        seed_dir = list(\n",
    "            filter(\n",
    "                lambda x: f\"deberta-v3-large_composition-prediction-for-{model}-on-{target_data}\"\n",
    "                in x\n",
    "                and f\"seed{seed}\" in x,\n",
    "                sorted(listdir(output_dir)),\n",
    "            )\n",
    "        )[0]\n",
    "        df = pd.read_parquet(\n",
    "            path.join(output_dir, seed_dir, f\"{target_data}_test_results.parquet\")\n",
    "        )\n",
    "        df[\"post_id\"] = df.input.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "\n",
    "        # COBRAFRAMES-specific fix\n",
    "        # Remove posts that are not present in filtered cobraframe dataset above\n",
    "        # (due to a bug in cluster) (should only be five posts)\n",
    "        df = df[\n",
    "            df[\"post_id\"].isin(\n",
    "                predictions_per_composition_test_per_model[model][\n",
    "                    list(prompt_compositions)[0]\n",
    "                ][\"post_id\"]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Load the id2component map to ensure that the predictions are in the same ordering as above\n",
    "        with open(path.join(output_dir, seed_dir, \"id2component_map.json\"), \"r\") as f:\n",
    "            id2component_map = json.load(f)\n",
    "        component2id_map = {value: int(key) for key, value in id2component_map.items()}\n",
    "\n",
    "        # Re-order the predicted probabilities per text instance according the to id2component map\n",
    "        # loaded above\n",
    "        adaptive_prompting_correctness_per_sample = []\n",
    "        for i, row in df.iterrows():\n",
    "            comp_df = predictions_per_composition_test_per_model[model][\n",
    "                row[\"pred_best_composition\"]\n",
    "            ]\n",
    "            row_comp_df = comp_df[comp_df[\"post_id\"] == row[\"post_id\"]].iloc[0]\n",
    "            try:\n",
    "                adaptive_prompting_correctness_per_sample.append(\n",
    "                    (row_comp_df[f\"output_{seed}\"] == row_comp_df[\"true_label\"]).astype(\n",
    "                        int\n",
    "                    )\n",
    "                )\n",
    "            except KeyError:\n",
    "                adaptive_prompting_correctness_per_sample.append(\n",
    "                    (row_comp_df[f\"output_23\"] == row_comp_df[\"true_label\"]).astype(int)\n",
    "                )\n",
    "        scores_per_seed[seed] = sum(adaptive_prompting_correctness_per_sample) / len(\n",
    "            adaptive_prompting_correctness_per_sample\n",
    "        )\n",
    "\n",
    "    print(scores_per_seed)\n",
    "    print(np.mean(list(scores_per_seed.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive prompting evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composition_prediction_scores_per_model = {}\n",
    "\n",
    "for model in composition_predictions_val_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "    # Validation split\n",
    "    all_seed_scores_val = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in composition_predictions_val_per_model[model].iterrows():\n",
    "            preds = predictions_per_composition_val_per_model[model_name_without_data][\n",
    "                row[f\"pred_best_composition_seed{seed}\"]\n",
    "            ]\n",
    "            if f\"output_{seed}\" in preds.columns:\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                )\n",
    "            else:\n",
    "                # If we don't have predictions for other seeds, use the primary seed\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                )\n",
    "            y_true_seed.append(\n",
    "                preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "            )\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_val.append(scores)\n",
    "\n",
    "    # Test split\n",
    "    all_seed_scores_test = []\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "        for i, row in composition_predictions_test_per_model[model].iterrows():\n",
    "            preds = predictions_per_composition_test_per_model[model_name_without_data][\n",
    "                row[f\"pred_best_composition_seed{seed}\"]\n",
    "            ]\n",
    "            if f\"output_{seed}\" in preds.columns:\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][f\"output_{seed}\"])\n",
    "                )\n",
    "            else:\n",
    "                # If we don't have predictions for other seeds, use the primary seed\n",
    "                y_pred_seed.append(\n",
    "                    (preds[preds.post_id == row.post_id].iloc[0][\"output_23\"])\n",
    "                )\n",
    "            y_true_seed.append(\n",
    "                preds[preds.post_id == row.post_id].iloc[0][\"true_label\"]\n",
    "            )\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores_test.append(scores)\n",
    "\n",
    "    composition_prediction_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores_test\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        composition_prediction_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        composition_prediction_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        composition_prediction_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_val_per_model = {}\n",
    "all_scores_test_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "\n",
    "    # Validation split\n",
    "    all_scores_val = {}\n",
    "    for name, predictions in predictions_per_composition_val_per_model[model].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_val[name] = scores\n",
    "\n",
    "    all_scores_val_per_model[model] = all_scores_val\n",
    "\n",
    "    # Test split\n",
    "    all_scores_test = {}\n",
    "    for name, predictions in predictions_per_composition_test_per_model[model].items():\n",
    "        try:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{seed}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        # In some cases, we don't have more than one seed, so we fall back to the primary seed\n",
    "        except KeyError:\n",
    "            scores = [\n",
    "                precision_recall_fscore_support(\n",
    "                    y_true=predictions[\"true_label\"],\n",
    "                    y_pred=predictions[f\"output_{RANDOM_SEED[0]}\"],\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                for seed in RANDOM_SEED\n",
    "            ]\n",
    "        all_scores_test[name] = scores\n",
    "\n",
    "    all_scores_test_per_model[model] = all_scores_test\n",
    "\n",
    "all_f1_scores_test_per_model = {}\n",
    "all_f1_scores_test_per_model_seed_scores = {}\n",
    "for model in MODELS:\n",
    "    all_f1_scores_test_per_model[model] = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    all_f1_scores_test_per_model_seed_scores[model] = [\n",
    "        [((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v]\n",
    "        for k, v in all_scores_test_per_model[model].items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_label = sbic_test_df.true_label.mode()[0]\n",
    "majority_baseline_pred = [majority_label for _ in range(len(sbic_test_df))]\n",
    "\n",
    "maj_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=sbic_test_df[\"true_label\"], y_pred=majority_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "maj_baseline_precision_macro_averaged_test = (\n",
    "    maj_baseline_scores[0][0] + maj_baseline_scores[0][1]\n",
    ") / 2\n",
    "maj_baseline_recall_macro_averaged_test = (\n",
    "    maj_baseline_scores[1][0] + maj_baseline_scores[1][1]\n",
    ") / 2\n",
    "maj_baseline_f1_macro_averaged_test = (\n",
    "    maj_baseline_scores[2][0] + maj_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\"Precision (macro) (over all seeds):\", maj_baseline_precision_macro_averaged_test)\n",
    "print(\"Recall (macro) (over all seeds):\", maj_baseline_recall_macro_averaged_test)\n",
    "print(\"F1 (macro) (over all seeds):\", maj_baseline_f1_macro_averaged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline_pred = np.random.randint(2, size=len(sbic_test_df))\n",
    "\n",
    "random_baseline_scores = precision_recall_fscore_support(\n",
    "    y_true=sbic_test_df[\"true_label\"], y_pred=random_baseline_pred, pos_label=1\n",
    ")\n",
    "\n",
    "random_baseline_precision_macro_averaged = (\n",
    "    random_baseline_scores[0][0] + random_baseline_scores[0][1]\n",
    ") / 2\n",
    "random_baseline_recall_macro_averaged = (\n",
    "    random_baseline_scores[1][0] + random_baseline_scores[1][1]\n",
    ") / 2\n",
    "random_baseline_f1_macro_averaged = (\n",
    "    random_baseline_scores[2][0] + random_baseline_scores[2][1]\n",
    ") / 2\n",
    "print(\"Precision (macro) (over all seeds):\", random_baseline_precision_macro_averaged)\n",
    "print(\"Recall (macro) (over all seeds):\", random_baseline_recall_macro_averaged)\n",
    "print(\"F1 (macro) (over all seeds):\", random_baseline_f1_macro_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle\n",
    "\n",
    "_Always chooses the correct label, if it was predicted by either of the compositions; only chooses the wrong label if no composition predicted the correct label_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Compiling oracle predictions\n",
    "    oracle_predictions_test = pd.DataFrame()\n",
    "    for seed in RANDOM_SEED:\n",
    "        all_seed_predictions = pd.DataFrame()\n",
    "        for composition, df in predictions_per_composition_test_per_model[\n",
    "            model\n",
    "        ].items():\n",
    "            if \"input\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"input\"] = df[\"input\"]\n",
    "            if \"true_label\" not in all_seed_predictions.columns:\n",
    "                all_seed_predictions[\"true_label\"] = df[\"true_label\"]\n",
    "\n",
    "            try:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[f\"output_{seed}\"]\n",
    "            except KeyError:\n",
    "                all_seed_predictions[f\"{composition}_{seed}\"] = df[\n",
    "                    f\"output_{RANDOM_SEED[0]}\"\n",
    "                ]\n",
    "\n",
    "        if \"input\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"input\"] = all_seed_predictions[\"input\"]\n",
    "        if \"true_label\" not in oracle_predictions_test.columns:\n",
    "            oracle_predictions_test[\"true_label\"] = all_seed_predictions[\"true_label\"]\n",
    "\n",
    "        # For each sample, choose the true_label if at least one prediction is the true label\n",
    "        # Since the true label is in this dataframe the first value, we check if it exists in all other columns\n",
    "        # If yes, we use the true_label as oracle prediction and otherwise the value of the first column (as\n",
    "        # this should be the wrong label, similar to all other columns)\n",
    "        oracle_predictions_test[f\"output_{seed}\"] = all_seed_predictions.loc[\n",
    "            :, [i for i in all_seed_predictions.columns if i not in [\"input\"]]\n",
    "        ].apply(\n",
    "            lambda row: (\n",
    "                row[\"true_label\"]\n",
    "                if row[\"true_label\"] in row.values[1:]\n",
    "                else row.values[1]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Calculating scores\n",
    "    oracle_seed_scores_test = [\n",
    "        precision_recall_fscore_support(\n",
    "            y_true=oracle_predictions_test[\"true_label\"],\n",
    "            y_pred=oracle_predictions_test[f\"output_{seed}\"],\n",
    "            pos_label=1,\n",
    "        )\n",
    "        for seed in RANDOM_SEED\n",
    "    ]\n",
    "\n",
    "    oracle_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in oracle_seed_scores_test\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Oracle Precision (macro):\",\n",
    "        oracle_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\"Oracle Recall (macro):\", oracle_scores_per_model[model][\"test_macro_recall\"])\n",
    "    print(\"Oracle F1 (macro):\", oracle_scores_per_model[model][\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No technique\n",
    "\n",
    "_Task description and input text only_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_technique_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    no_technique_i = list(all_scores_test_per_model[model].keys()).index(\n",
    "        \"task-description-only\"\n",
    "    )\n",
    "\n",
    "    no_technique_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_scores_test_per_model[model][\"task-description-only\"]\n",
    "        ],\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_scores_test_per_model[model][\"task-description-only\"]\n",
    "        ],\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_scores_test_per_model[model][\n",
    "                    \"task-description-only\"\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_scores_test_per_model[model][\"task-description-only\"]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"No technique Precision (macro):\",\n",
    "        no_technique_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique Recall (macro):\",\n",
    "        no_technique_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"No technique F1 (macro):\",\n",
    "        no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal composition\n",
    "\n",
    "_Best composition on the validation set in terms of f1 macro score, evaluated on the test set for precision, recall and f1_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_composition_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    # Validation split\n",
    "    optimal_precision_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    optimal_recall_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    optimal_f1_macro_averaged_scores_val = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in all_scores_val_per_model[model].items()\n",
    "    ]\n",
    "    # Test split\n",
    "    optimal_precision_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in v])\n",
    "        for k, v in all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    optimal_recall_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in v])\n",
    "        for k, v in all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "    optimal_f1_macro_averaged_scores_test = [\n",
    "        np.mean([((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in v])\n",
    "        for k, v in all_scores_test_per_model[model].items()\n",
    "    ]\n",
    "\n",
    "    # Find optimal model scores on test set\n",
    "    optimal_composition_val_f1_macro_i = np.argmax(optimal_f1_macro_averaged_scores_val)\n",
    "    optimal_composition_name = list(all_scores_val_per_model[model].keys())[\n",
    "        optimal_composition_val_f1_macro_i\n",
    "    ]\n",
    "\n",
    "    optimal_composition_scores_per_model[model] = {\n",
    "        \"composition_name\": optimal_composition_name,\n",
    "        \"test_macro_precision\": optimal_precision_macro_averaged_scores_test[\n",
    "            optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_precision_seed_scores\": [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_scores_test_per_model[model][\n",
    "                optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_recall\": optimal_recall_macro_averaged_scores_test[\n",
    "            optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_recall_seed_scores\": [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_scores_test_per_model[model][\n",
    "                optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "        \"test_macro_f1\": optimal_f1_macro_averaged_scores_test[\n",
    "            optimal_composition_val_f1_macro_i\n",
    "        ],\n",
    "        \"test_macro_f1_seed_scores\": [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_scores_test_per_model[model][\n",
    "                optimal_composition_name\n",
    "            ]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\n",
    "        \"Optimal validation composition:\",\n",
    "        optimal_composition_scores_per_model[model][\"composition_name\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Precision (macro):\",\n",
    "        optimal_composition_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition Recall (macro):\",\n",
    "        optimal_composition_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Optimal composition F1 (macro):\",\n",
    "        optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_scores_per_model = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    all_seed_scores = []\n",
    "\n",
    "    for seed in RANDOM_SEED:\n",
    "        y_true_seed = []\n",
    "        y_pred_seed = []\n",
    "\n",
    "        seed_df = pd.DataFrame()\n",
    "\n",
    "        for composition_name, comp_preds in predictions_per_composition_test_per_model[\n",
    "            model\n",
    "        ].items():\n",
    "            if \"input\" not in seed_df.columns:\n",
    "                seed_df[\"input\"] = comp_preds[\"input\"]\n",
    "                seed_df[\"post_id\"] = comp_preds[\"post_id\"]\n",
    "                seed_df[\"true_label\"] = comp_preds[\"true_label\"]\n",
    "            try:\n",
    "                seed_df = pd.merge(\n",
    "                    seed_df,\n",
    "                    comp_preds[[\"post_id\", f\"output_{seed}\"]].rename(\n",
    "                        columns={f\"output_{seed}\": f\"{composition_name}_{seed}\"}\n",
    "                    ),\n",
    "                    on=\"post_id\",\n",
    "                    how=\"left\",\n",
    "                )\n",
    "            except KeyError:\n",
    "                seed_df = pd.merge(\n",
    "                    seed_df,\n",
    "                    comp_preds[[\"post_id\", f\"output_{RANDOM_SEED[0]}\"]].rename(\n",
    "                        columns={\n",
    "                            f\"output_{RANDOM_SEED[0]}\": f\"{composition_name}_{seed}\"\n",
    "                        }\n",
    "                    ),\n",
    "                    on=\"post_id\",\n",
    "                    how=\"left\",\n",
    "                )\n",
    "\n",
    "        mode = seed_df.loc[\n",
    "            :,\n",
    "            [c for c in seed_df.columns if c not in [\"input\", \"post_id\", \"true_label\"]],\n",
    "        ].mode(axis=1)\n",
    "        # If there is a tie, use a random value between 0 and 1\n",
    "        seed_df[\"majority\"] = np.where(mode[1].isna(), mode[0], np.random.randint(2))\n",
    "\n",
    "        y_true_seed = seed_df[\"true_label\"]\n",
    "        y_pred_seed = seed_df[\"majority\"]\n",
    "\n",
    "        scores = precision_recall_fscore_support(\n",
    "            y_true=y_true_seed, y_pred=y_pred_seed, pos_label=1\n",
    "        )\n",
    "        all_seed_scores.append(scores)\n",
    "\n",
    "    ensemble_scores_per_model[model] = {\n",
    "        \"test_macro_precision\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_recall\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "        \"test_macro_f1\": np.mean(\n",
    "            [\n",
    "                ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "                for seed_scores in all_seed_scores\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(model, \"=\" * 50)\n",
    "    print(\"Averaged scores\")\n",
    "    print(\n",
    "        \"Precision (macro) (over all seeds):\",\n",
    "        ensemble_scores_per_model[model][\"test_macro_precision\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Recall (macro) (over all seeds):\",\n",
    "        ensemble_scores_per_model[model][\"test_macro_recall\"],\n",
    "    )\n",
    "    print(\n",
    "        \"F1 (macro) (over all seeds):\",\n",
    "        ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "\n",
    "all_seed_scores = []\n",
    "for seed in RANDOM_SEED:\n",
    "    model_name = f\"deberta-v3-large-finetune_20240605105831_sbic-seed{seed}\"\n",
    "    seed_df = pd.read_parquet(\n",
    "        path.join(\n",
    "            output_dir,\n",
    "            f\"sbic-test_predictions-{model_name}.parquet\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    seed_df = pd.merge(sbic_test_df, seed_df, on=\"md5_hash\", how=\"left\")\n",
    "\n",
    "    scores = precision_recall_fscore_support(\n",
    "        y_true=seed_df[\"true_label\"],\n",
    "        y_pred=seed_df[f\"prediction_{model_name}\"],\n",
    "        pos_label=1,\n",
    "    )\n",
    "    all_seed_scores.append(scores)\n",
    "\n",
    "\n",
    "finetune_scores = {\n",
    "    \"test_macro_precision\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[0][0] + seed_scores[0][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_precision_seed_scores\": [\n",
    "        ((seed_scores[0][0] + seed_scores[0][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_recall\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[1][0] + seed_scores[1][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_recall_seed_scores\": [\n",
    "        ((seed_scores[1][0] + seed_scores[1][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "    \"test_macro_f1\": np.mean(\n",
    "        [\n",
    "            ((seed_scores[2][0] + seed_scores[2][1]) / 2)\n",
    "            for seed_scores in all_seed_scores\n",
    "        ]\n",
    "    ),\n",
    "    \"test_macro_f1_seed_scores\": [\n",
    "        ((seed_scores[2][0] + seed_scores[2][1]) / 2) for seed_scores in all_seed_scores\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\"Precision (macro) (over all seeds):\", finetune_scores[\"test_macro_precision\"])\n",
    "print(\"Recall (macro) (over all seeds):\", finetune_scores[\"test_macro_recall\"])\n",
    "print(\"F1 (macro) (over all seeds):\", finetune_scores[\"test_macro_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-diagnosis baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results\"\n",
    "model_name = \"self-diagnosis\"\n",
    "\n",
    "sd_results_df = pd.read_parquet(\n",
    "    path.join(output_dir, model_name, \"baseline_self_diagnosis_sbic_test.parquet\")\n",
    ")\n",
    "\n",
    "sd_scores = precision_recall_fscore_support(\n",
    "    y_true=sd_results_df[\"true_label\"], y_pred=sd_results_df[\"output_23\"], pos_label=1\n",
    ")\n",
    "\n",
    "self_diagnosis_precision_macro_averaged_test = (sd_scores[0][0] + sd_scores[0][1]) / 2\n",
    "self_diagnosis_recall_macro_averaged_test = (sd_scores[1][0] + sd_scores[1][1]) / 2\n",
    "self_diagnosis_f1_macro_averaged_test = (sd_scores[2][0] + sd_scores[2][1]) / 2\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Averaged scores\")\n",
    "print(\n",
    "    \"Precision (macro) (over all seeds):\", self_diagnosis_precision_macro_averaged_test\n",
    ")\n",
    "print(\"Recall (macro) (over all seeds):\", self_diagnosis_recall_macro_averaged_test)\n",
    "print(\"F1 (macro) (over all seeds):\", self_diagnosis_f1_macro_averaged_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare best-on-test data\n",
    "best_on_test_scores = {}\n",
    "for model in MODELS:\n",
    "    best_composition = np.argmax(all_f1_scores_test_per_model[model])\n",
    "    best_on_test_scores[model] = {\n",
    "        \"test_macro_f1_seed_scores\": all_f1_scores_test_per_model_seed_scores[model][\n",
    "            best_composition\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "ttest_function = ttest_rel\n",
    "\n",
    "target_dataset_to_evaluate = \"sbic\"\n",
    "\n",
    "baselines = [\n",
    "    (\"BaseComposition\", no_technique_scores_per_model),\n",
    "    (\"BestOnVal\", optimal_composition_scores_per_model),\n",
    "    (\"BestOnTest\", best_on_test_scores),\n",
    "    (\"Finetune\", finetune_scores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"\\n\\n{model}\")\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    approach_name = \"CompositionPrediction\"\n",
    "    approach_scores = composition_prediction_scores_per_model[\n",
    "        f\"{model}__{target_dataset_to_evaluate}\"\n",
    "    ][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "    for baseline_name, baseline_scores_dict in baselines:\n",
    "        if baseline_name == \"Finetune\":\n",
    "            baseline_scores = baseline_scores_dict[\"test_macro_f1_seed_scores\"]\n",
    "        else:\n",
    "            baseline_scores = baseline_scores_dict[model][\"test_macro_f1_seed_scores\"]\n",
    "\n",
    "        if not np.mean(approach_scores) > np.mean(baseline_scores):\n",
    "            print(\n",
    "                f\"Skipped {approach_name} vs. {baseline_name} ({np.mean(approach_scores)} vs. {np.mean(baseline_scores)})\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        t_results = ttest_function(baseline_scores, approach_scores)\n",
    "        # correct for one sided test, according to Hitchhiker's guide\n",
    "        p_value = t_results[1] / 2\n",
    "\n",
    "        if p_value <= alpha:\n",
    "            print(\n",
    "                f\"{approach_name} is significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"{approach_name} is NOT significantly better than {baseline_name} with p-value {p_value:.4f} (t-test).\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_ALL_BASELINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "box = ax.boxplot(\n",
    "    [\n",
    "        all_model_scores\n",
    "        for model, all_model_scores in all_f1_scores_test_per_model.items()\n",
    "    ],\n",
    "    labels=MODELS,\n",
    ")\n",
    "legend_entities_handlers_per_model = {}\n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    print(\"\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"Median:\", np.median(all_f1_scores_test_per_model[model]))\n",
    "\n",
    "    best_composition_i = np.argmax(all_f1_scores_test_per_model[model])\n",
    "    worst_composition_i = np.argmin(all_f1_scores_test_per_model[model])\n",
    "    # Add best score as scatter\n",
    "    best_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        all_f1_scores_test_per_model[model][best_composition_i],\n",
    "        # (\n",
    "        #     f\"Best-on-test \"\n",
    "        #     f\"({np.round(all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"A\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"red\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"A ->\",\n",
    "        (\n",
    "            f\"Best-on-test \"\n",
    "            f\"({np.round(all_f1_scores_test_per_model[model][best_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add worst score as scatter\n",
    "    worst_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        alpha=0.6,\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        all_f1_scores_test_per_model[model][worst_composition_i],\n",
    "        # (\n",
    "        #     f\"Worst-on-test \"\n",
    "        #     f\"({np.round(all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        # ),\n",
    "        \"B\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"blue\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"B ->\",\n",
    "        (\n",
    "            f\"Worst-on-test \"\n",
    "            f\"({np.round(all_f1_scores_test_per_model[model][worst_composition_i], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add no-technique as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        no_technique_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\n",
    "        #     f\"Base composition \"\n",
    "        #     f\"({np.round(no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        # ),\n",
    "        \"C\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"C ->\",\n",
    "        (\n",
    "            f\"Base composition \"\n",
    "            f\"({np.round(no_technique_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add optimal model as scatter\n",
    "    optimal_composition_name = optimal_composition_scores_per_model[model][\n",
    "        \"composition_name\"\n",
    "    ]\n",
    "    optimal_composition_handle = plt.scatter(\n",
    "        i + 1,\n",
    "        optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"olive\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        optimal_composition_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\n",
    "        #     \"Best-on-validation \"\n",
    "        #     f\"({np.round(optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        # ),\n",
    "        \"D\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"olive\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"D ->\",\n",
    "        (\n",
    "            \"Best-on-validation \"\n",
    "            f\"({np.round(optimal_composition_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add naive ensemble as scatter\n",
    "    plt.scatter(\n",
    "        i + 1,\n",
    "        ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        alpha=0.6,\n",
    "        color=\"black\",\n",
    "        marker=\"x\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        i + 1,\n",
    "        ensemble_scores_per_model[model][\"test_macro_f1\"],\n",
    "        # (\n",
    "        #     \"Majority ensemble \"\n",
    "        #     f\"({np.round(ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        # ),\n",
    "        \"E\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    print(\n",
    "        \"E ->\",\n",
    "        (\n",
    "            \"Majority ensemble \"\n",
    "            f\"({np.round(ensemble_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add composition predictions as scatters\n",
    "    for k, training_dataset in enumerate(TRAINING_DATASETS):\n",
    "        composition_prediction_score = np.round(\n",
    "            composition_prediction_scores_per_model[f\"{model}__{training_dataset}\"][\n",
    "                \"test_macro_f1\"\n",
    "            ],\n",
    "            decimals=3,\n",
    "        )\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            composition_prediction_scores_per_model[f\"{model}__{training_dataset}\"][\n",
    "                \"test_macro_f1\"\n",
    "            ],\n",
    "            alpha=0.6,\n",
    "            color=\"green\",\n",
    "            marker=\"*\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            composition_prediction_scores_per_model[f\"{model}__{training_dataset}\"][\n",
    "                \"test_macro_f1\"\n",
    "            ],\n",
    "            # f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "            f\"F{k}\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"green\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            f\"F{k} ->\",\n",
    "            f\"Composition prediction ({training_dataset}) ({composition_prediction_score})\",\n",
    "        )\n",
    "\n",
    "    if SHOW_ALL_BASELINES:\n",
    "        # Add oracle as scatter\n",
    "        plt.scatter(\n",
    "            i + 1,\n",
    "            oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            alpha=0.6,\n",
    "            color=\"black\",\n",
    "            marker=\"x\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        plt.text(\n",
    "            i + 1,\n",
    "            oracle_scores_per_model[model][\"test_macro_f1\"],\n",
    "            # (f\"Oracle ({np.round(oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"),\n",
    "            \"G\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"black\",\n",
    "            zorder=3,\n",
    "        )\n",
    "        print(\n",
    "            \"G ->\",\n",
    "            (\n",
    "                f\"Oracle ({np.round(oracle_scores_per_model[model]['test_macro_f1'], decimals=3)})\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Always in the following order: best-on-test, worst-on-test, best-on-validation\n",
    "    legend_entities_handlers_per_model[model] = dict(\n",
    "        zip(\n",
    "            [\n",
    "                f\"bot: {list(all_scores_test_per_model[model].keys())[best_composition_i]}\",\n",
    "                f\"wot: {list(all_scores_test_per_model[model].keys())[worst_composition_i]}\",\n",
    "                f\"bov: {optimal_composition_scores_per_model[model]['composition_name']}\",\n",
    "            ],\n",
    "            [\n",
    "                best_composition_handle,\n",
    "                worst_composition_handle,\n",
    "                optimal_composition_handle,\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Add finetune model as scatter\n",
    "plt.axhline(\n",
    "    finetune_scores[\"test_macro_f1\"],\n",
    "    color=\"black\",\n",
    "    linestyle=\"dashed\",\n",
    "    alpha=0.4,\n",
    "    zorder=3,\n",
    ")\n",
    "plt.text(\n",
    "    0,\n",
    "    finetune_scores[\"test_macro_f1\"],\n",
    "    f\"DeBERTa-v3-large (finetuned) ({np.round(finetune_scores['test_macro_f1'], decimals=3)})\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    "    color=\"black\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Conditionally show outlier baselines\n",
    "if SHOW_ALL_BASELINES:\n",
    "    # Add majority label baseline as scatter\n",
    "    plt.axhline(\n",
    "        maj_baseline_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        maj_baseline_f1_macro_averaged_test,\n",
    "        f\"Majority label baseline ({np.round(maj_baseline_f1_macro_averaged_test, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "    # Add random baseline as scatter\n",
    "    plt.axhline(\n",
    "        random_baseline_f1_macro_averaged,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        random_baseline_f1_macro_averaged,\n",
    "        f\"Random baseline ({np.round(random_baseline_f1_macro_averaged, decimals=3)})\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "    # Add self-diagnosis as horizontal line scatter\n",
    "    plt.axhline(\n",
    "        self_diagnosis_f1_macro_averaged_test,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        alpha=0.4,\n",
    "        zorder=3,\n",
    "    )\n",
    "    plt.text(\n",
    "        0,\n",
    "        self_diagnosis_f1_macro_averaged_test,\n",
    "        f\"Self-diagnosis baseline ({np.round(self_diagnosis_f1_macro_averaged_test, decimals=3)})\",\n",
    "        # \"I\",\n",
    "        horizontalalignment=\"left\",\n",
    "        verticalalignment=\"top\",\n",
    "        color=\"black\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "handlers = []\n",
    "labels = []\n",
    "for model in MODELS:\n",
    "    # Add handlers, with first being dummy handler\n",
    "    handlers.append(\n",
    "        plt.scatter([0], [0], marker=\"None\", linestyle=\"None\", label=f\"dummy-{model}\")\n",
    "    )\n",
    "    handlers.extend(list(legend_entities_handlers_per_model[model].values()))\n",
    "\n",
    "    # Add labels, with first being model label\n",
    "    labels.append(model)\n",
    "    labels.extend(list(legend_entities_handlers_per_model[model].keys()))\n",
    "\n",
    "legend = fig.legend(handlers, labels, ncol=len(MODELS), loc=\"outside lower center\")\n",
    "\n",
    "ax.set_xlim(0, 4)\n",
    "plt.title(\"SBIC data\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1 (macro) (over all seeds)\")\n",
    "plt.savefig(\"outputs/figures/sbic__performance-box-plot.pdf\")\n",
    "plt.savefig(\"outputs/figures/sbic__performance-box-plot.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition frequency\n",
    "\n",
    "_How often was each composition chosen (bar chart with box plot), how often was each technique and combination of technqiues chosen (heatmap)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composition_counts_per_seed_per_model = {}\n",
    "\n",
    "for model in composition_predictions_test_per_model.keys():\n",
    "    model_name_without_data = model[: model.find(\"__\")]\n",
    "\n",
    "    composition_counts_per_seed_per_model[model] = {}\n",
    "    for seed in RANDOM_SEED:\n",
    "        comp_count = Counter(\n",
    "            composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]\n",
    "        )\n",
    "        for composition in predictions_per_composition_test_per_model[\n",
    "            model_name_without_data\n",
    "        ]:\n",
    "            if composition not in composition_counts_per_seed_per_model[model].keys():\n",
    "                composition_counts_per_seed_per_model[model][composition] = []\n",
    "\n",
    "            if composition in comp_count.keys():\n",
    "                composition_counts_per_seed_per_model[model][composition].append(\n",
    "                    comp_count[composition]\n",
    "                )\n",
    "            else:\n",
    "                composition_counts_per_seed_per_model[model][composition].append(0)\n",
    "\n",
    "    # Calculate bar heights (mean) and error bars (standard deviation)\n",
    "    compositions = list(composition_counts_per_seed_per_model[model].keys())\n",
    "    values = [\n",
    "        np.mean(composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    lower_errors = [\n",
    "        np.mean(composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.min(composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "    upper_errors = [\n",
    "        np.max(composition_counts_per_seed_per_model[model][comp])\n",
    "        - np.mean(composition_counts_per_seed_per_model[model][comp])\n",
    "        for comp in compositions\n",
    "    ]\n",
    "\n",
    "    # Combine the data into a list of tuples and sort by values (mean)\n",
    "    sorted_data = sorted(\n",
    "        zip(values, lower_errors, upper_errors, compositions),\n",
    "        key=lambda x: x[0],\n",
    "        reverse=False,\n",
    "    )\n",
    "\n",
    "    # Unpack the sorted data\n",
    "    values, lower_errors, upper_errors, compositions = zip(*sorted_data)\n",
    "\n",
    "    # Create asymmetric error arrays\n",
    "    asymmetric_errors = [lower_errors, upper_errors]\n",
    "\n",
    "    # Bar chart positions\n",
    "    x_pos = np.arange(len(compositions))\n",
    "\n",
    "    # Plot bars\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    # bars = plt.bar(x_pos, values, yerr=asymmetric_errors, align=\"center\", alpha=0.7, capsize=0)\n",
    "    bars = plt.barh(x_pos, values, align=\"center\", alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.yticks(x_pos, compositions, ha=\"right\")\n",
    "    plt.ylabel(\"Compositions\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.title(f\"{model}: Composition counts (over five random seeds) on SBIC\")\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/sbic__{model}__composition-frequency.pdf\", bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.savefig(\n",
    "        f\"outputs/figures/sbic__{model}__composition-frequency.png\", bbox_inches=\"tight\"\n",
    "    )\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each composition produces a correct prediction for each split\n",
    "target_dataset = \"sbic\"\n",
    "\n",
    "sbic_train_correct_prediction_counts_per_seed_per_model = {}\n",
    "sbic_val_correct_prediction_counts_per_seed_per_model = {}\n",
    "sbic_test_correct_prediction_counts_per_seed_per_model = {}\n",
    "\n",
    "# Train dataset (needs slightly different loading)\n",
    "# We use the composition names form the test data loaded further above, but then load the\n",
    "# predictions on the train data below from file\n",
    "for (\n",
    "    model,\n",
    "    composition_predictions,\n",
    ") in predictions_per_composition_test_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in sbic_train_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        sbic_train_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        # Load composition predictions for trainin dataset\n",
    "        if \"cot\" in composition:\n",
    "            comp_no_cot = composition.replace(\"cot_\", \"\")\n",
    "            model_composition_df_train = pd.read_parquet(\n",
    "                f\"outputs/prompt-predictions/{target_dataset}/{target_dataset}-cot-greedy-train_{model}_{comp_no_cot}.parquet\"\n",
    "            )\n",
    "        else:\n",
    "            model_composition_df_train = pd.read_parquet(\n",
    "                f\"outputs/prompt-predictions/{target_dataset}/{target_dataset}-greedy-train_{model}_{composition}.parquet\"\n",
    "            )\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df_train[\n",
    "                        model_composition_df_train[\"true_label\"]\n",
    "                        == model_composition_df_train[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df_train[\n",
    "                        model_composition_df_train[\"true_label\"]\n",
    "                        == model_composition_df_train[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        sbic_train_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ][composition] = correct_predictions_per_seed\n",
    "\n",
    "# Val dataset\n",
    "for model, composition_predictions in predictions_per_composition_val_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in sbic_val_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        sbic_val_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            model_composition_df = predictions_per_composition_val_per_model[model][\n",
    "                composition\n",
    "            ]\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        sbic_val_correct_prediction_counts_per_seed_per_model[model_name_in_convention][\n",
    "            composition\n",
    "        ] = correct_predictions_per_seed\n",
    "\n",
    "# Test dataset\n",
    "for (\n",
    "    model,\n",
    "    composition_predictions,\n",
    ") in predictions_per_composition_test_per_model.items():\n",
    "    model_name_in_convention = f\"{model}__{target_dataset}\"\n",
    "\n",
    "    if (\n",
    "        model_name_in_convention\n",
    "        not in sbic_test_correct_prediction_counts_per_seed_per_model.keys()\n",
    "    ):\n",
    "        sbic_test_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ] = {}\n",
    "\n",
    "    for composition in composition_predictions.keys():\n",
    "        correct_predictions_per_seed = []\n",
    "\n",
    "        for seed in RANDOM_SEED:\n",
    "            model_composition_df = predictions_per_composition_test_per_model[model][\n",
    "                composition\n",
    "            ]\n",
    "            try:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_{seed}\"]\n",
    "                    ]\n",
    "                )\n",
    "            except KeyError:\n",
    "                correct_predictions = len(\n",
    "                    model_composition_df[\n",
    "                        model_composition_df[\"true_label\"]\n",
    "                        == model_composition_df[f\"output_23\"]\n",
    "                    ]\n",
    "                )\n",
    "            correct_predictions_per_seed.append(correct_predictions)\n",
    "\n",
    "        sbic_test_correct_prediction_counts_per_seed_per_model[\n",
    "            model_name_in_convention\n",
    "        ][composition] = correct_predictions_per_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composition frequency tables\n",
    "target_dataset = \"sbic\"\n",
    "target_dataset_models = [\n",
    "    m\n",
    "    for m in composition_counts_per_seed_per_model.keys()\n",
    "    if m.endswith(target_dataset)\n",
    "]\n",
    "\n",
    "composition_counts_mean_per_target_dataset_models = {}\n",
    "\n",
    "for model in target_dataset_models:\n",
    "    for composition_name, composition_counts in composition_counts_per_seed_per_model[\n",
    "        model\n",
    "    ].items():\n",
    "        # Calculate average composition frequencies for current model over seeds\n",
    "        mean_frequency_counts = np.mean(composition_counts)\n",
    "\n",
    "        # Calculate standard deviation of composition frequencies over seeds\n",
    "        stddev_frequency_counts = np.std(composition_counts)\n",
    "\n",
    "        # Calculate how often each composition results in the correct prediction per split\n",
    "        mean_correct_prediction = np.mean(\n",
    "            sbic_train_correct_prediction_counts_per_seed_per_model[model][\n",
    "                composition_name\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate standard deviation of correct predictions per composition over seeds\n",
    "        stddev_correct_prediction = np.std(\n",
    "            sbic_train_correct_prediction_counts_per_seed_per_model[model][\n",
    "                composition_name\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            composition_name\n",
    "            not in composition_counts_mean_per_target_dataset_models.keys()\n",
    "        ):\n",
    "            composition_counts_mean_per_target_dataset_models[composition_name] = {}\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            f\"{model}__mean_frequencies_test\"\n",
    "        ] = f\"{mean_frequency_counts} (+- {np.round(stddev_frequency_counts, decimals=2):0.2f})\"\n",
    "\n",
    "        composition_counts_mean_per_target_dataset_models[composition_name][\n",
    "            f\"{model}__mean_correct_prediction_train\"\n",
    "        ] = f\"{mean_correct_prediction} (+- {np.round(stddev_correct_prediction, decimals=2):0.2f})\"\n",
    "\n",
    "\n",
    "# Make composition names nicer for final table\n",
    "counts_with_updated_composition_names = {}\n",
    "for (\n",
    "    composition_name,\n",
    "    composition_counts,\n",
    ") in composition_counts_mean_per_target_dataset_models.items():\n",
    "    composition_name_reformat_rules = {\n",
    "        \"cot\": \"Reasoning steps\",\n",
    "        \"category-few-shot\": \"In-context (category)\",\n",
    "        \"random-few-shot\": \"In-context (random)\",\n",
    "        \"similar-few-shot\": \"In-context (similar)\",\n",
    "        \"definitions\": \"Definitions\",\n",
    "        \"directional-stimulus\": \"Dir. stimulus\",\n",
    "        \"system-prompts\": \"Persona\",\n",
    "        \"task-description-only\": \"Base composition\",\n",
    "    }\n",
    "    composition_name_reformat = \", \".join(\n",
    "        [\n",
    "            composition_name_reformat_rules[comp].capitalize()\n",
    "            for comp in composition_name.split(\"_\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    counts_with_updated_composition_names[composition_name_reformat] = (\n",
    "        composition_counts\n",
    "    )\n",
    "\n",
    "composition_frequency_output_file = path.join(\n",
    "    f\"outputs/tables/composition-frequencies-{target_dataset}.csv\"\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=counts_with_updated_composition_names\n",
    ").transpose().sort_index().to_csv(composition_frequency_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = [\n",
    "    \"cateogory-few-shot\",\n",
    "    \"cot\",\n",
    "    \"definitions\",\n",
    "    \"directional-stimulus\",\n",
    "    \"random-few-shot\",\n",
    "    \"similar-few-shot\",\n",
    "    \"system-prompts\",\n",
    "    \"task-description-only\",\n",
    "]\n",
    "for model in composition_predictions_test_per_model.keys():\n",
    "    all_cooccurrences = []\n",
    "    for t_outer in techniques:\n",
    "        t_cooccurrences = []\n",
    "        for seed in RANDOM_SEED:\n",
    "            seed_cooccurrences = np.zeros(len(techniques))\n",
    "            for pred in composition_predictions_test_per_model[model][\n",
    "                f\"pred_best_composition_seed{seed}\"\n",
    "            ]:\n",
    "                if t_outer in pred:\n",
    "                    for t_inner in techniques:\n",
    "                        if t_inner in pred and t_inner != t_outer:\n",
    "                            seed_cooccurrences[techniques.index(t_inner)] += 1\n",
    "            t_cooccurrences.append(seed_cooccurrences)\n",
    "        all_cooccurrences.append(t_cooccurrences)\n",
    "\n",
    "    average_cooccurrences = np.array(\n",
    "        [\n",
    "            [np.mean(coocc) for coocc in list(zip(*per_seed_occurrences))]\n",
    "            for per_seed_occurrences in all_cooccurrences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # mask = np.triu(np.ones_like(average_cooccurrences, dtype=bool))\n",
    "    # masked_data = np.ma.masked_array(average_cooccurrences, mask)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(average_cooccurrences, cmap=\"plasma\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(len(techniques)), labels=techniques)\n",
    "    ax.set_yticks(np.arange(len(techniques)), labels=techniques)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(techniques)):\n",
    "        for j in range(len(techniques)):\n",
    "            text = ax.text(\n",
    "                j, i, average_cooccurrences[i, j], ha=\"center\", va=\"center\", color=\"w\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{model}: Average (over all seeds) cooccurrence for predicted compositions on SBIC\"\n",
    "    )\n",
    "    plt.savefig(f\"outputs/figures/sbic__{model}__technique-cooccurrences.pdf\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias-detection-prompting-gIi8-57c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
